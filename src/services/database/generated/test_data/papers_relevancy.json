[
    {
        "id": "2006.02482",
        "title": "Explaining the Behavior of Black-Box Prediction Algorithms with Causal\n  Learning",
        "abstract": "  Causal approaches to post-hoc explainability for black-box prediction models\n(e.g., deep neural networks trained on image pixel data) have become\nincreasingly popular. However, existing approaches have two important\nshortcomings: (i) the \"explanatory units\" are micro-level inputs into the\nrelevant prediction model, e.g., image pixels, rather than interpretable\nmacro-level features that are more useful for understanding how to possibly\nchange the algorithm's behavior, and (ii) existing approaches assume there\nexists no unmeasured confounding between features and target model predictions,\nwhich fails to hold when the explanatory units are macro-level variables. Our\nfocus is on the important setting where the analyst has no access to the inner\nworkings of the target prediction algorithm, rather only the ability to query\nthe output of the model in response to a particular input. To provide causal\nexplanations in such a setting, we propose to learn causal graphical\nrepresentations that allow for arbitrary unmeasured confounding among features.\nWe demonstrate the resulting graph can differentiate between interpretable\nfeatures that causally influence model predictions versus those that are merely\nassociated with model predictions due to confounding. Our approach is motivated\nby a counterfactual theory of causal explanation wherein good explanations\npoint to factors that are \"difference-makers\" in an interventionist sense.\n",
        "pdfLink": "http://arxiv.org/pdf/2006.02482.pdf",
        "authors": [
            "Sani Numair",
            "Malinsky Daniel",
            "Shpitser Ilya"
        ],
        "metaData": {
            "relevancy": 0.4122559666633606
        }
    },
    {
        "id": "2109.04398",
        "title": "Neural-IMLS: Self-supervised Implicit Moving Least-Squares Network for\n  Surface Reconstruction",
        "abstract": "  Surface reconstruction is very challenging when the input point clouds,\nparticularly real scans, are noisy and lack normals. Observing that the\nMultilayer Perceptron (MLP) and the implicit moving least-square function\n(IMLS) provide a dual representation of the underlying surface, we introduce\nNeural-IMLS, a novel approach that directly learns the noise-resistant signed\ndistance function (SDF) from unoriented raw point clouds in a self-supervised\nfashion. We use the IMLS to regularize the distance values reported by the MLP\nwhile using the MLP to regularize the normals of the data points for running\nthe IMLS. We also prove that at the convergence, our neural network, benefiting\nfrom the mutual learning mechanism between the MLP and the IMLS, produces a\nfaithful SDF whose zero-level set approximates the underlying surface. We\nconducted extensive experiments on various benchmarks, including synthetic\nscans and real scans. The experimental results show that {\\em Neural-IMLS} can\nreconstruct faithful shapes on various benchmarks with noise and missing parts.\nThe source code can be found at~\\url{https://github.com/bearprin/Neural-IMLS}.\n",
        "pdfLink": "http://arxiv.org/pdf/2109.04398.pdf",
        "authors": [
            "Wang Zixiong",
            "Wang Pengfei",
            "Wang Pengshuai",
            "Dong Qiujie",
            "Gao Junjie",
            "Chen Shuangmin",
            "Xin Shiqing",
            "Tu Changhe",
            "Wang Wenping"
        ],
        "metaData": {
            "relevancy": 0.2950620472431183
        }
    },
    {
        "id": "2207.06462",
        "title": "Quantum Metropolis Solver: A Quantum Walks Approach to Optimization\n  Problems",
        "abstract": "  The efficient resolution of optimization problems is one of the key issues in\ntoday's industry. This task relies mainly on classical algorithms that present\nscalability problems and processing limitations. Quantum computing has emerged\nto challenge these types of problems. In this paper, we focus on the\nMetropolis-Hastings quantum algorithm that is based on quantum walks. We use\nthis algorithm to build a quantum software tool called Quantum Metropolis\nSolver (QMS). We validate QMS with the N-Queen problem to show a potential\nquantum advantage in an example that can be easily extrapolated to an\nArtificial Intelligence domain. We carry out different simulations to validate\nthe performance of QMS and its configuration.\n",
        "pdfLink": "http://arxiv.org/pdf/2207.06462.pdf",
        "authors": [
            "Campos Roberto",
            "Casares Pablo A M",
            "Martin-Delgado M A"
        ],
        "metaData": {
            "relevancy": 0.23253855109214783
        }
    },
    {
        "id": "2207.07886",
        "title": "An Experimental Evaluation of Machine Learning Training on a Real\n  Processing-in-Memory System",
        "abstract": "  Training machine learning (ML) algorithms is a computationally intensive\nprocess, which is frequently memory-bound due to repeatedly accessing large\ntraining datasets. As a result, processor-centric systems (e.g., CPU, GPU)\nsuffer from costly data movement between memory units and processing units,\nwhich consumes large amounts of energy and execution cycles. Memory-centric\ncomputing systems, i.e., with processing-in-memory (PIM) capabilities, can\nalleviate this data movement bottleneck.\n  Our goal is to understand the potential of modern general-purpose PIM\narchitectures to accelerate ML training. To do so, we (1) implement several\nrepresentative classic ML algorithms (namely, linear regression, logistic\nregression, decision tree, K-Means clustering) on a real-world general-purpose\nPIM architecture, (2) rigorously evaluate and characterize them in terms of\naccuracy, performance and scaling, and (3) compare to their counterpart\nimplementations on CPU and GPU. Our evaluation on a real memory-centric\ncomputing system with more than 2500 PIM cores shows that general-purpose PIM\narchitectures can greatly accelerate memory-bound ML workloads, when the\nnecessary operations and datatypes are natively supported by PIM hardware. For\nexample, our PIM implementation of decision tree is $27\\times$ faster than a\nstate-of-the-art CPU version on an 8-core Intel Xeon, and $1.34\\times$ faster\nthan a state-of-the-art GPU version on an NVIDIA A100. Our K-Means clustering\non PIM is $2.8\\times$ and $3.2\\times$ than state-of-the-art CPU and GPU\nversions, respectively.\n  To our knowledge, our work is the first one to evaluate ML training on a\nreal-world PIM architecture. We conclude with key observations, takeaways, and\nrecommendations that can inspire users of ML workloads, programmers of PIM\narchitectures, and hardware designers & architects of future memory-centric\ncomputing systems.\n",
        "pdfLink": "http://arxiv.org/pdf/2207.07886.pdf",
        "authors": [
            "G\u00f3mez-Luna Juan",
            "Guo Yuxin",
            "Brocard Sylvan",
            "Legriel Julien",
            "Cimadomo Remy",
            "Oliveira Geraldo F.",
            "Singh Gagandeep",
            "Mutlu Onur"
        ],
        "metaData": {
            "relevancy": 0.34442576169967654
        }
    },
    {
        "id": "2208.05318",
        "title": "Generative Action Description Prompts for Skeleton-based Action\n  Recognition",
        "abstract": "  Skeleton-based action recognition has recently received considerable\nattention. Current approaches to skeleton-based action recognition are\ntypically formulated as one-hot classification tasks and do not fully exploit\nthe semantic relations between actions. For example, \"make victory sign\" and\n\"thumb up\" are two actions of hand gestures, whose major difference lies in the\nmovement of hands. This information is agnostic from the categorical one-hot\nencoding of action classes but could be unveiled from the action description.\nTherefore, utilizing action description in training could potentially benefit\nrepresentation learning. In this work, we propose a Generative\nAction-description Prompts (GAP) approach for skeleton-based action\nrecognition. More specifically, we employ a pre-trained large-scale language\nmodel as the knowledge engine to automatically generate text descriptions for\nbody parts movements of actions, and propose a multi-modal training scheme by\nutilizing the text encoder to generate feature vectors for different body parts\nand supervise the skeleton encoder for action representation learning.\nExperiments show that our proposed GAP method achieves noticeable improvements\nover various baseline models without extra computation cost at inference. GAP\nachieves new state-of-the-arts on popular skeleton-based action recognition\nbenchmarks, including NTU RGB+D, NTU RGB+D 120 and NW-UCLA. The source code is\navailable at https://github.com/MartinXM/GAP.\n",
        "pdfLink": "http://arxiv.org/pdf/2208.05318.pdf",
        "authors": [
            "Xiang Wangmeng",
            "Li Chao",
            "Zhou Yuxuan",
            "Wang Biao",
            "Zhang Lei"
        ],
        "metaData": {
            "relevancy": 0.46187402606010436
        }
    },
    {
        "id": "2208.13390",
        "title": "Unified Bayesian Frameworks for Multi-criteria Decision-making Problems",
        "abstract": "  This paper introduces Bayesian frameworks for tackling various aspects of\nmulti-criteria decision-making (MCDM) problems, leveraging a probabilistic\ninterpretation of MCDM methods and challenges. By harnessing the flexibility of\nBayesian models, the proposed frameworks offer statistically elegant solutions\nto key challenges in MCDM, such as group decision-making problems and criteria\ncorrelation. Additionally, these models can accommodate diverse forms of\nuncertainty in decision makers' (DMs) preferences, including normal and\ntriangular distributions, as well as interval preferences. To address\nlarge-scale group MCDM scenarios, a probabilistic mixture model is developed,\nenabling the identification of homogeneous subgroups of DMs. Furthermore, a\nprobabilistic ranking scheme is devised to assess the relative importance of\ncriteria and alternatives based on DM(s) preferences. Through experimentation\non various numerical examples, the proposed frameworks are validated,\ndemonstrating their effectiveness and highlighting their distinguishing\nfeatures in comparison to alternative methods.\n",
        "pdfLink": "http://arxiv.org/pdf/2208.13390.pdf",
        "authors": [
            "Mohammadi Majid"
        ],
        "metaData": {
            "relevancy": 0.2515254944562912
        }
    },
    {
        "id": "2210.10971",
        "title": "Optimal Settings for Cryptocurrency Trading Pairs",
        "abstract": "  The goal of cryptocurrencies is decentralization. In principle, all\ncurrencies have equal status. Unlike traditional stock markets, there is no\ndefault currency of denomination (fiat), thus the trading pairs can be set\nfreely. However, it is impractical to set up a trading market between every two\ncurrencies. In order to control management costs and ensure sufficient\nliquidity, we must give priority to covering those large-volume trading pairs\nand ensure that all coins are reachable. We note that this is an optimization\nproblem. Its particularity lies in: 1) the trading volume between most (>99.5%)\npossible trading pairs cannot be directly observed. 2) It satisfies the\nconnectivity constraint, that is, all currencies are guaranteed to be tradable.\n  To solve this problem, we use a two-stage process: 1) Fill in missing values\nbased on a regularized, truncated eigenvalue decomposition, where the\nregularization term is used to control what extent missing values should be\nlimited to zero. 2) Search for the optimal trading pairs, based on a branch and\nbound process, with heuristic search and pruning strategies.\n  The experimental results show that: 1) If the number of denominated coins is\nnot limited, we will get a more decentralized trading pair settings, which\nadvocates the establishment of trading pairs directly between large currency\npairs. 2) There is a certain room for optimization in all exchanges. The\nsetting of inappropriate trading pairs is mainly caused by subjectively setting\nsmall coins to quote, or failing to track emerging big coins in time. 3) Too\nfew trading pairs will lead to low coverage; too many trading pairs will need\nto be adjusted with markets frequently. Exchanges should consider striking an\nappropriate balance between them.\n",
        "pdfLink": "http://arxiv.org/pdf/2210.10971.pdf",
        "authors": [
            "Zhang Di",
            "Niu Qiang",
            "Zhou Youzhou"
        ],
        "metaData": {
            "relevancy": 0.1772991269826889
        }
    },
    {
        "id": "2210.15073",
        "title": "Hierarchical quantum circuit representations for neural architecture\n  search",
        "abstract": "  Machine learning with hierarchical quantum circuits, usually referred to as\nQuantum Convolutional Neural Networks (QCNNs), is a promising prospect for\nnear-term quantum computing. The QCNN is a circuit model inspired by the\narchitecture of Convolutional Neural Networks (CNNs). CNNs are successful\nbecause they do not need manual feature design and can learn high-level\nfeatures from raw data. Neural Architecture Search (NAS) builds on this success\nby learning network architecture and achieves state-of-the-art performance.\nHowever, applying NAS to QCNNs presents unique challenges due to the lack of a\nwell-defined search space. In this work, we propose a novel framework for\nrepresenting QCNN architectures using techniques from NAS, which enables search\nspace design and architecture search. Using this framework, we generate a\nfamily of popular QCNNs, those resembling reverse binary trees. We then\nevaluate this family of models on a music genre classification dataset, GTZAN,\nto justify the importance of circuit architecture. Furthermore, we employ a\ngenetic algorithm to perform Quantum Phase Recognition (QPR) as an example of\narchitecture search with our representation. This work provides a way to\nimprove model performance without increasing complexity and to jump around the\ncost landscape to avoid barren plateaus. Finally, we implement the framework as\nan open-source Python package to enable dynamic QCNN creation and facilitate\nQCNN search space design for NAS.\n",
        "pdfLink": "http://arxiv.org/pdf/2210.15073.pdf",
        "authors": [
            "Lourens Matt",
            "Sinayskiy Ilya",
            "Park Daniel K.",
            "Blank Carsten",
            "Petruccione Francesco"
        ],
        "metaData": {
            "relevancy": 0.30622280240058897
        }
    },
    {
        "id": "2211.06919",
        "title": "Towards Privacy-Aware Causal Structure Learning in Federated Setting",
        "abstract": "  Causal structure learning has been extensively studied and widely used in\nmachine learning and various applications. To achieve an ideal performance,\nexisting causal structure learning algorithms often need to centralize a large\namount of data from multiple data sources. However, in the privacy-preserving\nsetting, it is impossible to centralize data from all sources and put them\ntogether as a single dataset. To preserve data privacy, federated learning as a\nnew learning paradigm has attracted much attention in machine learning in\nrecent years. In this paper, we study a privacy-aware causal structure learning\nproblem in the federated setting and propose a novel Federated PC (FedPC)\nalgorithm with two new strategies for preserving data privacy without\ncentralizing data. Specifically, we first propose a novel layer-wise\naggregation strategy for a seamless adaptation of the PC algorithm into the\nfederated learning paradigm for federated skeleton learning, then we design an\neffective strategy for learning consistent separation sets for federated edge\norientation. The extensive experiments validate that FedPC is effective for\ncausal structure learning in a federated learning setting.\n",
        "pdfLink": "http://arxiv.org/pdf/2211.06919.pdf",
        "authors": [
            "Huang Jianli",
            "Guo Xianjie",
            "Yu Kui",
            "Cao Fuyuan",
            "Liang Jiye"
        ],
        "metaData": {
            "relevancy": 0.36978378891944885
        }
    },
    {
        "id": "2212.01071",
        "title": "Fake detection in imbalance dataset by Semi-supervised learning with GAN",
        "abstract": "  As social media grows faster, harassment becomes more prevalent which leads\nto considered fake detection a fascinating field among researchers. The graph\nnature of data with the large number of nodes caused different obstacles\nincluding a considerable amount of unrelated features in matrices as high\ndispersion and imbalance classes in the dataset. To deal with these issues\nAuto-encoders and a combination of semi-supervised learning and the GAN\nalgorithm which is called SGAN were used. This paper is deploying a smaller\nnumber of labels and applying SGAN as a classifier. The result of this test\nshowed that the accuracy had reached 91\\% in detecting fake accounts using only\n100 labeled samples.\n",
        "pdfLink": "http://arxiv.org/pdf/2212.01071.pdf",
        "authors": [
            "Bordbar Jinus",
            "Ardalan Saman",
            "Mohammadrezaie Mohammadreza",
            "Shiri Mohammad Ebrahim"
        ],
        "metaData": {
            "relevancy": 0.2490593433380127
        }
    },
    {
        "id": "2302.04954",
        "title": "Mixed formulation of physics-informed neural networks for\n  thermo-mechanically coupled systems and heterogeneous domains",
        "abstract": "  Physics-informed neural networks (PINNs) are a new tool for solving boundary\nvalue problems by defining loss functions of neural networks based on governing\nequations, boundary conditions, and initial conditions. Recent investigations\nhave shown that when designing loss functions for many engineering problems,\nusing first-order derivatives and combining equations from both strong and weak\nforms can lead to much better accuracy, especially when there are heterogeneity\nand variable jumps in the domain. This new approach is called the mixed\nformulation for PINNs, which takes ideas from the mixed finite element method.\nIn this method, the PDE is reformulated as a system of equations where the\nprimary unknowns are the fluxes or gradients of the solution, and the secondary\nunknowns are the solution itself. In this work, we propose applying the mixed\nformulation to solve multi-physical problems, specifically a stationary\nthermo-mechanically coupled system of equations. Additionally, we discuss both\nsequential and fully coupled unsupervised training and compare their accuracy\nand computational cost. To improve the accuracy of the network, we incorporate\nhard boundary constraints to ensure valid predictions. We then investigate how\ndifferent optimizers and architectures affect accuracy and efficiency. Finally,\nwe introduce a simple approach for parametric learning that is similar to\ntransfer learning. This approach combines data and physics to address the\nlimitations of PINNs regarding computational cost and improves the network's\nability to predict the response of the system for unseen cases. The outcomes of\nthis work will be useful for many other engineering applications where deep\nlearning is employed on multiple coupled systems of equations for fast and\nreliable computations.\n",
        "pdfLink": "http://arxiv.org/pdf/2302.04954.pdf",
        "authors": [
            "Harandi Ali",
            "Moeineddin Ahmad",
            "Kaliske Michael",
            "Reese Stefanie",
            "Rezaei Shahed"
        ],
        "metaData": {
            "relevancy": 0.24590750634670258
        }
    },
    {
        "id": "2302.06527",
        "title": "An Empirical Evaluation of Using Large Language Models for Automated\n  Unit Test Generation",
        "abstract": "  Unit tests play a key role in ensuring the correctness of software. However,\nmanually creating unit tests is a laborious task, motivating the need for\nautomation. Large Language Models (LLMs) have recently been applied to this\nproblem, utilizing additional training or few-shot learning on examples of\nexisting tests. This paper presents a large-scale empirical evaluation on the\neffectiveness of LLMs for automated unit test generation without additional\ntraining or manual effort, providing the LLM with the signature and\nimplementation of the function under test, along with usage examples extracted\nfrom documentation. We also attempt to repair failed generated tests by\nre-prompting the model with the failing test and error message. We implement\nour approach in TestPilot, a test generation tool for JavaScript that\nautomatically generates unit tests for all API functions in an npm package. We\nevaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a\ntotal of 1,684 API functions. The generated tests achieve a median statement\ncoverage of 70.2% and branch coverage of 52.8%, significantly improving on\nNessie, a recent feedback-directed JavaScript test generation technique, which\nachieves only 51.3% statement coverage and 25.6% branch coverage. We also find\nthat 92.8% of TestPilot's generated tests have no more than 50% similarity with\nexisting tests (as measured by normalized edit distance), with none of them\nbeing exact copies. Finally, we run TestPilot with two additional LLMs,\nOpenAI's older code-cushman-002 LLM and the open LLM StarCoder. Overall, we\nobserved similar results with the former (68.2% median statement coverage), and\nsomewhat worse results with the latter (54.0% median statement coverage),\nsuggesting that the effectiveness of the approach is influenced by the size and\ntraining set of the LLM, but does not fundamentally depend on the specific\nmodel.\n",
        "pdfLink": "http://arxiv.org/pdf/2302.06527.pdf",
        "authors": [
            "Sch\u00e4fer Max",
            "Nadi Sarah",
            "Eghbali Aryaz",
            "Tip Frank"
        ],
        "metaData": {
            "relevancy": 0.4168480396270752
        }
    },
    {
        "id": "2302.12598",
        "title": "Dynamic Graph Convolutional Network with Attention Fusion for Traffic\n  Flow Prediction",
        "abstract": "  Accurate and real-time traffic state prediction is of great practical\nimportance for urban traffic control and web mapping services. With the support\nof massive data, deep learning methods have shown their powerful capability in\ncapturing the complex spatialtemporal patterns of traffic networks. However,\nexisting approaches use pre-defined graphs and a simple set of spatial-temporal\ncomponents, making it difficult to model multi-scale spatial-temporal\ndependencies. In this paper, we propose a novel dynamic graph convolution\nnetwork with attention fusion to tackle this gap. The method first enhances the\ninteraction of temporal feature dimensions, and then it combines a dynamic\ngraph learner with GRU to jointly model synchronous spatial-temporal\ncorrelations. We also incorporate spatial-temporal attention modules to\neffectively capture longrange, multifaceted domain spatial-temporal patterns.\nWe conduct extensive experiments in four real-world traffic datasets to\ndemonstrate that our method surpasses state-of-the-art performance compared to\n18 baseline methods.\n",
        "pdfLink": "http://arxiv.org/pdf/2302.12598.pdf",
        "authors": [
            "Luo Xunlian",
            "Zhu Chunjiang",
            "Zhang Detian",
            "Li Qing"
        ],
        "metaData": {
            "relevancy": 0.37379419803619385
        }
    },
    {
        "id": "2303.05382",
        "title": "ChatGPT is on the Horizon: Could a Large Language Model be Suitable for\n  Intelligent Traffic Safety Research and Applications?",
        "abstract": "  ChatGPT embarks on a new era of artificial intelligence and will\nrevolutionize the way we approach intelligent traffic safety systems. This\npaper begins with a brief introduction about the development of large language\nmodels (LLMs). Next, we exemplify using ChatGPT to address key traffic safety\nissues. Furthermore, we discuss the controversies surrounding LLMs, raise\ncritical questions for their deployment, and provide our solutions. Moreover,\nwe propose an idea of multi-modality representation learning for smarter\ntraffic safety decision-making and open more questions for application\nimprovement. We believe that LLM will both shape and potentially facilitate\ncomponents of traffic safety research.\n",
        "pdfLink": "http://arxiv.org/pdf/2303.05382.pdf",
        "authors": [
            "Zheng Ou",
            "Abdel-Aty Mohamed",
            "Wang Dongdong",
            "Wang Zijin",
            "Ding Shengxuan"
        ],
        "metaData": {
            "relevancy": 0.4652212917804718
        }
    },
    {
        "id": "2303.15233",
        "title": "Text-to-Image Diffusion Models are Zero-Shot Classifiers",
        "abstract": "  The excellent generative capabilities of text-to-image diffusion models\nsuggest they learn informative representations of image-text data. However,\nwhat knowledge their representations capture is not fully understood, and they\nhave not been thoroughly explored on downstream tasks. We investigate diffusion\nmodels by proposing a method for evaluating them as zero-shot classifiers. The\nkey idea is using a diffusion model's ability to denoise a noised image given a\ntext description of a label as a proxy for that label's likelihood. We apply\nour method to Stable Diffusion and Imagen, using it to probe fine-grained\naspects of the models' knowledge and comparing them with CLIP's zero-shot\nabilities. They perform competitively with CLIP on a wide range of zero-shot\nimage classification datasets. Additionally, they achieve state-of-the-art\nresults on shape/texture bias tests and can successfully perform attribute\nbinding while CLIP cannot. Although generative pre-training is prevalent in\nNLP, visual foundation models often use other methods such as contrastive\nlearning. Based on our findings, we argue that generative pre-training should\nbe explored as a compelling alternative for vision-language tasks.\n",
        "pdfLink": "http://arxiv.org/pdf/2303.15233.pdf",
        "authors": [
            "Clark Kevin",
            "Jaini Priyank"
        ],
        "metaData": {
            "relevancy": 0.41845670342445374
        }
    },
    {
        "id": "2304.06044",
        "title": "Learning solution of nonlinear constitutive material models using\n  physics-informed neural networks: COMM-PINN",
        "abstract": "  We applied physics-informed neural networks to solve the constitutive\nrelations for nonlinear, path-dependent material behavior. As a result, the\ntrained network not only satisfies all thermodynamic constraints but also\ninstantly provides information about the current material state (i.e., free\nenergy, stress, and the evolution of internal variables) under any given\nloading scenario without requiring initial data. One advantage of this work is\nthat it bypasses the repetitive Newton iterations needed to solve nonlinear\nequations in complex material models. Additionally, strategies are provided to\nreduce the required order of derivative for obtaining the tangent operator. The\ntrained model can be directly used in any finite element package (or other\nnumerical methods) as a user-defined material model. However, challenges remain\nin the proper definition of collocation points and in integrating several\nnon-equality constraints that become active or non-active simultaneously. We\ntested this methodology on rate-independent processes such as the classical von\nMises plasticity model with a nonlinear hardening law, as well as local damage\nmodels for interface cracking behavior with a nonlinear softening law. In order\nto demonstrate the applicability of the methodology in handling complex path\ndependency in a three-dimensional (3D) scenario, we tested the approach using\nthe equations governing a damage model for a three-dimensional interface model.\nSuch models are frequently employed for intergranular fracture at grain\nboundaries. We have observed a perfect agreement between the results obtained\nthrough the proposed methodology and those obtained using the classical\napproach. Furthermore, the proposed approach requires significantly less effort\nin terms of implementation and computing time compared to the traditional\nmethods.\n",
        "pdfLink": "http://arxiv.org/pdf/2304.06044.pdf",
        "authors": [
            "Rezaei Shahed",
            "Moeineddin Ahmad",
            "Harandi Ali"
        ],
        "metaData": {
            "relevancy": 0.2692836582660675
        }
    },
    {
        "id": "2305.03472",
        "title": "Generative Steganography Diffusion",
        "abstract": "  Generative steganography (GS) is an emerging technique that generates stego\nimages directly from secret data. Various GS methods based on GANs or Flow have\nbeen developed recently. However, existing GAN-based GS methods cannot\ncompletely recover the hidden secret data due to the lack of network\ninvertibility, while Flow-based methods produce poor image quality due to the\nstringent reversibility restriction in each module. To address this issue, we\npropose a novel GS scheme called \"Generative Steganography Diffusion\" (GSD) by\ndevising an invertible diffusion model named \"StegoDiffusion\". It not only\ngenerates realistic stego images but also allows for 100\\% recovery of the\nhidden secret data. The proposed StegoDiffusion model leverages a non-Markov\nchain with a fast sampling technique to achieve efficient stego image\ngeneration. By constructing an ordinary differential equation (ODE) based on\nthe transition probability of the generation process in StegoDiffusion, secret\ndata and stego images can be converted to each other through the approximate\nsolver of ODE -- Euler iteration formula, enabling the use of irreversible but\nmore expressive network structures to achieve model invertibility. Our proposed\nGSD has the advantages of both reversibility and high performance,\nsignificantly outperforming existing GS methods in all metrics.\n",
        "pdfLink": "http://arxiv.org/pdf/2305.03472.pdf",
        "authors": [
            "Wei Ping",
            "Zhou Qing",
            "Wang Zichi",
            "Qian Zhenxing",
            "Zhang Xinpeng",
            "Li Sheng"
        ],
        "metaData": {
            "relevancy": 0.1747236967086792
        }
    },
    {
        "id": "2305.03859",
        "title": "Open problems in causal structure learning: A case study of COVID-19 in\n  the UK",
        "abstract": "  Causal machine learning (ML) algorithms recover graphical structures that\ntell us something about cause-and-effect relationships. The causal\nrepresentation praovided by these algorithms enables transparency and\nexplainability, which is necessary for decision making in critical real-world\nproblems. Yet, causal ML has had limited impact in practice compared to\nassociational ML. This paper investigates the challenges of causal ML with\napplication to COVID-19 UK pandemic data. We collate data from various public\nsources and investigate what the various structure learning algorithms learn\nfrom these data. We explore the impact of different data formats on algorithms\nspanning different classes of learning, and assess the results produced by each\nalgorithm, and groups of algorithms, in terms of graphical structure, model\ndimensionality, sensitivity analysis, confounding variables, predictive and\ninterventional inference. We use these results to highlight open problems in\ncausal structure learning and directions for future research. To facilitate\nfuture work, we make all graphs, models, data sets, and source code publicly\navailable online.\n",
        "pdfLink": "http://arxiv.org/pdf/2305.03859.pdf",
        "authors": [
            "Constantinou Anthony",
            "Kitson Neville K.",
            "Liu Yang",
            "Chobtham Kiattikun",
            "Hashemzadeh Arian",
            "Nanavati Praharsh A.",
            "Mbuvha Rendani",
            "Petrungaro Bruno"
        ],
        "metaData": {
            "relevancy": 0.41105244159698484
        }
    },
    {
        "id": "2305.18798",
        "title": "AnoOnly: Semi-Supervised Anomaly Detection with the Only Loss on\n  Anomalies",
        "abstract": "  Semi-supervised anomaly detection (SSAD) methods have demonstrated their\neffectiveness in enhancing unsupervised anomaly detection (UAD) by leveraging\nfew-shot but instructive abnormal instances. However, the dominance of\nhomogeneous normal data over anomalies biases the SSAD models against\neffectively perceiving anomalies. To address this issue and achieve balanced\nsupervision between heavily imbalanced normal and abnormal data, we develop a\nnovel framework called AnoOnly (Anomaly Only). Unlike existing SSAD methods\nthat resort to strict loss supervision, AnoOnly suspends it and introduces a\nform of weak supervision for normal data. This weak supervision is instantiated\nthrough the utilization of batch normalization, which implicitly performs\ncluster learning on normal data. When integrated into existing SSAD methods,\nthe proposed AnoOnly demonstrates remarkable performance enhancements across\nvarious models and datasets, achieving new state-of-the-art performance.\nAdditionally, our AnoOnly is natively robust to label noise when suffering from\ndata contamination. Our code is publicly available at\nhttps://github.com/cool-xuan/AnoOnly.\n",
        "pdfLink": "http://arxiv.org/pdf/2305.18798.pdf",
        "authors": [
            "Zhou Yixuan",
            "Yang Peiyu",
            "Qu Yi",
            "Xu Xing",
            "Sun Zhe",
            "Cichocki Andrzej"
        ],
        "metaData": {
            "relevancy": 0.2927365958690643
        }
    },
    {
        "id": "2306.01665",
        "title": "SourceP: Detecting Ponzi Schemes on Ethereum with Source Code",
        "abstract": "  As blockchain technology becomes more and more popular, a typical financial\nscam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.\nThis Ponzi scheme deployed through smart contracts, also known as the smart\nPonzi scheme, has caused a lot of economic losses and negative impacts.\nExisting methods for detecting smart Ponzi schemes on Ethereum mainly rely on\nbytecode features, opcode features, account features, and transaction behavior\nfeatures of smart contracts, and the performance of identifying schemes is\ninsufficient. In this paper, we propose SourceP, a method to detect smart Ponzi\nschemes on the Ethereum platform using pre-trained models and data flow, which\nonly requires using the source code of smart contracts as features to explore\nthe possibility of detecting smart Ponzi schemes from another direction.\nSourceP reduces the difficulty of data acquisition and feature extraction of\nexisting detection methods while increasing the interpretability of the model.\nSpecifically, we first convert the source code of a smart contract into a data\nflow graph and then introduce a pre-trained model based on learning code\nrepresentations to build a classification model to identify Ponzi schemes in\nsmart contracts. The experimental results show that SourceP achieves 87.2\\%\nrecall and 90.7\\% F-score for detecting smart Ponzi schemes within Ethereum's\nsmart contract dataset, outperforming state-of-the-art methods in terms of\nperformance and sustainability. We also demonstrate through additional\nexperiments that pre-trained models and data flow play an important\ncontribution to SourceP, as well as proving that SourceP has a good\ngeneralization ability.\n",
        "pdfLink": "http://arxiv.org/pdf/2306.01665.pdf",
        "authors": [
            "Lu Pengcheng",
            "Cai Liang",
            "Yin Keting"
        ],
        "metaData": {
            "relevancy": 0.2510835975408554
        }
    },
    {
        "id": "2306.03933",
        "title": "High-dimensional and Permutation Invariant Anomaly Detection",
        "abstract": "  Methods for anomaly detection of new physics processes are often limited to\nlow-dimensional spaces due to the difficulty of learning high-dimensional\nprobability densities. Particularly at the constituent level, incorporating\ndesirable properties such as permutation invariance and variable-length inputs\nbecomes difficult within popular density estimation methods. In this work, we\nintroduce a permutation-invariant density estimator for particle physics data\nbased on diffusion models, specifically designed to handle variable-length\ninputs. We demonstrate the efficacy of our methodology by utilizing the learned\ndensity as a permutation-invariant anomaly detection score, effectively\nidentifying jets with low likelihood under the background-only hypothesis. To\nvalidate our density estimation method, we investigate the ratio of learned\ndensities and compare to those obtained by a supervised classification\nalgorithm.\n",
        "pdfLink": "http://arxiv.org/pdf/2306.03933.pdf",
        "authors": [
            "Mikuni Vinicius",
            "Nachman Benjamin"
        ],
        "metaData": {
            "relevancy": 0.2051766037940979
        }
    },
    {
        "id": "2306.10587",
        "title": "Acceleration in Policy Optimization",
        "abstract": "  We work towards a unifying paradigm for accelerating policy optimization\nmethods in reinforcement learning (RL) by integrating foresight in the policy\nimprovement step via optimistic and adaptive updates. Leveraging the connection\nbetween policy iteration and policy gradient methods, we view policy\noptimization algorithms as iteratively solving a sequence of surrogate\nobjectives, local lower bounds on the original objective. We define optimism as\npredictive modelling of the future behavior of a policy, and adaptivity as\ntaking immediate and anticipatory corrective actions to mitigate accumulating\nerrors from overshooting predictions or delayed responses to change. We use\nthis shared lens to jointly express other well-known algorithms, including\nmodel-based policy improvement based on forward search, and optimistic\nmeta-learning algorithms. We analyze properties of this formulation, and show\nconnections to other accelerated optimization algorithms. Then, we design an\noptimistic policy gradient algorithm, adaptive via meta-gradient learning, and\nempirically highlight several design choices pertaining to acceleration, in an\nillustrative task.\n",
        "pdfLink": "http://arxiv.org/pdf/2306.10587.pdf",
        "authors": [
            "Chelu Veronica",
            "Zahavy Tom",
            "Guez Arthur",
            "Precup Doina",
            "Flennerhag Sebastian"
        ],
        "metaData": {
            "relevancy": 0.5468094110488891
        }
    },
    {
        "id": "2307.02131",
        "title": "Beyond Known Reality: Exploiting Counterfactual Explanations for Medical\n  Research",
        "abstract": "  The field of explainability in artificial intelligence has witnessed a\ngrowing number of studies and increasing scholarly interest. However, the lack\nof human-friendly and individual interpretations in explaining the outcomes of\nmachine learning algorithms has significantly hindered the acceptance of these\nmethods by clinicians in their research and clinical practice. To address this,\nour study employs counterfactual explanations to explore \"what if?\" scenarios\nin medical research, aiming to expand our understanding beyond existing\nboundaries on magnetic resonance imaging (MRI) features for diagnosing\npediatric posterior fossa brain tumors. In our case study, the proposed concept\nprovides a novel way to examine alternative decision-making scenarios that\noffer personalized and context-specific insights, enabling the validation of\npredictions and clarification of variations under diverse circumstances.\nAdditionally, we explore the potential use of counterfactuals for data\naugmentation and evaluate their feasibility as an alternative approach in our\nmedical research case. The results demonstrate the promising potential of using\ncounterfactual explanations to enhance trust and acceptance of AI-driven\nmethods in clinical research.\n",
        "pdfLink": "http://arxiv.org/pdf/2307.02131.pdf",
        "authors": [
            "Tanyel Toygar",
            "Ayvaz Serkan",
            "Keserci Bilgin"
        ],
        "metaData": {
            "relevancy": 0.40362787842750547
        }
    },
    {
        "id": "2307.02694",
        "title": "Loss Functions and Metrics in Deep Learning",
        "abstract": "  One of the essential components of deep learning is the choice of the loss\nfunction and performance metrics used to train and evaluate models. This paper\nreviews the most prevalent loss functions and performance measurements in deep\nlearning. We examine the benefits and limits of each technique and illustrate\ntheir application to various deep-learning problems. Our review aims to give a\ncomprehensive picture of the different loss functions and performance\nindicators used in the most common deep learning tasks and help practitioners\nchoose the best method for their specific task.\n",
        "pdfLink": "http://arxiv.org/pdf/2307.02694.pdf",
        "authors": [
            "Terven Juan",
            "Cordova-Esparza Diana M.",
            "Ramirez-Pedraza Alfonso",
            "Chavez-Urbiola Edgar A."
        ],
        "metaData": {
            "relevancy": 0.2909985274076462
        }
    },
    {
        "id": "2307.04869",
        "title": "Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual\n  Learning",
        "abstract": "  Federated continual learning (FCL) learns incremental tasks over time from\nconfidential datasets distributed across clients. This paper focuses on\nrehearsal-free FCL, which has severe forgetting issues when learning new tasks\ndue to the lack of access to historical task data. To address this issue, we\npropose Fed-CPrompt based on prompt learning techniques to obtain task-specific\nprompts in a communication-efficient way. Fed-CPrompt introduces two key\ncomponents, asynchronous prompt learning, and contrastive continual loss, to\nhandle asynchronous task arrival and heterogeneous data distributions in FCL,\nrespectively. Extensive experiments demonstrate the effectiveness of\nFed-CPrompt in achieving SOTA rehearsal-free FCL performance.\n",
        "pdfLink": "http://arxiv.org/pdf/2307.04869.pdf",
        "authors": [
            "Bagwe Gaurav",
            "Yuan Xiaoyong",
            "Pan Miao",
            "Zhang Lan"
        ],
        "metaData": {
            "relevancy": 0.4129948794841766
        }
    },
    {
        "id": "2307.15778",
        "title": "Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding\n  for Ising MRF Models: Classical and Quantum Topology Machine Learning",
        "abstract": "  The paper introduces the application of information geometry to describe the\nground states of Ising models by utilizing parity-check matrices of cyclic and\nquasi-cyclic codes on toric and spherical topologies. The approach establishes\na connection between machine learning and error-correcting coding. This\nproposed approach has implications for the development of new embedding methods\nbased on trapping sets. Statistical physics and number geometry applied for\noptimize error-correcting codes, leading to these embedding and sparse\nfactorization methods. The paper establishes a direct connection between DNN\narchitecture and error-correcting coding by demonstrating how state-of-the-art\narchitectures (ChordMixer, Mega, Mega-chunk, CDIL, ...) from the long-range\narena can be equivalent to of block and convolutional LDPC codes (Cage-graph,\nRepeat Accumulate). QC codes correspond to certain types of chemical elements,\nwith the carbon element being represented by the mixed automorphism\nShu-Lin-Fossorier QC-LDPC code. The connections between Belief Propagation and\nthe Permanent, Bethe-Permanent, Nishimori Temperature, and Bethe-Hessian Matrix\nare elaborated upon in detail. The Quantum Approximate Optimization Algorithm\n(QAOA) used in the Sherrington-Kirkpatrick Ising model can be seen as analogous\nto the back-propagation loss function landscape in training DNNs. This\nsimilarity creates a comparable problem with TS pseudo-codeword, resembling the\nbelief propagation method. Additionally, the layer depth in QAOA correlates to\nthe number of decoding belief propagation iterations in the Wiberg decoding\ntree. Overall, this work has the potential to advance multiple fields, from\nInformation Theory, DNN architecture design (sparse and structured prior graph\ntopology), efficient hardware design for Quantum and Classical DPU/TPU (graph,\nquantize and shift register architect.) to Materials Science and beyond.\n",
        "pdfLink": "http://arxiv.org/pdf/2307.15778.pdf",
        "authors": [
            "Usatyuk Vasiliy",
            "Egorov Sergey",
            "Sapozhnikov Denis"
        ],
        "metaData": {
            "relevancy": 0.3002145379781723
        }
    },
    {
        "id": "2308.02219",
        "title": "Federated Learning: Organizational Opportunities, Challenges, and\n  Adoption Strategies",
        "abstract": "  Restrictive rules for data sharing in many industries have led to the\ndevelopment of federated learning. Federated learning is a machine-learning\ntechnique that allows distributed clients to train models collaboratively\nwithout the need to share their respective training data with others. In this\npaper, we first explore the technical foundations of federated learning and its\norganizational opportunities. Second, we present a conceptual framework for the\nadoption of federated learning, mapping four types of organizations by their\nartificial intelligence capabilities and limits to data sharing. We then\ndiscuss why exemplary organizations in different contexts - including public\nauthorities, financial service providers, manufacturing companies, as well as\nresearch and development consortia - might consider different approaches to\nfederated learning. To conclude, we argue that federated learning presents\norganizational challenges with ample interdisciplinary opportunities for\ninformation systems researchers.\n",
        "pdfLink": "http://arxiv.org/pdf/2308.02219.pdf",
        "authors": [
            "Fernandez Joaquin Delgado",
            "Brennecke Martin",
            "Barbereau Tom",
            "Rieger Alexander",
            "Fridgen Gilbert"
        ],
        "metaData": {
            "relevancy": 0.2913378655910492
        }
    },
    {
        "id": "2308.09520",
        "title": "Proceedings of the 2nd International Workshop on Adaptive Cyber Defense",
        "abstract": "  The 2nd International Workshop on Adaptive Cyber Defense was held at the\nFlorida Institute of Technology, Florida. This workshop was organized to share\nresearch that explores unique applications of Artificial Intelligence (AI) and\nMachine Learning (ML) as foundational capabilities for the pursuit of adaptive\ncyber defense. The cyber domain cannot currently be reliably and effectively\ndefended without extensive reliance on human experts. Skilled cyber defenders\nare in short supply and often cannot respond fast enough to cyber threats.\n  Building on recent advances in AI and ML the Cyber defense research community\nhas been motivated to develop new dynamic and sustainable defenses through the\nadoption of AI and ML techniques to cyber settings. Bridging critical gaps\nbetween AI and Cyber researchers and practitioners can accelerate efforts to\ncreate semi-autonomous cyber defenses that can learn to recognize and respond\nto cyber attacks or discover and mitigate weaknesses in cooperation with other\ncyber operation systems and human experts. Furthermore, these defenses are\nexpected to be adaptive and able to evolve over time to thwart changes in\nattacker behavior, changes in the system health and readiness, and natural\nshifts in user behavior over time.\n  The workshop was comprised of invited keynote talks, technical presentations\nand a panel discussion about how AI/ML can enable autonomous mitigation of\ncurrent and future cyber attacks. Workshop submissions were peer reviewed by a\npanel of domain experts with a proceedings consisting of six technical articles\nexploring challenging problems of critical importance to national and global\nsecurity. Participation in this workshop offered new opportunities to stimulate\nresearch and innovation in the emerging domain of adaptive and autonomous cyber\ndefense.\n",
        "pdfLink": "http://arxiv.org/pdf/2308.09520.pdf",
        "authors": [
            "Carvalho Marco",
            "Marriott Damian",
            "Bilinski Mark",
            "Ridley Ahmad"
        ],
        "metaData": {
            "relevancy": 0.3809628367424011
        }
    },
    {
        "id": "2308.10807",
        "title": "DynED: Dynamic Ensemble Diversification in Data Stream Classification",
        "abstract": "  Ensemble methods are commonly used in classification due to their remarkable\nperformance. Achieving high accuracy in a data stream environment is a\nchallenging task considering disruptive changes in the data distribution, also\nknown as concept drift. A greater diversity of ensemble components is known to\nenhance prediction accuracy in such settings. Despite the diversity of\ncomponents within an ensemble, not all contribute as expected to its overall\nperformance. This necessitates a method for selecting components that exhibit\nhigh performance and diversity. We present a novel ensemble construction and\nmaintenance approach based on MMR (Maximal Marginal Relevance) that dynamically\ncombines the diversity and prediction accuracy of components during the process\nof structuring an ensemble. The experimental results on both four real and 11\nsynthetic datasets demonstrate that the proposed approach (DynED) provides a\nhigher average mean accuracy compared to the five state-of-the-art baselines.\n",
        "pdfLink": "http://arxiv.org/pdf/2308.10807.pdf",
        "authors": [
            "Abadifard Soheil",
            "Bakhshi Sepehr",
            "Gheibuni Sanaz",
            "Can Fazli"
        ],
        "metaData": {
            "relevancy": 0.265998899936676
        }
    },
    {
        "id": "2308.14360",
        "title": "InstructME: An Instruction Guided Music Edit And Remix Framework with\n  Latent Diffusion Models",
        "abstract": "  Music editing primarily entails the modification of instrument tracks or\nremixing in the whole, which offers a novel reinterpretation of the original\npiece through a series of operations. These music processing methods hold\nimmense potential across various applications but demand substantial expertise.\nPrior methodologies, although effective for image and audio modifications,\nfalter when directly applied to music. This is attributed to music's\ndistinctive data nature, where such methods can inadvertently compromise the\nintrinsic harmony and coherence of music. In this paper, we develop InstructME,\nan Instruction guided Music Editing and remixing framework based on latent\ndiffusion models. Our framework fortifies the U-Net with multi-scale\naggregation in order to maintain consistency before and after editing. In\naddition, we introduce chord progression matrix as condition information and\nincorporate it in the semantic space to improve melodic harmony while editing.\nFor accommodating extended musical pieces, InstructME employs a chunk\ntransformer, enabling it to discern long-term temporal dependencies within\nmusic sequences. We tested InstructME in instrument-editing, remixing, and\nmulti-round editing. Both subjective and objective evaluations indicate that\nour proposed method significantly surpasses preceding systems in music quality,\ntext relevance and harmony. Demo samples are available at\nhttps://musicedit.github.io/\n",
        "pdfLink": "http://arxiv.org/pdf/2308.14360.pdf",
        "authors": [
            "Han Bing",
            "Dai Junyu",
            "Song Xuchen",
            "Hao Weituo",
            "He Xinyan",
            "Guo Dong",
            "Chen Jitong",
            "Wang Yuxuan",
            "Qian Yanmin"
        ],
        "metaData": {
            "relevancy": 0.34724586009979247
        }
    },
    {
        "id": "2308.14781",
        "title": "Conflict-Aware Active Automata Learning (Extended Version)",
        "abstract": "  Active automata learning algorithms cannot easily handle conflict in the\nobservation data (different outputs observed for the same inputs). This\ninherent inability to recover after a conflict impairs their effective\napplicability in scenarios where noise is present or the system under learning\nis mutating.\n  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to\nenable handling conflicting information during the learning process. The core\nidea is to consider the so-called observation tree as a first-class citizen in\nthe learning process. Though this idea is explored in recent work, we take it\nto its full effect by enabling its use with any existing learner and minimizing\nthe number of tests performed on the system under learning, specially in the\nface of conflicts. We evaluate C3AL in a large set of benchmarks, covering over\n30 different realistic targets, and over 18,000 different scenarios. The\nresults of the evaluation show that C3AL is a suitable alternative framework\nfor closed-box learning that can better handle noise and mutations.\n",
        "pdfLink": "http://arxiv.org/pdf/2308.14781.pdf",
        "authors": [
            "Ferreira Tiago",
            "Henry L\u00e9o",
            "da Silva Raquel Fernandes",
            "Silva Alexandra"
        ],
        "metaData": {
            "relevancy": 0.4166871070861816
        }
    },
    {
        "id": "2308.16741",
        "title": "Socratis: Are large multimodal models emotionally aware?",
        "abstract": "  Existing emotion prediction benchmarks contain coarse emotion labels which do\nnot consider the diversity of emotions that an image and text can elicit in\nhumans due to various reasons. Learning diverse reactions to multimodal content\nis important as intelligent machines take a central role in generating and\ndelivering content to society. To address this gap, we propose Socratis, a\nsocietal reactions benchmark, where each image-caption (IC) pair is annotated\nwith multiple emotions and the reasons for feeling them. Socratis contains 18K\nfree-form reactions for 980 emotions on 2075 image-caption pairs from 5\nwidely-read news and image-caption (IC) datasets. We benchmark the capability\nof state-of-the-art multimodal large language models to generate the reasons\nfor feeling an emotion given an IC pair. Based on a preliminary human study, we\nobserve that humans prefer human-written reasons over 2 times more often than\nmachine-generated ones. This shows our task is harder than standard generation\ntasks because it starkly contrasts recent findings where humans cannot tell\napart machine vs human-written news articles, for instance. We further see that\ncurrent captioning metrics based on large vision-language models also fail to\ncorrelate with human preferences. We hope that these findings and our benchmark\nwill inspire further research on training emotionally aware models.\n",
        "pdfLink": "http://arxiv.org/pdf/2308.16741.pdf",
        "authors": [
            "Deng Katherine",
            "Ray Arijit",
            "Tan Reuben",
            "Gabriel Saadia",
            "Plummer Bryan A.",
            "Saenko Kate"
        ],
        "metaData": {
            "relevancy": 0.5049201488494873
        }
    },
    {
        "id": "2308.16781",
        "title": "StratMed: Relevance Stratification for Low-resource Medication\n  Recommendation",
        "abstract": "  With the growing imbalance between limited medical resources and escalating\ndemands, AI-based clinical tasks have become paramount. Medication\nrecommendation, as a sub-domain, aims to amalgamate longitudinal patient\nhistory with medical knowledge, assisting physicians in prescribing safer and\nmore accurate medication combinations. Existing methods overlook the inherent\nlong-tail distribution in medical data, lacking balanced representation between\nhead and tail data, which leads to sub-optimal model performance. To address\nthis challenge, we introduce StratMed, a model that incorporates an innovative\nrelevance stratification mechanism. It harmonizes discrepancies in data\nlong-tail distribution and strikes a balance between the safety and accuracy of\nmedication combinations. Specifically, we first construct a pre-training method\nusing deep learning networks to obtain entity representation. After that, we\ndesign a pyramid-like data stratification method to obtain more generalized\nentity relationships by reinforcing the features of unpopular entities. Based\non this relationship, we designed two graph structures to express medication\nprecision and safety at the same level to obtain visit representations.\nFinally, the patient's historical clinical information is fitted to generate\nmedication combinations for the current health condition. Experiments on the\nMIMIC-III dataset demonstrate that our method has outperformed current\nstate-of-the-art methods in four evaluation metrics (including safety and\naccuracy).\n",
        "pdfLink": "http://arxiv.org/pdf/2308.16781.pdf",
        "authors": [
            "Li Xiang",
            "Liang Shunpan",
            "Ma Tengfei",
            "Hou Yulei"
        ],
        "metaData": {
            "relevancy": 0.48644517064094545
        }
    },
    {
        "id": "2309.00424",
        "title": "Learning Speech Representation From Contrastive Token-Acoustic\n  Pretraining",
        "abstract": "  For fine-grained generation and recognition tasks such as\nminimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic\nspeech recognition (ASR), the intermediate representations extracted from\nspeech should serve as a \"bridge\" between text and acoustic information,\ncontaining information from both modalities. The semantic content is\nemphasized, while the paralinguistic information such as speaker identity and\nacoustic details should be de-emphasized. However, existing methods for\nextracting fine-grained intermediate representations from speech suffer from\nissues of excessive redundancy and dimension explosion. Contrastive learning is\na good method for modeling intermediate representations from two modalities.\nHowever, existing contrastive learning methods in the audio field focus on\nextracting global descriptive information for downstream audio classification\ntasks, making them unsuitable for TTS, VC, and ASR tasks. To address these\nissues, we propose a method named \"Contrastive Token-Acoustic Pretraining\n(CTAP)\", which uses two encoders to bring phoneme and speech into a joint\nmultimodal space, learning how to connect phoneme and speech at the frame\nlevel. The CTAP model is trained on 210k speech and phoneme text pairs,\nachieving minimally-supervised TTS, VC, and ASR. The proposed CTAP method\noffers a promising solution for fine-grained generation and recognition\ndownstream tasks in speech processing.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.00424.pdf",
        "authors": [
            "Qiang Chunyu",
            "Li Hao",
            "Tian Yixin",
            "Fu Ruibo",
            "Wang Tao",
            "Wang Longbiao",
            "Dang Jianwu"
        ],
        "metaData": {
            "relevancy": 0.4354353129863739
        }
    },
    {
        "id": "2309.01069",
        "title": "Separable Hamiltonian Neural Networks",
        "abstract": "  The modelling of dynamical systems from discrete observations is a challenge\nfaced by modern scientific and engineering data systems. Hamiltonian systems\nare one such fundamental and ubiquitous class of dynamical systems. Hamiltonian\nneural networks are state-of-the-art models that unsupervised-ly regress the\nHamiltonian of a dynamical system from discrete observations of its vector\nfield under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics\nare often complicated, especially in higher dimensions where the state space of\nthe Hamiltonian system is large relative to the number of samples. A recently\ndiscovered remedy to alleviate the complexity between state variables in the\nstate space is to leverage the additive separability of the Hamiltonian system\nand embed that additive separability into the Hamiltonian neural network.\nFollowing the nomenclature of physics-informed machine learning, we propose\nthree separable Hamiltonian neural networks. These models embed additive\nseparability within Hamiltonian neural networks. The first model uses additive\nseparability to quadratically scale the amount of data for training Hamiltonian\nneural networks. The second model embeds additive separability within the loss\nfunction of the Hamiltonian neural network. The third model embeds additive\nseparability through the architecture of the Hamiltonian neural network using\nconjoined multilayer perceptions. We empirically compare the three models\nagainst state-of-the-art Hamiltonian neural networks, and demonstrate that the\nseparable Hamiltonian neural networks, which alleviate complexity between the\nstate variables, are more effective at regressing the Hamiltonian and its\nvector field.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.01069.pdf",
        "authors": [
            "Khoo Zi-Yu",
            "Low Jonathan Sze Choong",
            "Bressan St\u00e9phane"
        ],
        "metaData": {
            "relevancy": 0.3144713997840881
        }
    },
    {
        "id": "2309.01365",
        "title": "Refined Temporal Pyramidal Compression-and-Amplification Transformer for\n  3D Human Pose Estimation",
        "abstract": "  Accurately estimating the 3D pose of humans in video sequences requires both\naccuracy and a well-structured architecture. With the success of transformers,\nwe introduce the Refined Temporal Pyramidal Compression-and-Amplification\n(RTPCA) transformer. Exploiting the temporal dimension, RTPCA extends\nintra-block temporal modeling via its Temporal Pyramidal\nCompression-and-Amplification (TPCA) structure and refines inter-block feature\ninteraction with a Cross-Layer Refinement (XLR) module. In particular, TPCA\nblock exploits a temporal pyramid paradigm, reinforcing key and value\nrepresentation capabilities and seamlessly extracting spatial semantics from\nmotion sequences. We stitch these TPCA blocks with XLR that promotes rich\nsemantic representation through continuous interaction of queries, keys, and\nvalues. This strategy embodies early-stage information with current flows,\naddressing typical deficits in detail and stability seen in other\ntransformer-based methods. We demonstrate the effectiveness of RTPCA by\nachieving state-of-the-art results on Human3.6M, HumanEva-I, and MPI-INF-3DHP\nbenchmarks with minimal computational overhead. The source code is available at\nhttps://github.com/hbing-l/RTPCA.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.01365.pdf",
        "authors": [
            "Liu Hanbing",
            "Xiang Wangmeng",
            "He Jun-Yan",
            "Cheng Zhi-Qi",
            "Luo Bin",
            "Geng Yifeng",
            "Xie Xuansong"
        ],
        "metaData": {
            "relevancy": 0.3297164738178253
        }
    },
    {
        "id": "2309.01507",
        "title": "Memory Efficient Optimizers with 4-bit States",
        "abstract": "  Optimizer states are a major source of memory consumption for training neural\nnetworks, limiting the maximum trainable model within given memory budget.\nCompressing the optimizer states from 32-bit floating points to lower bitwidth\nis promising to reduce the training memory footprint, while the current lowest\nachievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth\ndown to 4-bit through a detailed empirical analysis of first and second\nmoments. Specifically, we find that moments have complicated outlier patterns,\nthat current block-wise quantization cannot accurately approximate. We use a\nsmaller block size and propose to utilize both row-wise and column-wise\ninformation for better quantization. We further identify a zero point problem\nof quantizing the second moment, and solve this problem with a linear quantizer\nthat excludes the zero point. Our 4-bit optimizer is evaluated on a wide\nvariety of benchmarks including natural language understanding, machine\ntranslation, image classification, and instruction tuning. On all the tasks our\noptimizers can achieve comparable accuracy with their full-precision\ncounterparts, while enjoying better memory efficiency.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.01507.pdf",
        "authors": [
            "Li Bingrui",
            "Chen Jianfei",
            "Zhu Jun"
        ],
        "metaData": {
            "relevancy": 0.30462283492088316
        }
    },
    {
        "id": "2309.01793",
        "title": "Neural-Singular-Hessian: Implicit Neural Representation of Unoriented\n  Point Clouds by Enforcing Singular Hessian",
        "abstract": "  Neural implicit representation is a promising approach for reconstructing\nsurfaces from point clouds. Existing methods combine various regularization\nterms, such as the Eikonal and Laplacian energy terms, to enforce the learned\nneural function to possess the properties of a Signed Distance Function (SDF).\nHowever, inferring the actual topology and geometry of the underlying surface\nfrom poor-quality unoriented point clouds remains challenging. In accordance\nwith Differential Geometry, the Hessian of the SDF is singular for points\nwithin the differential thin-shell space surrounding the surface. Our approach\nenforces the Hessian of the neural implicit function to have a zero determinant\nfor points near the surface. This technique aligns the gradients for a\nnear-surface point and its on-surface projection point, producing a rough but\nfaithful shape within just a few iterations. By annealing the weight of the\nsingular-Hessian term, our approach ultimately produces a high-fidelity\nreconstruction result. Extensive experimental results demonstrate that our\napproach effectively suppresses ghost geometry and recovers details from\nunoriented point clouds with better expressiveness than existing fitting-based\nmethods.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.01793.pdf",
        "authors": [
            "Wang Zixiong",
            "Zhang Yunxiao",
            "Xu Rui",
            "Zhang Fan",
            "Wang Pengshuai",
            "Chen Shuangmin",
            "Xin Shiqing",
            "Wang Wenping",
            "Tu Changhe"
        ],
        "metaData": {
            "relevancy": 0.26319690644741056
        }
    },
    {
        "id": "2309.01866",
        "title": "Efficient Query-Based Attack against ML-Based Android Malware Detection\n  under Zero Knowledge Setting",
        "abstract": "  The widespread adoption of the Android operating system has made malicious\nAndroid applications an appealing target for attackers. Machine learning-based\n(ML-based) Android malware detection (AMD) methods are crucial in addressing\nthis problem; however, their vulnerability to adversarial examples raises\nconcerns. Current attacks against ML-based AMD methods demonstrate remarkable\nperformance but rely on strong assumptions that may not be realistic in\nreal-world scenarios, e.g., the knowledge requirements about feature space,\nmodel parameters, and training dataset. To address this limitation, we\nintroduce AdvDroidZero, an efficient query-based attack framework against\nML-based AMD methods that operates under the zero knowledge setting. Our\nextensive evaluation shows that AdvDroidZero is effective against various\nmainstream ML-based AMD methods, in particular, state-of-the-art such methods\nand real-world antivirus solutions.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.01866.pdf",
        "authors": [
            "He Ping",
            "Xia Yifan",
            "Zhang Xuhong",
            "Ji Shouling"
        ],
        "metaData": {
            "relevancy": 0.25753611922264097
        }
    },
    {
        "id": "2309.02232",
        "title": "FSD: An Initial Chinese Dataset for Fake Song Detection",
        "abstract": "  Singing voice synthesis and singing voice conversion have significantly\nadvanced, revolutionizing musical experiences. However, the rise of \"Deepfake\nSongs\" generated by these technologies raises concerns about authenticity.\nUnlike Audio DeepFake Detection (ADD), the field of song deepfake detection\nlacks specialized datasets or methods for song authenticity verification. In\nthis paper, we initially construct a Chinese Fake Song Detection (FSD) dataset\nto investigate the field of song deepfake detection. The fake songs in the FSD\ndataset are generated by five state-of-the-art singing voice synthesis and\nsinging voice conversion methods. Our initial experiments on FSD revealed the\nineffectiveness of existing speech-trained ADD models for the task of song\ndeepFake detection. Thus, we employ the FSD dataset for the training of ADD\nmodels. We subsequently evaluate these models under two scenarios: one with the\noriginal songs and another with separated vocal tracks. Experiment results show\nthat song-trained ADD models exhibit a 38.58% reduction in average equal error\nrate compared to speech-trained ADD models on the FSD test set.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02232.pdf",
        "authors": [
            "Xie Yuankun",
            "Zhou Jingjing",
            "Lu Xiaolin",
            "Jiang Zhenghao",
            "Yang Yuxin",
            "Cheng Haonan",
            "Ye Long"
        ],
        "metaData": {
            "relevancy": 0.3001240909099579
        }
    },
    {
        "id": "2309.02332",
        "title": "Information Processing by Neuron Populations in the Central Nervous\n  System: Mathematical Structure of Data and Operations",
        "abstract": "  In the intricate architecture of the mammalian central nervous system,\nneurons form populations. Axonal bundles communicate between these clusters\nusing spike trains as their medium. However, these neuron populations' precise\nencoding and operations have yet to be discovered. In our analysis, the\nstarting point is a state-of-the-art mechanistic model of a generic neuron\nendowed with plasticity. From this simple framework emerges a profound\nmathematical construct: The representation and manipulation of information can\nbe precisely characterized by an algebra of finite convex cones. Furthermore,\nthese neuron populations are not merely passive transmitters. They act as\noperators within this algebraic structure, mirroring the functionality of a\nlow-level programming language. When these populations interconnect, they\nembody succinct yet potent algebraic expressions. These networks allow them to\nimplement many operations, such as specialization, generalization, novelty\ndetection, dimensionality reduction, inverse modeling, prediction, and\nassociative memory. In broader terms, this work illuminates the potential of\nmatrix embeddings in advancing our understanding in fields like cognitive\nscience and AI. These embeddings enhance the capacity for concept processing\nand hierarchical description over their vector counterparts.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02332.pdf",
        "authors": [
            "Nilsson Martin N. P."
        ],
        "metaData": {
            "relevancy": 0.36705095171928404
        }
    },
    {
        "id": "2309.02442",
        "title": "Observe Locally, Classify Globally: Using GNNs to Identify Sparse Matrix\n  Structure",
        "abstract": "  The performance of sparse matrix computation highly depends on the matching\nof the matrix format with the underlying structure of the data being computed\non. Different sparse matrix formats are suitable for different structures of\ndata. Therefore, the first challenge is identifying the matrix structure before\nthe computation to match it with an appropriate data format. The second\nchallenge is to avoid reading the entire dataset before classifying it. This\ncan be done by identifying the matrix structure through samples and their\nfeatures. Yet, it is possible that global features cannot be determined from a\nsampling set and must instead be inferred from local features. To address these\nchallenges, we develop a framework that generates sparse matrix structure\nclassifiers using graph convolutional networks. The framework can also be\nextended to other matrix structures using user-provided generators. The\napproach achieves 97% classification accuracy on a set of representative sparse\nmatrix shapes.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02442.pdf",
        "authors": [
            "Abdelaal Khaled",
            "Veras Richard"
        ],
        "metaData": {
            "relevancy": 0.3378855586051941
        }
    },
    {
        "id": "2309.02458",
        "title": "Towards frugal unsupervised detection of subtle abnormalities in medical\n  imaging",
        "abstract": "  Anomaly detection in medical imaging is a challenging task in contexts where\nabnormalities are not annotated. This problem can be addressed through\nunsupervised anomaly detection (UAD) methods, which identify features that do\nnot match with a reference model of normal profiles. Artificial neural networks\nhave been extensively used for UAD but they do not generally achieve an optimal\ntrade-o$\\hookleftarrow$ between accuracy and computational demand. As an\nalternative, we investigate mixtures of probability distributions whose\nversatility has been widely recognized for a variety of data and tasks, while\nnot requiring excessive design e$\\hookleftarrow$ort or tuning. Their\nexpressivity makes them good candidates to account for complex multivariate\nreference models. Their much smaller number of parameters makes them more\namenable to interpretation and e cient learning. However, standard estimation\nprocedures, such as the Expectation-Maximization algorithm, do not scale well\nto large data volumes as they require high memory usage. To address this issue,\nwe propose to incrementally compute inferential quantities. This online\napproach is illustrated on the challenging detection of subtle abnormalities in\nMR brain scans for the follow-up of newly diagnosed Parkinsonian patients. The\nidentified structural abnormalities are consistent with the disease\nprogression, as accounted by the Hoehn and Yahr scale.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02458.pdf",
        "authors": [
            "Oudoumanessah Geoffroy",
            "Lartizien Carole",
            "Dojat Michel",
            "Forbes Florence"
        ],
        "metaData": {
            "relevancy": 0.3394071817398071
        }
    },
    {
        "id": "2309.02460",
        "title": "Effective Multi-Graph Neural Networks for Illicit Account Detection on\n  Cryptocurrency Transaction Networks",
        "abstract": "  We study illicit account detection on transaction networks of\ncryptocurrencies that are increasi_testngly important in online financial\nmarkets. The surge of illicit activities on cryptocurrencies has resulted in\nbillions of losses from normal users. Existing solutions either rely on tedious\nfeature engineering to get handcrafted features, or are inadequate to fully\nutilize the rich semantics of cryptocurrency transaction data, and\nconsequently, yield sub-optimal performance. In this paper, we formulate the\nillicit account detection problem as a classification task over directed\nmultigraphs with edge attributes, and present DIAM, a novel multi-graph neural\nnetwork model to effectively detect illicit accounts on large transaction\nnetworks. First, DIAM includes an Edge2Seq module that automatically learns\neffective node representations preserving intrinsic transaction patterns of\nparallel edges, by considering both edge attributes and directed edge sequence\ndependencies. Then utilizing the multigraph topology, DIAM employs a new\nMultigraph Discrepancy (MGD) module with a well-designed message passing\nmechanism to capture the discrepant features between normal and illicit nodes,\nsupported by an attention mechanism. Assembling all techniques, DIAM is trained\nin an end-to-end manner. Extensive experiments, comparing against 14 existing\nsolutions on 4 large cryptocurrency datasets of Bitcoin and Ethereum,\ndemonstrate that DIAM consistently achieves the best performance to accurately\ndetect illicit accounts, while being efficient. For instance, on a Bitcoin\ndataset with 20 million nodes and 203 million edges, DIAM achieves F1 score\n96.55%, significantly higher than the F1 score 83.92% of the best competitor.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02460.pdf",
        "authors": [
            "Ding Zhihao",
            "Shi Jieming",
            "Li Qing",
            "Cao Jiannong"
        ],
        "metaData": {
            "relevancy": 0.3432795822620392
        }
    },
    {
        "id": "2309.02465",
        "title": "Towards Foundational AI Models for Additive Manufacturing: Language\n  Models for G-Code Debugging, Manipulation, and Comprehension",
        "abstract": "  3D printing or additive manufacturing is a revolutionary technology that\nenables the creation of physical objects from digital models. However, the\nquality and accuracy of 3D printing depend on the correctness and efficiency of\nthe G-code, a low-level numerical control programming language that instructs\n3D printers how to move and extrude material. Debugging G-code is a challenging\ntask that requires a syntactic and semantic understanding of the G-code format\nand the geometry of the part to be printed. In this paper, we present the first\nextensive evaluation of six state-of-the-art foundational large language models\n(LLMs) for comprehending and debugging G-code files for 3D printing. We design\neffective prompts to enable pre-trained LLMs to understand and manipulate\nG-code and test their performance on various aspects of G-code debugging and\nmanipulation, including detection and correction of common errors and the\nability to perform geometric transformations. We analyze their strengths and\nweaknesses for understanding complete G-code files. We also discuss the\nimplications and limitations of using LLMs for G-code comprehension.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02465.pdf",
        "authors": [
            "Jignasu Anushrut",
            "Marshall Kelly",
            "Ganapathysubramanian Baskar",
            "Balu Aditya",
            "Hegde Chinmay",
            "Krishnamurthy Adarsh"
        ],
        "metaData": {
            "relevancy": 0.3678528964519501
        }
    },
    {
        "id": "2309.02473",
        "title": "A Survey of Imitation Learning: Algorithms, Recent Developments, and\n  Challenges",
        "abstract": "  In recent years, the development of robotics and artificial intelligence (AI)\nsystems has been nothing short of remarkable. As these systems continue to\nevolve, they are being utilized in increasingly complex and unstructured\nenvironments, such as autonomous driving, aerial robotics, and natural language\nprocessing. As a consequence, programming their behaviors manually or defining\ntheir behavior through reward functions (as done in reinforcement learning\n(RL)) has become exceedingly difficult. This is because such environments\nrequire a high degree of flexibility and adaptability, making it challenging to\nspecify an optimal set of rules or reward signals that can account for all\npossible situations. In such environments, learning from an expert's behavior\nthrough imitation is often more appealing. This is where imitation learning\n(IL) comes into play - a process where desired behavior is learned by imitating\nan expert's behavior, which is provided through demonstrations.\n  This paper aims to provide an introduction to IL and an overview of its\nunderlying assumptions and approaches. It also offers a detailed description of\nrecent advances and emerging areas of research in the field. Additionally, the\npaper discusses how researchers have addressed common challenges associated\nwith IL and provides potential directions for future research. Overall, the\ngoal of the paper is to provide a comprehensive guide to the growing field of\nIL in robotics and AI.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02473.pdf",
        "authors": [
            "Zare Maryam",
            "Kebria Parham M.",
            "Khosravi Abbas",
            "Nahavandi Saeid"
        ],
        "metaData": {
            "relevancy": 0.5612025201320648
        }
    },
    {
        "id": "2309.02478",
        "title": "Enhancing Semantic Communication with Deep Generative Models -- An\n  ICASSP Special Session Overview",
        "abstract": "  Semantic communication is poised to play a pivotal role in shaping the\nlandscape of future AI-driven communication systems. Its challenge of\nextracting semantic information from the original complex content and\nregenerating semantically consistent data at the receiver, possibly being\nrobust to channel corruptions, can be addressed with deep generative models.\nThis ICASSP special session overview paper discloses the semantic communication\nchallenges from the machine learning perspective and unveils how deep\ngenerative models will significantly enhance semantic communication frameworks\nin dealing with real-world complex data, extracting and exploiting semantic\ninformation, and being robust to channel corruptions. Alongside establishing\nthis emerging field, this paper charts novel research pathways for the next\ngenerative semantic communication frameworks.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02478.pdf",
        "authors": [
            "Grassucci Eleonora",
            "Mitsufuji Yuki",
            "Zhang Ping",
            "Comminiello Danilo"
        ],
        "metaData": {
            "relevancy": 0.5006729245185852
        }
    },
    {
        "id": "2309.02524",
        "title": "Do You Trust ChatGPT? -- Perceived Credibility of Human and AI-Generated\n  Content",
        "abstract": "  This paper examines how individuals perceive the credibility of content\noriginating from human authors versus content generated by large language\nmodels, like the GPT language model family that powers ChatGPT, in different\nuser interface versions. Surprisingly, our results demonstrate that regardless\nof the user interface presentation, participants tend to attribute similar\nlevels of credibility. While participants also do not report any different\nperceptions of competence and trustworthiness between human and AI-generated\ncontent, they rate AI-generated content as being clearer and more engaging. The\nfindings from this study serve as a call for a more discerning approach to\nevaluating information sources, encouraging users to exercise caution and\ncritical thinking when engaging with content generated by AI systems.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02524.pdf",
        "authors": [
            "Huschens Martin",
            "Briesch Martin",
            "Sobania Dominik",
            "Rothlauf Franz"
        ],
        "metaData": {
            "relevancy": 0.4296627104282379
        }
    },
    {
        "id": "2309.02534",
        "title": "Experience and Prediction: A Metric of Hardness for a Novel Litmus Test",
        "abstract": "  In the last decade, the Winograd Schema Challenge (WSC) has become a central\naspect of the research community as a novel litmus test. Consequently, the WSC\nhas spurred research interest because it can be seen as the means to understand\nhuman behavior. In this regard, the development of new techniques has made\npossible the usage of Winograd schemas in various fields, such as the design of\nnovel forms of CAPTCHAs.\n  Work from the literature that established a baseline for human adult\nperformance on the WSC has shown that not all schemas are the same, meaning\nthat they could potentially be categorized according to their perceived\nhardness for humans. In this regard, this \\textit{hardness-metric} could be\nused in future challenges or in the WSC CAPTCHA service to differentiate\nbetween Winograd schemas.\n  Recent work of ours has shown that this could be achieved via the design of\nan automated system that is able to output the hardness-indexes of Winograd\nschemas, albeit with limitations regarding the number of schemas it could be\napplied on. This paper adds to previous research by presenting a new system\nthat is based on Machine Learning (ML), able to output the hardness of any\nWinograd schema faster and more accurately than any other previously used\nmethod. Our developed system, which works within two different approaches,\nnamely the random forest and deep learning (LSTM-based), is ready to be used as\nan extension of any other system that aims to differentiate between Winograd\nschemas, according to their perceived hardness for humans. At the same time,\nalong with our developed system we extend previous work by presenting the\nresults of a large-scale experiment that shows how human performance varies\nacross Winograd schemas.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02534.pdf",
        "authors": [
            "Isaak Nicos",
            "Michael Loizos"
        ],
        "metaData": {
            "relevancy": 0.3582306385040283
        }
    },
    {
        "id": "2309.02547",
        "title": "Structural Concept Learning via Graph Attention for Multi-Level\n  Rearrangement Planning",
        "abstract": "  Robotic manipulation tasks, such as object rearrangement, play a crucial role\nin enabling robots to interact with complex and arbitrary environments.\nExisting work focuses primarily on single-level rearrangement planning and,\neven if multiple levels exist, dependency relations among substructures are\ngeometrically simpler, like tower stacking. We propose Structural Concept\nLearning (SCL), a deep learning approach that leverages graph attention\nnetworks to perform multi-level object rearrangement planning for scenes with\nstructural dependency hierarchies. It is trained on a self-generated simulation\ndata set with intuitive structures, works for unseen scenes with an arbitrary\nnumber of objects and higher complexity of structures, infers independent\nsubstructures to allow for task parallelization over multiple manipulators, and\ngeneralizes to the real world. We compare our method with a range of classical\nand model-based baselines to show that our method leverages its scene\nunderstanding to achieve better performance, flexibility, and efficiency. The\ndataset, supplementary details, videos, and code implementation are available\nat: https://manavkulshrestha.github.io/scl\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02547.pdf",
        "authors": [
            "Kulshrestha Manav",
            "Qureshi Ahmed H."
        ],
        "metaData": {
            "relevancy": 0.4542501628398895
        }
    },
    {
        "id": "2309.02551",
        "title": "Continual Improvement of Threshold-Based Novelty Detection",
        "abstract": "  When evaluated in dynamic, open-world situations, neural networks struggle to\ndetect unseen classes. This issue complicates the deployment of continual\nlearners in realistic environments where agents are not explicitly informed\nwhen novel categories are encountered. A common family of techniques for\ndetecting novelty relies on thresholds of similarity between observed data\npoints and the data used for training. However, these methods often require\nmanually specifying (ahead of time) the value of these thresholds, and are\ntherefore incapable of adapting to the nature of the data. We propose a new\nmethod for automatically selecting these thresholds utilizing a linear search\nand leave-one-out cross-validation on the ID classes. We demonstrate that this\nnovel method for selecting thresholds results in improved total accuracy on\nMNIST, Fashion MNIST, and CIFAR-10.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02551.pdf",
        "authors": [
            "Ejilemele Abe",
            "Mendez-Mendez Jorge"
        ],
        "metaData": {
            "relevancy": 0.33916046023368834
        }
    },
    {
        "id": "2309.02562",
        "title": "Recurrence-Free Survival Prediction for Anal Squamous Cell Carcinoma\n  Chemoradiotherapy using Planning CT-based Radiomics Model",
        "abstract": "  Objectives: Approximately 30% of non-metastatic anal squamous cell carcinoma\n(ASCC) patients will experience recurrence after chemoradiotherapy (CRT), and\ncurrently available clinical variables are poor predictors of treatment\nresponse. We aimed to develop a model leveraging information extracted from\nradiation pretreatment planning CT to predict recurrence-free survival (RFS) in\nASCC patients after CRT. Methods: Radiomics features were extracted from\nplanning CT images of 96 ASCC patients. Following pre-feature selection, the\noptimal feature set was selected via step-forward feature selection with a\nmultivariate Cox proportional hazard model. The RFS prediction was generated\nfrom a radiomics-clinical combined model based on an optimal feature set with\nfive repeats of five-fold cross validation. The risk stratification ability of\nthe proposed model was evaluated with Kaplan-Meier analysis. Results: Shape-\nand texture-based radiomics features significantly predicted RFS. Compared to a\nclinical-only model, radiomics-clinical combined model achieves better\nperformance in the testing cohort with higher C-index (0.80 vs 0.73) and AUC\n(0.84 vs 0.79 for 1-year RFS, 0.84 vs 0.78 for 2-year RFS, and 0.86 vs 0.83 for\n3-year RFS), leading to distinctive high- and low-risk of recurrence groups\n(p<0.001). Conclusions: A treatment planning CT based radiomics and clinical\ncombined model had improved prognostic performance in predicting RFS for ASCC\npatients treated with CRT as compared to a model using clinical features only.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02562.pdf",
        "authors": [
            "Tang Shanshan",
            "Wang Kai",
            "Hein David",
            "Lin Gloria",
            "Sanford Nina N.",
            "Wang Jing"
        ],
        "metaData": {
            "relevancy": 0.19580503106117247
        }
    },
    {
        "id": "2309.02564",
        "title": "Diffusion-based Time Series Data Imputation for Microsoft 365",
        "abstract": "  Reliability is extremely important for large-scale cloud systems like\nMicrosoft 365. Cloud failures such as disk failure, node failure, etc. threaten\nservice reliability, resulting in online service interruptions and economic\nloss. Existing works focus on predicting cloud failures and proactively taking\naction before failures happen. However, they suffer from poor data quality like\ndata missing in model training and prediction, which limits the performance. In\nthis paper, we focus on enhancing data quality through data imputation by the\nproposed Diffusion+, a sample-efficient diffusion model, to impute the missing\ndata efficiently based on the observed data. Our experiments and application\npractice show that our model contributes to improving the performance of the\ndownstream failure prediction task.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02564.pdf",
        "authors": [
            "Yang Fangkai",
            "Yin Wenjie",
            "Wang Lu",
            "Li Tianci",
            "Zhao Pu",
            "Liu Bo",
            "Wang Paul",
            "Qiao Bo",
            "Liu Yudong",
            "Bj\u00f6rkman M\u00e5rten",
            "Rajmohan Saravan",
            "Lin Qingwei",
            "Zhang Dongmei"
        ],
        "metaData": {
            "relevancy": 0.2755514234304428
        }
    },
    {
        "id": "2309.02580",
        "title": "Unveiling Intractable Epileptogenic Brain Networks with Deep Learning\n  Algorithms: A Novel and Comprehensive Framework for Scalable Seizure\n  Prediction with Unimodal Neuroimaging Data in Pediatric Patients",
        "abstract": "  Epilepsy is a prevalent neurological disorder affecting 50 million\nindividuals worldwide and 1.2 million Americans. There exist millions of\npediatric patients with intractable epilepsy, a condition in which seizures\nfail to come under control. The occurrence of seizures can result in physical\ninjury, disorientation, unconsciousness, and additional symptoms that could\nimpede children's ability to participate in everyday tasks. Predicting seizures\ncan help parents and healthcare providers take precautions, prevent risky\nsituations, and mentally prepare children to minimize anxiety and nervousness\nassociated with the uncertainty of a seizure. This research proposes a novel\nand comprehensive framework to predict seizures in pediatric patients by\nevaluating machine learning algorithms on unimodal neuroimaging data consisting\nof electroencephalogram signals. The bandpass filtering and independent\ncomponent analysis proved to be effective in reducing the noise and artifacts\nfrom the dataset. Various machine learning algorithms' performance is evaluated\non important metrics such as accuracy, precision, specificity, sensitivity, F1\nscore and MCC. The results show that the deep learning algorithms are more\nsuccessful in predicting seizures than logistic Regression, and k nearest\nneighbors. The recurrent neural network (RNN) gave the highest precision and F1\nScore, long short-term memory (LSTM) outperformed RNN in accuracy and\nconvolutional neural network (CNN) resulted in the highest Specificity. This\nresearch has significant implications for healthcare providers in proactively\nmanaging seizure occurrence in pediatric patients, potentially transforming\nclinical practices, and improving pediatric care.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02580.pdf",
        "authors": [
            "Singhal Bliss",
            "Pooja Fnu"
        ],
        "metaData": {
            "relevancy": 0.28646849989891054
        }
    },
    {
        "id": "2309.02583",
        "title": "Representation Learning for Sequential Volumetric Design Tasks",
        "abstract": "  Volumetric design, also called massing design, is the first and critical step\nin professional building design which is sequential in nature. As the\nvolumetric design process is complex, the underlying sequential design process\nencodes valuable information for designers. Many efforts have been made to\nautomatically generate reasonable volumetric designs, but the quality of the\ngenerated design solutions varies, and evaluating a design solution requires\neither a prohibitively comprehensive set of metrics or expensive human\nexpertise. While previous approaches focused on learning only the final design\ninstead of sequential design tasks, we propose to encode the design knowledge\nfrom a collection of expert or high-performing design sequences and extract\nuseful representations using transformer-based models. Later we propose to\nutilize the learned representations for crucial downstream applications such as\ndesign preference evaluation and procedural design generation. We develop the\npreference model by estimating the density of the learned representations\nwhereas we train an autoregressive transformer model for sequential design\ngeneration. We demonstrate our ideas by leveraging a novel dataset of thousands\nof sequential volumetric designs. Our preference model can compare two\narbitrarily given design sequences and is almost 90% accurate in evaluation\nagainst random design sequences. Our autoregressive model is also capable of\nautocompleting a volumetric design sequence from a partial design sequence.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02583.pdf",
        "authors": [
            "Alam Md Ferdous",
            "Wang Yi",
            "Tran Linh",
            "Cheng Chin-Yi",
            "Luo Jieliang"
        ],
        "metaData": {
            "relevancy": 0.3586651086807251
        }
    },
    {
        "id": "2309.02600",
        "title": "Comparative Evaluation of Metaheuristic Algorithms for Hyperparameter\n  Selection in Short-Term Weather Forecasting",
        "abstract": "  Weather forecasting plays a vital role in numerous sectors, but accurately\ncapturing the complex dynamics of weather systems remains a challenge for\ntraditional statistical models. Apart from Auto Regressive time forecasting\nmodels like ARIMA, deep learning techniques (Vanilla ANNs, LSTM and GRU\nnetworks), have shown promise in improving forecasting accuracy by capturing\ntemporal dependencies. This paper explores the application of metaheuristic\nalgorithms, namely Genetic Algorithm (GA), Differential Evolution (DE), and\nParticle Swarm Optimization (PSO), to automate the search for optimal\nhyperparameters in these model architectures. Metaheuristic algorithms excel in\nglobal optimization, offering robustness, versatility, and scalability in\nhandling non-linear problems. We present a comparative analysis of different\nmodel architectures integrated with metaheuristic optimization, evaluating\ntheir performance in weather forecasting based on metrics such as Mean Squared\nError (MSE) and Mean Absolute Percentage Error (MAPE). The results demonstrate\nthe potential of metaheuristic algorithms in enhancing weather forecasting\naccuracy \\& helps in determining the optimal set of hyper-parameters for each\nmodel. The paper underscores the importance of harnessing advanced optimization\ntechniques to select the most suitable metaheuristic algorithm for the given\nweather forecasting task.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02600.pdf",
        "authors": [
            "Sen Anuvab",
            "Mazumder Arul Rhik",
            "Dutta Dibyarup",
            "Sen Udayon",
            "Syam Pathikrit",
            "Dhar Sandipan"
        ],
        "metaData": {
            "relevancy": 0.29147526025772097
        }
    },
    {
        "id": "2309.02603",
        "title": "Detection of Unknown-Unknowns in Cyber-Physical Systems using\n  Statistical Conformance with Physics Guided Process Models",
        "abstract": "  Unknown unknowns are operational scenarios in a cyber-physical system that\nare not accounted for in the design and test phase. As such under\nunknown-unknown scenarios, the operational behavior of the CPS is not\nguaranteed to meet requirements such as safety and efficacy specified using\nSignal Temporal Logic (STL) on the output trajectories. We propose a novel\nframework for analyzing the stochastic conformance of operational output\ncharacteristics of safety-critical cyber-physical systems that can discover\nunknown-unknown scenarios and evaluate potential safety hazards. We propose\ndynamics-induced hybrid recurrent neural networks (DiH-RNN) to mine a\nphysics-guided surrogate model (PGSM) which is used to check the model\nconformance using STL on the model coefficients. We demonstrate the detection\nof operational changes in an Artificial Pancreas(AP) due to unknown insulin\ncartridge errors.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02603.pdf",
        "authors": [
            "Maity Aranyak",
            "Banerjee Ayan",
            "Gupta Sandeep"
        ],
        "metaData": {
            "relevancy": 0.30975993275642394
        }
    },
    {
        "id": "2309.02614",
        "title": "Utilizing Generative Adversarial Networks for Stable Structure\n  Generation in Angry Birds",
        "abstract": "  This paper investigates the suitability of using Generative Adversarial\nNetworks (GANs) to generate stable structures for the physics-based puzzle game\nAngry Birds. While previous applications of GANs for level generation have been\nmostly limited to tile-based representations, this paper explores their\nsuitability for creating stable structures made from multiple smaller blocks.\nThis includes a detailed encoding/decoding process for converting between Angry\nBirds level descriptions and a suitable grid-based representation, as well as\nutilizing state-of-the-art GAN architectures and training methods to produce\nnew structure designs. Our results show that GANs can be successfully applied\nto generate a varied range of complex and stable Angry Birds structures.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02614.pdf",
        "authors": [
            "Abraham Frederic",
            "Stephenson Matthew"
        ],
        "metaData": {
            "relevancy": 0.30571566224098207
        }
    },
    {
        "id": "2309.02632",
        "title": "Deep Reinforcement Learning from Hierarchical Weak Preference Feedback",
        "abstract": "  Reward design is a fundamental, yet challenging aspect of practical\nreinforcement learning (RL). For simple tasks, researchers typically handcraft\nthe reward function, e.g., using a linear combination of several reward\nfactors. However, such reward engineering is subject to approximation bias,\nincurs large tuning cost, and often cannot provide the granularity required for\ncomplex tasks. To avoid these difficulties, researchers have turned to\nreinforcement learning from human feedback (RLHF), which learns a reward\nfunction from human preferences between pairs of trajectory sequences. By\nleveraging preference-based reward modeling, RLHF learns complex rewards that\nare well aligned with human preferences, allowing RL to tackle increasingly\ndifficult problems. Unfortunately, the applicability of RLHF is limited due to\nthe high cost and difficulty of obtaining human preference data. In light of\nthis cost, we investigate learning reward functions for complex tasks with less\nhuman effort; simply by ranking the importance of the reward factors. More\nspecifically, we propose a new RL framework -- HERON, which compares\ntrajectories using a hierarchical decision tree induced by the given ranking.\nThese comparisons are used to train a preference-based reward model, which is\nthen used for policy learning. We find that our framework can not only train\nhigh performing agents on a variety of difficult tasks, but also provide\nadditional benefits such as improved sample efficiency and robustness. Our code\nis available at https://github.com/abukharin3/HERON.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02632.pdf",
        "authors": [
            "Bukharin Alexander",
            "Li Yixiao",
            "He Pengcheng",
            "Chen Weizhu",
            "Zhao Tuo"
        ],
        "metaData": {
            "relevancy": 0.5293589472770691
        }
    },
    {
        "id": "2309.02641",
        "title": "TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for\n  Failure Prediction",
        "abstract": "  Hard Disk Drive (HDD) failures in datacenters are costly - from catastrophic\ndata loss to a question of goodwill, stakeholders want to avoid it like the\nplague. An important tool in proactively monitoring against HDD failure is\ntimely estimation of the Remaining Useful Life (RUL). To this end, the\nSelf-Monitoring, Analysis and Reporting Technology employed within HDDs\n(S.M.A.R.T.) provide critical logs for long-term maintenance of the security\nand dependability of these essential data storage devices. Data-driven\npredictive models in the past have used these S.M.A.R.T. logs and CNN/RNN based\narchitectures heavily. However, they have suffered significantly in providing a\nconfidence interval around the predicted RUL values as well as in processing\nvery long sequences of logs. In addition, some of these approaches, such as\nthose based on LSTMs, are inherently slow to train and have tedious feature\nengineering overheads. To overcome these challenges, in this work we propose a\nnovel transformer architecture - a Temporal-fusion Bi-encoder Self-attention\nTransformer (TFBEST) for predicting failures in hard-drives. It is an\nencoder-decoder based deep learning technique that enhances the context gained\nfrom understanding health statistics sequences and predicts a sequence of the\nnumber of days remaining before a disk potentially fails. In this paper, we\nalso provide a novel confidence margin statistic that can help manufacturers\nreplace a hard-drive within a time frame. Experiments on Seagate HDD data show\nthat our method significantly outperforms the state-of-the-art RUL prediction\nmethods during testing over the exhaustive 10-year data from Backblaze\n(2013-present). Although validated on HDD failure prediction, the TFBEST\narchitecture is well-suited for other prognostics applications and may be\nadapted for allied regression problems.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02641.pdf",
        "authors": [
            "Mohapatra Rohan",
            "Sengupta Saptarshi"
        ],
        "metaData": {
            "relevancy": 0.33698670864105223
        }
    },
    {
        "id": "2309.02662",
        "title": "Subsethood Measures of Spatial Granules",
        "abstract": "  Subsethood, which is to measure the degree of set inclusion relation, is\npredominant in fuzzy set theory. This paper introduces some basic concepts of\nspatial granules, coarse-fine relation, and operations like meet, join,\nquotient meet and quotient join. All the atomic granules can be hierarchized by\nset-inclusion relation and all the granules can be hierarchized by coarse-fine\nrelation. Viewing an information system from the micro and the macro\nperspectives, we can get a micro knowledge space and a micro knowledge space,\nfrom which a rough set model and a spatial rough granule model are respectively\nobtained. The classical rough set model is the special case of the rough set\nmodel induced from the micro knowledge space, while the spatial rough granule\nmodel will be play a pivotal role in the problem-solving of structures. We\ndiscuss twelve axioms of monotone increasing subsethood and twelve\ncorresponding axioms of monotone decreasing supsethood, and generalize\nsubsethood and supsethood to conditional granularity and conditional fineness\nrespectively. We develop five conditional granularity measures and five\nconditional fineness measures and prove that each conditional granularity or\nfineness measure satisfies its corresponding twelve axioms although its\nsubsethood or supsethood measure only hold one of the two boundary conditions.\nWe further define five conditional granularity entropies and five conditional\nfineness entropies respectively, and each entropy only satisfies part of the\nboundary conditions but all the ten monotone conditions.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02662.pdf",
        "authors": [
            "Zhao Liquan",
            "Yao Yiyu"
        ],
        "metaData": {
            "relevancy": 0.28095628023147584
        }
    },
    {
        "id": "2309.02671",
        "title": "RLSynC: Offline-Online Reinforcement Learning for Synthon Completion",
        "abstract": "  Retrosynthesis is the process of determining the set of reactant molecules\nthat can react to form a desired product. Semi-template-based retrosynthesis\nmethods, which imitate the reverse logic of synthesis reactions, first predict\nthe reaction centers in the products, and then complete the resulting synthons\nback into reactants. These methods enable necessary interpretability and high\npractical utility to inform synthesis planning. We develop a new offline-online\nreinforcement learning method RLSynC for synthon completion in\nsemi-template-based methods. RLSynC assigns one agent to each synthon, all of\nwhich complete the synthons by conducting actions step by step in a\nsynchronized fashion. RLSynC learns the policy from both offline training\nepisodes and online interactions which allow RLSynC to explore new reaction\nspaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the\npredicted reactants in synthesizing a product, and thus guides the action\nsearch. We compare RLSynC with the state-of-the-art retrosynthesis methods. Our\nexperimental results demonstrate that RLSynC can outperform these methods with\nimprovement as high as 14.9% on synthon completion, and 14.0% on\nretrosynthesis, highlighting its potential in synthesis planning.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02671.pdf",
        "authors": [
            "Baker Frazier N.",
            "Chen Ziqi",
            "Ning Xia"
        ],
        "metaData": {
            "relevancy": 0.4630018711090088
        }
    },
    {
        "id": "2309.02705",
        "title": "Certifying LLM Safety against Adversarial Prompting",
        "abstract": "  Large language models (LLMs) released for public use incorporate guardrails\nto ensure their output is safe, often referred to as \"model alignment.\" An\naligned language model should decline a user's request to produce harmful\ncontent. However, such safety measures are vulnerable to adversarial prompts,\nwhich contain maliciously designed token sequences to circumvent the model's\nsafety guards and cause it to produce harmful content. In this work, we\nintroduce erase-and-check, the first framework to defend against adversarial\nprompts with verifiable safety guarantees. We erase tokens individually and\ninspect the resulting subsequences using a safety filter. Our procedure labels\nthe input prompt as harmful if any subsequences or the input prompt are\ndetected as harmful by the filter. This guarantees that any adversarial\nmodification of a harmful prompt up to a certain size is also labeled harmful.\nWe defend against three attack modes: i) adversarial suffix, which appends an\nadversarial sequence at the end of the prompt; ii) adversarial insertion, where\nthe adversarial sequence is inserted anywhere in the middle of the prompt; and\niii) adversarial infusion, where adversarial tokens are inserted at arbitrary\npositions in the prompt, not necessarily as a contiguous block. Empirical\nresults demonstrate that our technique obtains strong certified safety\nguarantees on harmful prompts while maintaining good performance on safe\nprompts. For example, against adversarial suffixes of length 20, it certifiably\ndetects 93% of the harmful prompts and labels 94% of the safe prompts as safe\nusing the open source language model Llama 2 as the safety filter.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02705.pdf",
        "authors": [
            "Kumar Aounon",
            "Agarwal Chirag",
            "Srinivas Suraj",
            "Feizi Soheil",
            "Lakkaraju Hima"
        ],
        "metaData": {
            "relevancy": 0.36586637496948243
        }
    },
    {
        "id": "2309.02711",
        "title": "Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic\n  Extension",
        "abstract": "  Symmetry, a fundamental concept to understand our environment, often\noversimplifies reality from a mathematical perspective. Humans are a prime\nexample, deviating from perfect symmetry in terms of appearance and cognitive\nbiases (e.g. having a dominant hand). Nevertheless, our brain can easily\novercome these imperfections and efficiently adapt to symmetrical tasks. The\ndriving motivation behind this work lies in capturing this ability through\nreinforcement learning. To this end, we introduce Adaptive Symmetry Learning\n(ASL) $\\unicode{x2013}$ a model-minimization actor-critic extension that\naddresses incomplete or inexact symmetry descriptions by adapting itself during\nthe learning process. ASL consists of a symmetry fitting component and a\nmodular loss function that enforces a common symmetric relation across all\nstates while adapting to the learned policy. The performance of ASL is compared\nto existing symmetry-enhanced methods in a case study involving a four-legged\nant model for multidirectional locomotion tasks. The results demonstrate that\nASL is capable of recovering from large perturbations and generalizing\nknowledge to hidden symmetric states. It achieves comparable or better\nperformance than alternative methods in most scenarios, making it a valuable\napproach for leveraging model symmetry while compensating for inherent\nperturbations.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02711.pdf",
        "authors": [
            "Abreu Miguel",
            "Reis Luis Paulo",
            "Lau Nuno"
        ],
        "metaData": {
            "relevancy": 0.44023979902267457
        }
    },
    {
        "id": "2309.02712",
        "title": "Unveiling the frontiers of deep learning: innovations shaping diverse\n  domains",
        "abstract": "  Deep learning (DL) enables the development of computer models that are\ncapable of learning, visualizing, optimizing, refining, and predicting data. In\nrecent years, DL has been applied in a range of fields, including audio-visual\ndata processing, agriculture, transportation prediction, natural language,\nbiomedicine, disaster management, bioinformatics, drug design, genomics, face\nrecognition, and ecology. To explore the current state of deep learning, it is\nnecessary to investigate the latest developments and applications of deep\nlearning in these disciplines. However, the literature is lacking in exploring\nthe applications of deep learning in all potential sectors. This paper thus\nextensively investigates the potential applications of deep learning across all\nmajor fields of study as well as the associated benefits and challenges. As\nevidenced in the literature, DL exhibits accuracy in prediction and analysis,\nmakes it a powerful computational tool, and has the ability to articulate\nitself and optimize, making it effective in processing data with no prior\ntraining. Given its independence from training data, deep learning necessitates\nmassive amounts of data for effective analysis and processing, much like data\nvolume. To handle the challenge of compiling huge amounts of medical,\nscientific, healthcare, and environmental data for use in deep learning, gated\narchitectures like LSTMs and GRUs can be utilized. For multimodal learning,\nshared neurons in the neural network for all activities and specialized neurons\nfor particular tasks are necessary.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02712.pdf",
        "authors": [
            "Ahmed Shams Forruque",
            "Alam Md. Sakib Bin",
            "Kabir Maliha",
            "Afrin Shaila",
            "Rafa Sabiha Jannat",
            "Mehjabin Aanushka",
            "Gandomi Amir H."
        ],
        "metaData": {
            "relevancy": 0.3731460690498352
        }
    },
    {
        "id": "2309.02713",
        "title": "SlAction: Non-intrusive, Lightweight Obstructive Sleep Apnea Detection\n  using Infrared Video",
        "abstract": "  Obstructive sleep apnea (OSA) is a prevalent sleep disorder affecting\napproximately one billion people world-wide. The current gold standard for\ndiagnosing OSA, Polysomnography (PSG), involves an overnight hospital stay with\nmultiple attached sensors, leading to potential inaccuracies due to the\nfirst-night effect. To address this, we present SlAction, a non-intrusive OSA\ndetection system for daily sleep environments using infrared videos.\nRecognizing that sleep videos exhibit minimal motion, this work investigates\nthe fundamental question: \"Are respiratory events adequately reflected in human\nmotions during sleep?\" Analyzing the largest sleep video dataset of 5,098\nhours, we establish correlations between OSA events and human motions during\nsleep. Our approach uses a low frame rate (2.5 FPS), a large size (60 seconds)\nand step (30 seconds) for sliding window analysis to capture slow and long-term\nmotions related to OSA. Furthermore, we utilize a lightweight deep neural\nnetwork for resource-constrained devices, ensuring all video streams are\nprocessed locally without compromising privacy. Evaluations show that SlAction\nachieves an average F1 score of 87.6% in detecting OSA across various\nenvironments. Implementing SlAction on NVIDIA Jetson Nano enables real-time\ninference (~3 seconds for a 60-second video clip), highlighting its potential\nfor early detection and personalized treatment of OSA.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02713.pdf",
        "authors": [
            "Choi You Rim",
            "Eo Gyeongseon",
            "Youn Wonhyuck",
            "Lee Hyojin",
            "Jang Haemin",
            "Kim Dongyoon",
            "Shin Hyunwoo",
            "Kim Hyung-Sin"
        ],
        "metaData": {
            "relevancy": 0.22041634917259217
        }
    },
    {
        "id": "2309.02724",
        "title": "Offensive Hebrew Corpus and Detection using BERT",
        "abstract": "  Offensive language detection has been well studied in many languages, but it\nis lagging behind in low-resource languages, such as Hebrew. In this paper, we\npresent a new offensive language corpus in Hebrew. A total of 15,881 tweets\nwere retrieved from Twitter. Each was labeled with one or more of five classes\n(abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew\nbilingual speakers. The annotation process was challenging as each annotator is\nexpected to be familiar with the Israeli culture, politics, and practices to\nunderstand the context of each tweet. We fine-tuned two Hebrew BERT models,\nHeBERT and AlephBERT, using our proposed dataset and another published dataset.\nWe observed that our data boosts HeBERT performance by 2% when combined with\nD_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69%\naccuracy, while fine-tuning on D_OLaH and testing on our data yields 57%\naccuracy, which may be an indication to the generalizability our data offers.\nOur dataset and fine-tuned models are available on GitHub and Huggingface.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02724.pdf",
        "authors": [
            "Hamad Nagham",
            "Jarrar Mustafa",
            "Khalilia Mohammad",
            "Nashif Nadim"
        ],
        "metaData": {
            "relevancy": 0.38248883485794066
        }
    },
    {
        "id": "2309.02726",
        "title": "Large Language Models for Automated Open-domain Scientific Hypotheses\n  Discovery",
        "abstract": "  Hypothetical induction is recognized as the main reasoning type when\nscientists make observations about the world and try to propose hypotheses to\nexplain those observations. Past research on hypothetical induction has a\nlimited setting that (1) the observation annotations of the dataset are not raw\nweb corpus but are manually selected sentences (resulting in a close-domain\nsetting); and (2) the ground truth hypotheses annotations are mostly\ncommonsense knowledge, making the task less challenging. In this work, we\npropose the first NLP dataset for social science academic hypotheses discovery,\nconsisting of 50 recent papers published in top social science journals. Raw\nweb corpora that are necessary for developing hypotheses in the published\npapers are also collected in the dataset, with the final goal of creating a\nsystem that automatically generates valid, novel, and helpful (to human\nresearchers) hypotheses, given only a pile of raw web corpora. The new dataset\ncan tackle the previous problems because it requires to (1) use raw web corpora\nas observations; and (2) propose hypotheses even new to humanity. A\nmulti-module framework is developed for the task, as well as three different\nfeedback mechanisms that empirically show performance gain over the base\nframework. Finally, our framework exhibits high performance in terms of both\nGPT-4 based evaluation and social science expert evaluation.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02726.pdf",
        "authors": [
            "Yang Zonglin",
            "Du Xinya",
            "Li Junxian",
            "Zheng Jie",
            "Poria Soujanya",
            "Cambria Erik"
        ],
        "metaData": {
            "relevancy": 0.4434214234352112
        }
    },
    {
        "id": "2309.02731",
        "title": "HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus",
        "abstract": "  ChatGPT has gained significant interest due to its impressive performance,\nbut people are increasingly concerned about its potential risks, particularly\naround the detection of AI-generated content (AIGC), which is often difficult\nfor untrained humans to identify. Current datasets utilized for detecting\nChatGPT-generated text primarily center around question-answering, yet they\ntend to disregard tasks that possess semantic-invariant properties, such as\nsummarization, translation, and paraphrasing. Our primary studies demonstrate\nthat detecting model-generated text on semantic-invariant tasks is more\ndifficult. To fill this gap, we introduce a more extensive and comprehensive\ndataset that considers more types of tasks than previous work, including\nsemantic-invariant tasks. In addition, the model after a large number of task\ninstruction fine-tuning shows a strong powerful performance. Owing to its\nprevious success, we further instruct fine-tuning Tk-instruct and built a more\npowerful detection system. Experimental results show that our proposed detector\noutperforms the previous state-of-the-art RoBERTa-based detector.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02731.pdf",
        "authors": [
            "Su Zhenpeng",
            "Wu Xing",
            "Zhou Wei",
            "Ma Guangyuan",
            "Hu Songlin"
        ],
        "metaData": {
            "relevancy": 0.5576255321502686
        }
    },
    {
        "id": "2309.02740",
        "title": "Rubric-Specific Approach to Automated Essay Scoring with Augmentation\n  Training",
        "abstract": "  Neural based approaches to automatic evaluation of subjective responses have\nshown superior performance and efficiency compared to traditional rule-based\nand feature engineering oriented solutions. However, it remains unclear whether\nthe suggested neural solutions are sufficient replacements of human raters as\nwe find recent works do not properly account for rubric items that are\nessential for automated essay scoring during model training and validation. In\nthis paper, we propose a series of data augmentation operations that train and\ntest an automated scoring model to learn features and functions overlooked by\nprevious works while still achieving state-of-the-art performance in the\nAutomated Student Assessment Prize dataset.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02740.pdf",
        "authors": [
            "Cho Brian",
            "Jang Youngbin",
            "Yoon Jaewoong"
        ],
        "metaData": {
            "relevancy": 0.3864979803562164
        }
    },
    {
        "id": "2309.02742",
        "title": "MLN-net: A multi-source medical image segmentation method for clustered\n  microcalcifications using multiple layer normalization",
        "abstract": "  Accurate segmentation of clustered microcalcifications in mammography is\ncrucial for the diagnosis and treatment of breast cancer. Despite exhibiting\nexpert-level accuracy, recent deep learning advancements in medical image\nsegmentation provide insufficient contribution to practical applications, due\nto the domain shift resulting from differences in patient postures, individual\ngland density, and imaging modalities of mammography etc. In this paper, a\nnovel framework named MLN-net, which can accurately segment multi-source images\nusing only single source images, is proposed for clustered microcalcification\nsegmentation. We first propose a source domain image augmentation method to\ngenerate multi-source images, leading to improved generalization. And a\nstructure of multiple layer normalization (LN) layers is used to construct the\nsegmentation network, which can be found efficient for clustered\nmicrocalcification segmentation in different domains. Additionally, a branch\nselection strategy is designed for measuring the similarity of the source\ndomain data and the target domain data. To validate the proposed MLN-net,\nextensive analyses including ablation experiments are performed, comparison of\n12 baseline methods. Extensive experiments validate the effectiveness of\nMLN-net in segmenting clustered microcalcifications from different domains and\nthe its segmentation accuracy surpasses state-of-the-art methods. Code will be\navailable at https://github.com/yezanting/MLN-NET-VERSON1.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02742.pdf",
        "authors": [
            "Wang Ke",
            "Ye Zanting",
            "Xie Xiang",
            "Cui Haidong",
            "Chen Tao",
            "Liu Banteng"
        ],
        "metaData": {
            "relevancy": 0.2206460416316986
        }
    },
    {
        "id": "2309.02752",
        "title": "SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time\n  Series",
        "abstract": "  Time series classification (TSC) has emerged as a critical task in various\ndomains, and deep neural models have shown superior performance in TSC tasks.\nHowever, these models are vulnerable to adversarial attacks, where subtle\nperturbations can significantly impact the prediction results. Existing\nadversarial methods often suffer from over-parameterization or random logit\nperturbation, hindering their effectiveness. Additionally, increasing the\nattack success rate (ASR) typically involves generating more noise, making the\nattack more easily detectable. To address these limitations, we propose SWAP, a\nnovel attacking method for TSC models. SWAP focuses on enhancing the confidence\nof the second-ranked logits while minimizing the manipulation of other logits.\nThis is achieved by minimizing the Kullback-Leibler divergence between the\ntarget logit distribution and the predictive logit distribution. Experimental\nresults demonstrate that SWAP achieves state-of-the-art performance, with an\nASR exceeding 50% and an 18% increase compared to existing methods.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02752.pdf",
        "authors": [
            "Dong Chang George",
            "Zheng Liangwei Nathan",
            "Chen Weitong",
            "Zhang Wei Emma",
            "Yue Lin"
        ],
        "metaData": {
            "relevancy": 0.328003990650177
        }
    },
    {
        "id": "2309.02783",
        "title": "Improving diagnosis and prognosis of lung cancer using vision\n  transformers: A scoping review",
        "abstract": "  Vision transformer-based methods are advancing the field of medical\nartificial intelligence and cancer imaging, including lung cancer applications.\nRecently, many researchers have developed vision transformer-based AI methods\nfor lung cancer diagnosis and prognosis. This scoping review aims to identify\nthe recent developments on vision transformer-based AI methods for lung cancer\nimaging applications. It provides key insights into how vision transformers\ncomplemented the performance of AI and deep learning methods for lung cancer.\nFurthermore, the review also identifies the datasets that contributed to\nadvancing the field. Of the 314 retrieved studies, this review included 34\nstudies published from 2020 to 2022. The most commonly addressed task in these\nstudies was the classification of lung cancer types, such as lung squamous cell\ncarcinoma versus lung adenocarcinoma, and identifying benign versus malignant\npulmonary nodules. Other applications included survival prediction of lung\ncancer patients and segmentation of lungs. The studies lacked clear strategies\nfor clinical transformation. SWIN transformer was a popular choice of the\nresearchers; however, many other architectures were also reported where vision\ntransformer was combined with convolutional neural networks or UNet model. It\ncan be concluded that vision transformer-based models are increasingly in\npopularity for developing AI methods for lung cancer applications. However,\ntheir computational complexity and clinical relevance are important factors to\nbe considered for future research work. This review provides valuable insights\nfor researchers in the field of AI and healthcare to advance the\nstate-of-the-art in lung cancer diagnosis and prognosis. We provide an\ninteractive calender on lung-cancer.onrender.com/.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02783.pdf",
        "authors": [
            "Ali Hazrat",
            "Mohsen Farida",
            "Shah Zubair"
        ],
        "metaData": {
            "relevancy": 0.2315775752067566
        }
    },
    {
        "id": "2309.02784",
        "title": "Norm Tweaking: High-performance Low-bit Quantization of Large Language\n  Models",
        "abstract": "  As the size of large language models (LLMs) continues to grow, model\ncompression without sacrificing accuracy has become a crucial challenge for\ndeployment. While some quantization methods, such as GPTQ, have made progress\nin achieving acceptable 4-bit weight-only quantization, attempts at lower bit\nquantization often result in severe performance degradation. In this paper, we\nintroduce a technique called norm tweaking, which can be used as a plugin in\ncurrent PTQ methods to achieve high precision while being cost-efficient. Our\napproach is inspired by the observation that rectifying the quantized\nactivation distribution to match its float counterpart can readily restore\naccuracy for LLMs. To achieve this, we carefully design a tweaking strategy\nthat includes calibration data generation and channel-wise distance constraint\nto update the weights of normalization layers for better generalization. We\nconduct extensive experiments on various datasets using several open-sourced\nLLMs. Our method demonstrates significant improvements in both weight-only\nquantization and joint quantization of weights and activations, surpassing\nexisting PTQ methods. On GLM-130B and OPT-66B, our method even achieves the\nsame level of accuracy at 2-bit quantization as their float ones. Our simple\nand effective approach makes it more practical for real-world applications.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02784.pdf",
        "authors": [
            "Li Liang",
            "Li Qingyuan",
            "Zhang Bo",
            "Chu Xiangxiang"
        ],
        "metaData": {
            "relevancy": 0.3513453841209412
        }
    },
    {
        "id": "2309.02815",
        "title": "Near-continuous time Reinforcement Learning for continuous state-action\n  spaces",
        "abstract": "  We consider the Reinforcement Learning problem of controlling an unknown\ndynamical system to maximise the long-term average reward along a single\ntrajectory. Most of the literature considers system interactions that occur in\ndiscrete time and discrete state-action spaces. Although this standpoint is\nsuitable for games, it is often inadequate for mechanical or digital systems in\nwhich interactions occur at a high frequency, if not in continuous time, and\nwhose state spaces are large if not inherently continuous. Perhaps the only\nexception is the Linear Quadratic framework for which results exist both in\ndiscrete and continuous time. However, its ability to handle continuous states\ncomes with the drawback of a rigid dynamic and reward structure. This work aims\nto overcome these shortcomings by modelling interaction times with a Poisson\nclock of frequency $\\varepsilon^{-1}$, which captures arbitrary time scales:\nfrom discrete ($\\varepsilon=1$) to continuous time ($\\varepsilon\\downarrow0$).\nIn addition, we consider a generic reward function and model the state dynamics\naccording to a jump process with an arbitrary transition kernel on\n$\\mathbb{R}^d$. We show that the celebrated optimism protocol applies when the\nsub-tasks (learning and planning) can be performed effectively. We tackle\nlearning within the eluder dimension framework and propose an approximate\nplanning method based on a diffusive limit approximation of the jump process.\nOverall, our algorithm enjoys a regret of order\n$\\tilde{\\mathcal{O}}(\\varepsilon^{1/2} T+\\sqrt{T})$. As the frequency of\ninteractions blows up, the approximation error $\\varepsilon^{1/2} T$ vanishes,\nshowing that $\\tilde{\\mathcal{O}}(\\sqrt{T})$ is attainable in near-continuous\ntime.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02815.pdf",
        "authors": [
            "Croissant Lorenzo",
            "Abeille Marc",
            "Bouchard Bruno"
        ],
        "metaData": {
            "relevancy": 0.37820319533348085
        }
    },
    {
        "id": "2309.02818",
        "title": "Combining Thermodynamics-based Model of the Centrifugal Compressors and\n  Active Machine Learning for Enhanced Industrial Design Optimization",
        "abstract": "  The design process of centrifugal compressors requires applying an\noptimization process which is computationally expensive due to complex\nanalytical equations underlying the compressor's dynamical equations. Although\nthe regression surrogate models could drastically reduce the computational cost\nof such a process, the major challenge is the scarcity of data for training the\nsurrogate model. Aiming to strategically exploit the labeled samples, we\npropose the Active-CompDesign framework in which we combine a\nthermodynamics-based compressor model (i.e., our internal software for\ncompressor design) and Gaussian Process-based surrogate model within a\ndeployable Active Learning (AL) setting. We first conduct experiments in an\noffline setting and further, extend it to an online AL framework where a\nreal-time interaction with the thermodynamics-based compressor's model allows\nthe deployment in production. ActiveCompDesign shows a significant performance\nimprovement in surrogate modeling by leveraging on uncertainty-based query\nfunction of samples within the AL framework with respect to the random\nselection of data points. Moreover, our framework in production has reduced the\ntotal computational time of compressor's design optimization to around 46%\nfaster than relying on the internal thermodynamics-based simulator, achieving\nthe same performance.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02818.pdf",
        "authors": [
            "Ghiasi Shadi",
            "Pazzi Guido",
            "Del Grosso Concettina",
            "De Magistris Giovanni",
            "Veneri Giacomo"
        ],
        "metaData": {
            "relevancy": 0.3343314349651337
        }
    },
    {
        "id": "2309.02820",
        "title": "Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative\n  Inference Framework for Deep Learning Classification Tasks",
        "abstract": "  Deep learning classifiers are crucial in the age of artificial intelligence.\nThe device-edge-based collaborative inference has been widely adopted as an\nefficient framework for promoting its applications in IoT and 5G/6G networks.\nHowever, it suffers from accuracy degradation under non-i.i.d. data\ndistribution and privacy disclosure. For accuracy degradation, direct use of\ntransfer learning and split learning is high cost and privacy issues remain.\nFor privacy disclosure, cryptography-based approaches lead to a huge overhead.\nOther lightweight methods assume that the ground truth is non-sensitive and can\nbe exposed. But for many applications, the ground truth is the user's crucial\nprivacy-sensitive information. In this paper, we propose a framework of\nRoulette, which is a task-oriented semantic privacy-preserving collaborative\ninference framework for deep learning classifiers. More than input data, we\ntreat the ground truth of the data as private information. We develop a novel\nparadigm of split learning where the back-end DNN is frozen and the front-end\nDNN is retrained to be both a feature extractor and an encryptor. Moreover, we\nprovide a differential privacy guarantee and analyze the hardness of ground\ntruth inference attacks. To validate the proposed Roulette, we conduct\nextensive performance evaluations using realistic datasets, which demonstrate\nthat Roulette can effectively defend against various attacks and meanwhile\nachieve good model accuracy. In a situation where the non-i.i.d. is very\nsevere, Roulette improves the inference accuracy by 21\\% averaged over\nbenchmarks, while making the accuracy of discrimination attacks almost\nequivalent to random guessing.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02820.pdf",
        "authors": [
            "Li Jingyi",
            "Liao Guocheng",
            "Chen Lin",
            "Chen Xu"
        ],
        "metaData": {
            "relevancy": 0.32906665802001955
        }
    },
    {
        "id": "2309.02823",
        "title": "Promoting Open-domain Dialogue Generation through Learning Pattern\n  Information between Contexts and Responses",
        "abstract": "  Recently, utilizing deep neural networks to build the opendomain dialogue\nmodels has become a hot topic. However, the responses generated by these models\nsuffer from many problems such as responses not being contextualized and tend\nto generate generic responses that lack information content, damaging the\nuser's experience seriously. Therefore, many studies try introducing more\ninformation into the dialogue models to make the generated responses more vivid\nand informative. Unlike them, this paper improves the quality of generated\nresponses by learning the implicit pattern information between contexts and\nresponses in the training samples. In this paper, we first build an open-domain\ndialogue model based on the pre-trained language model (i.e., GPT-2). And then,\nan improved scheduled sampling method is proposed for pre-trained models, by\nwhich the responses can be used to guide the response generation in the\ntraining phase while avoiding the exposure bias problem. More importantly, we\ndesign a response-aware mechanism for mining the implicit pattern information\nbetween contexts and responses so that the generated replies are more diverse\nand approximate to human replies. Finally, we evaluate the proposed model (RAD)\non the Persona-Chat and DailyDialog datasets; and the experimental results show\nthat our model outperforms the baselines on most automatic and manual metrics.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02823.pdf",
        "authors": [
            "Liu Mengjuan",
            "Liu Chenyang",
            "Yang Yunfan",
            "Liu Jiang",
            "Jing Mohan"
        ],
        "metaData": {
            "relevancy": 0.6248626232147216
        }
    },
    {
        "id": "2309.02856",
        "title": "Getting too personal(ized): The importance of feature choice in online\n  adaptive algorithms",
        "abstract": "  Digital educational technologies offer the potential to customize students'\nexperiences and learn what works for which students, enhancing the technology\nas more students interact with it. We consider whether and when attempting to\ndiscover how to personalize has a cost, such as if the adaptation to personal\ninformation can delay the adoption of policies that benefit all students. We\nexplore these issues in the context of using multi-armed bandit (MAB)\nalgorithms to learn a policy for what version of an educational technology to\npresent to each student, varying the relation between student characteristics\nand outcomes and also whether the algorithm is aware of these characteristics.\nThrough simulations, we demonstrate that the inclusion of student\ncharacteristics for personalization can be beneficial when those\ncharacteristics are needed to learn the optimal action. In other scenarios,\nthis inclusion decreases performance of the bandit algorithm. Moreover,\nincluding unneeded student characteristics can systematically disadvantage\nstudents with less common values for these characteristics. Our simulations do\nhowever suggest that real-time personalization will be helpful in particular\nreal-world scenarios, and we illustrate this through case studies using\nexisting experimental results in ASSISTments. Overall, our simulations show\nthat adaptive personalization in educational technologies can be a double-edged\nsword: real-time adaptation improves student experiences in some contexts, but\nthe slower adaptation and potentially discriminatory results mean that a more\npersonalized model is not always beneficial.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02856.pdf",
        "authors": [
            "Li ZhaoBin",
            "Yee Luna",
            "Sauerberg Nathaniel",
            "Sakson Irene",
            "Williams Joseph Jay",
            "Rafferty Anna N."
        ],
        "metaData": {
            "relevancy": 0.44951980710029604
        }
    },
    {
        "id": "2309.02858",
        "title": "Generalised Mutual Information: a Framework for Discriminative\n  Clustering",
        "abstract": "  In the last decade, recent successes in deep clustering majorly involved the\nMutual Information (MI) as an unsupervised objective for training neural\nnetworks with increasing regularisations. While the quality of the\nregularisations have been largely discussed for improvements, little attention\nhas been dedicated to the relevance of MI as a clustering objective. In this\npaper, we first highlight how the maximisation of MI does not lead to\nsatisfying clusters. We identified the Kullback-Leibler divergence as the main\nreason of this behaviour. Hence, we generalise the mutual information by\nchanging its core distance, introducing the Generalised Mutual Information\n(GEMINI): a set of metrics for unsupervised neural network training. Unlike MI,\nsome GEMINIs do not require regularisations when training as they are\ngeometry-aware thanks to distances or kernels in the data space. Finally, we\nhighlight that GEMINIs can automatically select a relevant number of clusters,\na property that has been little studied in deep discriminative clustering\ncontext where the number of clusters is a priori unknown.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02858.pdf",
        "authors": [
            "Ohl Louis",
            "Mattei Pierre-Alexandre",
            "Bouveyron Charles",
            "Harchaoui Warith",
            "Leclercq Micka\u00ebl",
            "Droit Arnaud",
            "Precioso Fr\u00e9d\u00e9ric"
        ],
        "metaData": {
            "relevancy": 0.3893319070339203
        }
    },
    {
        "id": "2309.02870",
        "title": "Rethinking Momentum Knowledge Distillation in Online Continual Learning",
        "abstract": "  Online Continual Learning (OCL) addresses the problem of training neural\nnetworks on a continuous data stream where multiple classification tasks emerge\nin sequence. In contrast to offline Continual Learning, data can be seen only\nonce in OCL. In this context, replay-based strategies have achieved impressive\nresults and most state-of-the-art approaches are heavily depending on them.\nWhile Knowledge Distillation (KD) has been extensively used in offline\nContinual Learning, it remains under-exploited in OCL, despite its potential.\nIn this paper, we theoretically analyze the challenges in applying KD to OCL.\nWe introduce a direct yet effective methodology for applying Momentum Knowledge\nDistillation (MKD) to many flagship OCL methods and demonstrate its\ncapabilities to enhance existing approaches. In addition to improving existing\nstate-of-the-arts accuracy by more than $10\\%$ points on ImageNet100, we shed\nlight on MKD internal mechanics and impacts during training in OCL. We argue\nthat similar to replay, MKD should be considered a central component of OCL.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02870.pdf",
        "authors": [
            "Michel Nicolas",
            "Wang Maorong",
            "Xiao Ling",
            "Yamasaki Toshihiko"
        ],
        "metaData": {
            "relevancy": 0.40862141251564027
        }
    },
    {
        "id": "2309.02875",
        "title": "MAD: Modality Agnostic Distance Measure for Image Registration",
        "abstract": "  Multi-modal image registration is a crucial pre-processing step in many\nmedical applications. However, it is a challenging task due to the complex\nintensity relationships between different imaging modalities, which can result\nin large discrepancy in image appearance. The success of multi-modal image\nregistration, whether it is conventional or learning based, is predicated upon\nthe choice of an appropriate distance (or similarity) measure. Particularly,\ndeep learning registration algorithms lack in accuracy or even fail completely\nwhen attempting to register data from an \"unseen\" modality. In this work, we\npresent Modality Agnostic Distance (MAD), a deep image distance}] measure that\nutilises random convolutions to learn the inherent geometry of the images while\nbeing robust to large appearance changes. Random convolutions are\ngeometry-preserving modules which we use to simulate an infinite number of\nsynthetic modalities alleviating the need for aligned paired data during\ntraining. We can therefore train MAD on a mono-modal dataset and successfully\napply it to a multi-modal dataset. We demonstrate that not only can MAD\naffinely register multi-modal images successfully, but it has also a larger\ncapture range than traditional measures such as Mutual Information and\nNormalised Gradient Fields.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02875.pdf",
        "authors": [
            "Sideri-Lampretsa Vasiliki",
            "Zimmer Veronika A.",
            "Qiu Huaqi",
            "Kaissis Georgios",
            "Rueckert Daniel"
        ],
        "metaData": {
            "relevancy": 0.33879082202911376
        }
    },
    {
        "id": "2309.02887",
        "title": "A deep Natural Language Inference predictor without language-specific\n  training data",
        "abstract": "  In this paper we present a technique of NLP to tackle the problem of\ninference relation (NLI) between pairs of sentences in a target language of\nchoice without a language-specific training dataset. We exploit a generic\ntranslation dataset, manually translated, along with two instances of the same\npre-trained model - the first to generate sentence embeddings for the source\nlanguage, and the second fine-tuned over the target language to mimic the\nfirst. This technique is known as Knowledge Distillation. The model has been\nevaluated over machine translated Stanford NLI test dataset, machine translated\nMulti-Genre NLI test dataset, and manually translated RTE3-ITA test dataset. We\nalso test the proposed architecture over different tasks to empirically\ndemonstrate the generality of the NLI task. The model has been evaluated over\nthe native Italian ABSITA dataset, on the tasks of Sentiment Analysis,\nAspect-Based Sentiment Analysis, and Topic Recognition. We emphasise the\ngenerality and exploitability of the Knowledge Distillation technique that\noutperforms other methodologies based on machine translation, even though the\nformer was not directly trained on the data it was tested over.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02887.pdf",
        "authors": [
            "Corradi Lorenzo",
            "Manenti Alessandro",
            "Del Bonifro Francesca",
            "Setti Francesco",
            "Del Sorbo Dario"
        ],
        "metaData": {
            "relevancy": 0.5428449749946594
        }
    },
    {
        "id": "2309.02908",
        "title": "DECODE: Data-driven Energy Consumption Prediction leveraging Historical\n  Data and Environmental Factors in Buildings",
        "abstract": "  Energy prediction in buildings plays a crucial role in effective energy\nmanagement. Precise predictions are essential for achieving optimal energy\nconsumption and distribution within the grid. This paper introduces a Long\nShort-Term Memory (LSTM) model designed to forecast building energy consumption\nusing historical energy data, occupancy patterns, and weather conditions. The\nLSTM model provides accurate short, medium, and long-term energy predictions\nfor residential and commercial buildings compared to existing prediction\nmodels. We compare our LSTM model with established prediction methods,\nincluding linear regression, decision trees, and random forest. Encouragingly,\nthe proposed LSTM model emerges as the superior performer across all metrics.\nIt demonstrates exceptional prediction accuracy, boasting the highest R2 score\nof 0.97 and the most favorable mean absolute error (MAE) of 0.007. An\nadditional advantage of our developed model is its capacity to achieve\nefficient energy consumption forecasts even when trained on a limited dataset.\nWe address concerns about overfitting (variance) and underfitting (bias)\nthrough rigorous training and evaluation on real-world data. In summary, our\nresearch contributes to energy prediction by offering a robust LSTM model that\noutperforms alternative methods and operates with remarkable efficiency,\ngeneralizability, and reliability.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02908.pdf",
        "authors": [
            "Mishra Aditya",
            "Lone Haroon R.",
            "Mishra Aayush"
        ],
        "metaData": {
            "relevancy": 0.28495543003082274
        }
    },
    {
        "id": "2309.02912",
        "title": "On the Challenges of Building Datasets for Hate Speech Detection",
        "abstract": "  Detection of hate speech has been formulated as a standalone application of\nNLP and different approaches have been adopted for identifying the target\ngroups, obtaining raw data, defining the labeling process, choosing the\ndetection algorithm, and evaluating the performance in the desired setting.\nHowever, unlike other downstream tasks, hate speech suffers from the lack of\nlarge-sized, carefully curated, generalizable datasets owing to the highly\nsubjective nature of the task. In this paper, we first analyze the issues\nsurrounding hate speech detection through a data-centric lens. We then outline\na holistic framework to encapsulate the data creation pipeline across seven\nbroad dimensions by taking the specific example of hate speech towards sexual\nminorities. We posit that practitioners would benefit from following this\nframework as a form of best practice when creating hate speech datasets in the\nfuture.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02912.pdf",
        "authors": [
            "Bhandari Vitthal"
        ],
        "metaData": {
            "relevancy": 0.3724883198738098
        }
    },
    {
        "id": "2309.02935",
        "title": "Estimating irregular water demands with physics-informed machine\n  learning to inform leakage detection",
        "abstract": "  Leakages in drinking water distribution networks pose significant challenges\nto water utilities, leading to infrastructure failure, operational disruptions,\nenvironmental hazards, property damage, and economic losses. The timely\nidentification and accurate localisation of such leakages is paramount for\nutilities to mitigate these unwanted effects. However, implementation of\nalgorithms for leakage detection is limited in practice by requirements of\neither hydraulic models or large amounts of training data. Physics-informed\nmachine learning can utilise hydraulic information thereby circumventing both\nlimitations. In this work, we present a physics-informed machine learning\nalgorithm that analyses pressure data and therefrom estimates unknown irregular\nwater demands via a fully connected neural network, ultimately leveraging the\nBernoulli equation and effectively linearising the leakage detection problem.\nOur algorithm is tested on data from the L-Town benchmark network, and results\nindicate a good capability for estimating most irregular demands, with R2\nlarger than 0.8. Identification results for leakages under the presence of\nirregular demands could be improved by a factor of 5.3 for abrupt leaks and a\nfactor of 3.0 for incipient leaks when compared the results disregarding\nirregular demands.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02935.pdf",
        "authors": [
            "Daniel Ivo",
            "Cominola Andrea"
        ],
        "metaData": {
            "relevancy": 0.2915047943592072
        }
    },
    {
        "id": "2309.02936",
        "title": "EdgeFL: A Lightweight Decentralized Federated Learning Framework",
        "abstract": "  Federated Learning (FL) has emerged as a promising approach for collaborative\nmachine learning, addressing data privacy concerns. However, existing FL\nplatforms and frameworks often present challenges for software engineers in\nterms of complexity, limited customization options, and scalability\nlimitations. In this paper, we introduce EdgeFL, an edge-only lightweight\ndecentralized FL framework, designed to overcome the limitations of centralized\naggregation and scalability in FL deployments. By adopting an edge-only model\ntraining and aggregation approach, EdgeFL eliminates the need for a central\nserver, enabling seamless scalability across diverse use cases. With a\nstraightforward integration process requiring just four lines of code (LOC),\nsoftware engineers can easily incorporate FL functionalities into their AI\nproducts. Furthermore, EdgeFL offers the flexibility to customize aggregation\nfunctions, empowering engineers to adapt them to specific needs. Based on the\nresults, we demonstrate that EdgeFL achieves superior performance compared to\nexisting FL platforms/frameworks. Our results show that EdgeFL reduces weights\nupdate latency and enables faster model evolution, enhancing the efficiency of\nedge devices. Moreover, EdgeFL exhibits improved classification accuracy\ncompared to traditional centralized FL approaches. By leveraging EdgeFL,\nsoftware engineers can harness the benefits of federated learning while\novercoming the challenges associated with existing FL platforms/frameworks.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.02936.pdf",
        "authors": [
            "Zhang Hongyi",
            "Bosch Jan",
            "Olsson Helena Holmstr\u00f6m"
        ],
        "metaData": {
            "relevancy": 0.378526097536087
        }
    },
    {
        "id": "2309.03023",
        "title": "Universal Preprocessing Operators for Embedding Knowledge Graphs with\n  Literals",
        "abstract": "  Knowledge graph embeddings are dense numerical representations of entities in\na knowledge graph (KG). While the majority of approaches concentrate only on\nrelational information, i.e., relations between entities, fewer approaches\nexist which also take information about literal values (e.g., textual\ndescriptions or numerical information) into account. Those which exist are\ntypically tailored towards a particular modality of literal and a particular\nembedding method. In this paper, we propose a set of universal preprocessing\noperators which can be used to transform KGs with literals for numerical,\ntemporal, textual, and image information, so that the transformed KGs can be\nembedded with any method. The results on the kgbench dataset with three\ndifferent embedding methods show promising results.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03023.pdf",
        "authors": [
            "Preisner Patryk",
            "Paulheim Heiko"
        ],
        "metaData": {
            "relevancy": 0.5558435320854187
        }
    },
    {
        "id": "2309.03036",
        "title": "An Efficient Temporary Deepfake Location Approach Based Embeddings for\n  Partially Spoofed Audio Detection",
        "abstract": "  Partially spoofed audio detection is a challenging task, lying in the need to\naccurately locate the authenticity of audio at the frame level. To address this\nissue, we propose a fine-grained partially spoofed audio detection method,\nnamely Temporal Deepfake Location (TDL), which can effectively capture\ninformation of both features and locations. Specifically, our approach involves\ntwo novel parts: embedding similarity module and temporal convolution\noperation. To enhance the identification between the real and fake features,\nthe embedding similarity module is designed to generate an embedding space that\ncan separate the real frames from fake frames. To effectively concentrate on\nthe position information, temporal convolution operation is proposed to\ncalculate the frame-specific similarities among neighboring frames, and\ndynamically select informative neighbors to convolution. Extensive experiments\nshow that our method outperform baseline models in ASVspoof2019 Partial Spoof\ndataset and demonstrate superior performance even in the crossdataset scenario.\nThe code is released online.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03036.pdf",
        "authors": [
            "Xie Yuankun",
            "Cheng Haonan",
            "Wang Yutian",
            "Ye Long"
        ],
        "metaData": {
            "relevancy": 0.2686361938714981
        }
    },
    {
        "id": "2309.03041",
        "title": "A Refutation of Shapley Values for Explainability",
        "abstract": "  Recent work demonstrated the existence of Boolean functions for which Shapley\nvalues provide misleading information about the relative importance of features\nin rule-based explanations. Such misleading information was broadly categorized\ninto a number of possible issues. Each of those issues relates with features\nbeing relevant or irrelevant for a prediction, and all are significant\nregarding the inadequacy of Shapley values for rule-based explainability. This\nearlier work devised a brute-force approach to identify Boolean functions,\ndefined on small numbers of features, and also associated instances, which\ndisplayed such inadequacy-revealing issues, and so served as evidence to the\ninadequacy of Shapley values for rule-based explainability. However, an\noutstanding question is how frequently such inadequacy-revealing issues can\noccur for Boolean functions with arbitrary large numbers of features. It is\nplain that a brute-force approach would be unlikely to provide insights on how\nto tackle this question. This paper answers the above question by proving that,\nfor any number of features, there exist Boolean functions that exhibit one or\nmore inadequacy-revealing issues, thereby contributing decisive arguments\nagainst the use of Shapley values as the theoretical underpinning of\nfeature-attribution methods in explainability.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03041.pdf",
        "authors": [
            "Huang Xuanxiang",
            "Marques-Silva Joao"
        ],
        "metaData": {
            "relevancy": 0.3161251783370972
        }
    },
    {
        "id": "2309.03047",
        "title": "Combining pre-trained Vision Transformers and CIDER for Out Of Domain\n  Detection",
        "abstract": "  Out-of-domain (OOD) detection is a crucial component in industrial\napplications as it helps identify when a model encounters inputs that are\noutside the training distribution. Most industrial pipelines rely on\npre-trained models for downstream tasks such as CNN or Vision Transformers.\nThis paper investigates the performance of those models on the task of\nout-of-domain detection. Our experiments demonstrate that pre-trained\ntransformers models achieve higher detection performance out of the box.\nFurthermore, we show that pre-trained ViT and CNNs can be combined with\nrefinement methods such as CIDER to improve their OOD detection performance\neven more. Our results suggest that transformers are a promising approach for\nOOD detection and set a stronger baseline for this task in many contexts\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03047.pdf",
        "authors": [
            "Jouet Gr\u00e9gor",
            "Duhart Cl\u00e9ment",
            "Rousseaux Francis",
            "Laborde Julio",
            "de Runz Cyril"
        ],
        "metaData": {
            "relevancy": 0.2735887825489044
        }
    },
    {
        "id": "2309.03057",
        "title": "Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy\n  Protection",
        "abstract": "  Numerous companies have started offering services based on large language\nmodels (LLM), such as ChatGPT, which inevitably raises privacy concerns as\nusers' prompts are exposed to the model provider. Previous research on secure\nreasoning using multi-party computation (MPC) has proven to be impractical for\nLLM applications due to its time-consuming and communication-intensive nature.\nWhile lightweight anonymization techniques can protect private information in\nprompts through substitution or masking, they fail to recover sensitive data\nreplaced in the LLM-generated results. In this paper, we expand the application\nscenarios of anonymization techniques by training a small local model to\nde-anonymize the LLM's returned results with minimal computational overhead. We\nintroduce the HaS framework, where \"H(ide)\" and \"S(eek)\" represent its two core\nprocesses: hiding private entities for anonymization and seeking private\nentities for de-anonymization, respectively. To quantitatively assess HaS's\nprivacy protection performance, we propose both black-box and white-box\nadversarial models. Furthermore, we conduct experiments to evaluate HaS's\nusability in translation and classification tasks. The experimental findings\ndemonstrate that the HaS framework achieves an optimal balance between privacy\nprotection and utility.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03057.pdf",
        "authors": [
            "Chen Yu",
            "Li Tingxin",
            "Liu Huiming",
            "Yu Yang"
        ],
        "metaData": {
            "relevancy": 0.37494332790374757
        }
    },
    {
        "id": "2309.03064",
        "title": "A Multimodal Analysis of Influencer Content on Twitter",
        "abstract": "  Influencer marketing involves a wide range of strategies in which brands\ncollaborate with popular content creators (i.e., influencers) to leverage their\nreach, trust, and impact on their audience to promote and endorse products or\nservices. Because followers of influencers are more likely to buy a product\nafter receiving an authentic product endorsement rather than an explicit direct\nproduct promotion, the line between personal opinions and commercial content\npromotion is frequently blurred. This makes automatic detection of regulatory\ncompliance breaches related to influencer advertising (e.g., misleading\nadvertising or hidden sponsorships) particularly difficult. In this work, we\n(1) introduce a new Twitter (now X) dataset consisting of 15,998 influencer\nposts mapped into commercial and non-commercial categories for assisting in the\nautomatic detection of commercial influencer content; (2) experiment with an\nextensive set of predictive models that combine text and visual information\nshowing that our proposed cross-attention approach outperforms state-of-the-art\nmultimodal models; and (3) conduct a thorough analysis of strengths and\nlimitations of our models. We show that multimodal modeling is useful for\nidentifying commercial posts, reducing the amount of false positives, and\ncapturing relevant context that aids in the discovery of undisclosed commercial\nposts.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03064.pdf",
        "authors": [
            "Villegas Danae S\u00e1nchez",
            "Goanta Catalina",
            "Aletras Nikolaos"
        ],
        "metaData": {
            "relevancy": 0.3630797863006592
        }
    },
    {
        "id": "2309.03084",
        "title": "Pure Monte Carlo Counterfactual Regret Minimization",
        "abstract": "  Counterfactual Regret Minimization (CFR) and its variants are the best\nalgorithms so far for solving large-scale incomplete information games.\nBuilding upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR)\nfor achieving better performance. PCFR can be seen as a combination of CFR and\nFictitious Play (FP), inheriting the concept of counterfactual regret (value)\nfrom CFR, and using the best response strategy instead of the regret matching\nstrategy for the next iteration. Our theoretical proof that PCFR can achieve\nBlackwell approachability enables PCFR's ability to combine with any CFR\nvariant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR)\ncan significantly reduce time and space complexity. Particularly, the\nconvergence speed of PMCCFR is at least three times more than that of MCCFR. In\naddition, since PMCCFR does not pass through the path of strictly dominated\nstrategies, we developed a new warm-start algorithm inspired by the strictly\ndominated strategies elimination method. Consequently, the PMCCFR with new warm\nstart algorithm can converge by two orders of magnitude faster than the CFR+\nalgorithm.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03084.pdf",
        "authors": [
            "Qi Ju",
            "Feng Ting",
            "Hei Falun",
            "Fang Zhemei",
            "Luo Yunfeng"
        ],
        "metaData": {
            "relevancy": 0.47349992394447327
        }
    },
    {
        "id": "2309.03087",
        "title": "Unreflected Acceptance -- Investigating the Negative Consequences of\n  ChatGPT-Assisted Problem Solving in Physics Education",
        "abstract": "  Large language models (LLMs) have recently gained popularity. However, the\nimpact of their general availability through ChatGPT on sensitive areas of\neveryday life, such as education, remains unclear. Nevertheless, the societal\nimpact on established educational methods is already being experienced by both\nstudents and educators. Our work focuses on higher physics education and\nexamines problem solving strategies. In a study, students with a background in\nphysics were assigned to solve physics exercises, with one group having access\nto an internet search engine (N=12) and the other group being allowed to use\nChatGPT (N=27). We evaluated their performance, strategies, and interaction\nwith the provided tools. Our results showed that nearly half of the solutions\nprovided with the support of ChatGPT were mistakenly assumed to be correct by\nthe students, indicating that they overly trusted ChatGPT even in their field\nof expertise. Likewise, in 42% of cases, students used copy & paste to query\nChatGPT -- an approach only used in 4% of search engine queries -- highlighting\nthe stark differences in interaction behavior between the groups and indicating\nlimited reflection when using ChatGPT. In our work, we demonstrated a need to\n(1) guide students on how to interact with LLMs and (2) create awareness of\npotential shortcomings for users.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03087.pdf",
        "authors": [
            "Krupp Lars",
            "Steinert Steffen",
            "Kiefer-Emmanouilidis Maximilian",
            "Avila Karina E.",
            "Lukowicz Paul",
            "Kuhn Jochen",
            "K\u00fcchemann Stefan",
            "Karolus Jakob"
        ],
        "metaData": {
            "relevancy": 0.42276148200035096
        }
    },
    {
        "id": "2309.03092",
        "title": "Establishing Markov Equivalence in Cyclic Directed Graphs",
        "abstract": "  We present a new, efficient procedure to establish Markov equivalence between\ndirected graphs that may or may not contain cycles under the\n\\textit{d}-separation criterion. It is based on the Cyclic Equivalence Theorem\n(CET) in the seminal works on cyclic models by Thomas Richardson in the mid\n'90s, but now rephrased from an ancestral perspective. The resulting\ncharacterization leads to a procedure for establishing Markov equivalence\nbetween graphs that no longer requires tests for d-separation, leading to a\nsignificantly reduced algorithmic complexity. The conceptually simplified\ncharacterization may help to reinvigorate theoretical research towards sound\nand complete cyclic discovery in the presence of latent confounders. This\nversion includes a correction to rule (iv) in Theorem 1, and the subsequent\nadjustment in part 2 of Algorithm 2.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03092.pdf",
        "authors": [
            "Claassen Tom",
            "Mooij Joris M."
        ],
        "metaData": {
            "relevancy": 0.32101392149925234
        }
    },
    {
        "id": "2309.03094",
        "title": "Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex\n  Penalties",
        "abstract": "  This paper investigates quantile regression in the presence of non-convex and\nnon-smooth sparse penalties, such as the minimax concave penalty (MCP) and\nsmoothly clipped absolute deviation (SCAD). The non-smooth and non-convex\nnature of these problems often leads to convergence difficulties for many\nalgorithms. While iterative techniques like coordinate descent and local linear\napproximation can facilitate convergence, the process is often slow. This\nsluggish pace is primarily due to the need to run these approximation\ntechniques until full convergence at each step, a requirement we term as a\n\\emph{secondary convergence iteration}. To accelerate the convergence speed, we\nemploy the alternating direction method of multipliers (ADMM) and introduce a\nnovel single-loop smoothing ADMM algorithm with an increasing penalty\nparameter, named SIAD, specifically tailored for sparse-penalized quantile\nregression. We first delve into the convergence properties of the proposed SIAD\nalgorithm and establish the necessary conditions for convergence.\nTheoretically, we confirm a convergence rate of $o\\big({k^{-\\frac{1}{4}}}\\big)$\nfor the sub-gradient bound of augmented Lagrangian. Subsequently, we provide\nnumerical results to showcase the effectiveness of the SIAD algorithm. Our\nfindings highlight that the SIAD method outperforms existing approaches,\nproviding a faster and more stable solution for sparse-penalized quantile\nregression.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03094.pdf",
        "authors": [
            "Mirzaeifard Reza",
            "Venkategowda Naveen K. D.",
            "Gogineni Vinay Chakravarthi",
            "Werner Stefan"
        ],
        "metaData": {
            "relevancy": 0.21756330132484436
        }
    },
    {
        "id": "2309.03113",
        "title": "Detecting Manufacturing Defects in PCBs via Data-Centric Machine\n  Learning on Solder Paste Inspection Features",
        "abstract": "  Automated detection of defects in Printed Circuit Board (PCB) manufacturing\nusing Solder Paste Inspection (SPI) and Automated Optical Inspection (AOI)\nmachines can help improve operational efficiency and significantly reduce the\nneed for manual intervention. In this paper, using SPI-extracted features of 6\nmillion pins, we demonstrate a data-centric approach to train Machine Learning\n(ML) models to detect PCB defects at three stages of PCB manufacturing. The 6\nmillion PCB pins correspond to 2 million components that belong to 15,387 PCBs.\nUsing a base extreme gradient boosting (XGBoost) ML model, we iterate on the\ndata pre-processing step to improve detection performance. Combining pin-level\nSPI features using component and PCB IDs, we developed training instances also\nat the component and PCB level. This allows the ML model to capture any\ninter-pin, inter-component, or spatial effects that may not be apparent at the\npin level. Models are trained at the pin, component, and PCB levels, and the\ndetection results from the different models are combined to identify defective\ncomponents.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03113.pdf",
        "authors": [
            "Prasad-Rao Jubilee",
            "Heidary Roohollah",
            "Williams Jesse"
        ],
        "metaData": {
            "relevancy": 0.2168203115463257
        }
    },
    {
        "id": "2309.03130",
        "title": "MyoDex: A Generalizable Prior for Dexterous Manipulation",
        "abstract": "  Human dexterity is a hallmark of motor control. Our hands can rapidly\nsynthesize new behaviors despite the complexity (multi-articular and\nmulti-joints, with 23 joints controlled by more than 40 muscles) of\nmusculoskeletal sensory-motor circuits. In this work, we take inspiration from\nhow human dexterity builds on a diversity of prior experiences, instead of\nbeing acquired through a single task. Motivated by this observation, we set out\nto develop agents that can build upon their previous experience to quickly\nacquire new (previously unattainable) behaviors. Specifically, our approach\nleverages multi-task learning to implicitly capture task-agnostic behavioral\npriors (MyoDex) for human-like dexterity, using a physiologically realistic\nhuman hand model - MyoHand. We demonstrate MyoDex's effectiveness in few-shot\ngeneralization as well as positive transfer to a large repertoire of unseen\ndexterous manipulation tasks. Agents leveraging MyoDex can solve approximately\n3x more tasks, and 4x faster in comparison to a distillation baseline. While\nprior work has synthesized single musculoskeletal control behaviors, MyoDex is\nthe first generalizable manipulation prior that catalyzes the learning of\ndexterous physiological control across a large variety of contact-rich\nbehaviors. We also demonstrate the effectiveness of our paradigms beyond\nmusculoskeletal control towards the acquisition of dexterity in 24 DoF Adroit\nHand. Website: https://sites.google.com/view/myodex\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03130.pdf",
        "authors": [
            "Caggiano Vittorio",
            "Dasari Sudeep",
            "Kumar Vikash"
        ],
        "metaData": {
            "relevancy": 0.44749270677566527
        }
    },
    {
        "id": "2309.03133",
        "title": "Risk-reducing design and operations toolkit: 90 strategies for managing\n  risk and uncertainty in decision problems",
        "abstract": "  Uncertainty is a pervasive challenge in decision analysis, and decision\ntheory recognizes two classes of solutions: probabilistic models and cognitive\nheuristics. However, engineers, public planners and other decision-makers\ninstead use a third class of strategies that could be called RDOT\n(Risk-reducing Design and Operations Toolkit). These include incorporating\nrobustness into designs, contingency planning, and others that do not fall into\nthe categories of probabilistic models or cognitive heuristics. Moreover,\nidentical strategies appear in several domains and disciplines, pointing to an\nimportant shared toolkit.\n  The focus of this paper is to develop a catalog of such strategies and\ndevelop a framework for them. The paper finds more than 90 examples of such\nstrategies falling into six broad categories and argues that they provide an\nefficient response to decision problems that are seemingly intractable due to\nhigh uncertainty. It then proposes a framework to incorporate them into\ndecision theory using multi-objective optimization.\n  Overall, RDOT represents an overlooked class of responses to uncertainty.\nBecause RDOT strategies do not depend on accurate forecasting or estimation,\nthey could be applied fruitfully to certain decision problems affected by high\nuncertainty and make them much more tractable.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03133.pdf",
        "authors": [
            "Gutfraind Alexander"
        ],
        "metaData": {
            "relevancy": 0.34943569302558897
        }
    },
    {
        "id": "2309.03164",
        "title": "J-Guard: Journalism Guided Adversarially Robust Detection of\n  AI-generated News",
        "abstract": "  The rapid proliferation of AI-generated text online is profoundly reshaping\nthe information landscape. Among various types of AI-generated text,\nAI-generated news presents a significant threat as it can be a prominent source\nof misinformation online. While several recent efforts have focused on\ndetecting AI-generated text in general, these methods require enhanced\nreliability, given concerns about their vulnerability to simple adversarial\nattacks. Furthermore, due to the eccentricities of news writing, applying these\ndetection methods for AI-generated news can produce false positives,\npotentially damaging the reputation of news organizations. To address these\nchallenges, we leverage the expertise of an interdisciplinary team to develop a\nframework, J-Guard, capable of steering existing supervised AI text detectors\nfor detecting AI-generated news while boosting adversarial robustness. By\nincorporating stylistic cues inspired by the unique journalistic attributes,\nJ-Guard effectively distinguishes between real-world journalism and\nAI-generated news articles. Our experiments on news articles generated by a\nvast array of AI models, including ChatGPT (GPT3.5), demonstrate the\neffectiveness of J-Guard in enhancing detection capabilities while maintaining\nan average performance decrease of as low as 7% when faced with adversarial\nattacks.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03164.pdf",
        "authors": [
            "Kumarage Tharindu",
            "Bhattacharjee Amrita",
            "Padejski Djordje",
            "Roschke Kristy",
            "Gillmor Dan",
            "Ruston Scott",
            "Liu Huan",
            "Garland Joshua"
        ],
        "metaData": {
            "relevancy": 0.43479623198509215
        }
    },
    {
        "id": "2309.03167",
        "title": "Split-Boost Neural Networks",
        "abstract": "  The calibration and training of a neural network is a complex and\ntime-consuming procedure that requires significant computational resources to\nachieve satisfactory results. Key obstacles are a large number of\nhyperparameters to select and the onset of overfitting in the face of a small\namount of data. In this framework, we propose an innovative training strategy\nfor feed-forward architectures - called split-boost - that improves performance\nand automatically includes a regularizing behaviour without modeling it\nexplicitly. Such a novel approach ultimately allows us to avoid explicitly\nmodeling the regularization term, decreasing the total number of\nhyperparameters and speeding up the tuning phase. The proposed strategy is\ntested on a real-world (anonymized) dataset within a benchmark medical\ninsurance design problem.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03167.pdf",
        "authors": [
            "Cestari Raffaele Giuseppe",
            "Maroni Gabriele",
            "Cannelli Loris",
            "Piga Dario",
            "Formentin Simone"
        ],
        "metaData": {
            "relevancy": 0.3020551085472107
        }
    },
    {
        "id": "2309.03198",
        "title": "My Art My Choice: Adversarial Protection Against Unruly AI",
        "abstract": "  Generative AI is on the rise, enabling everyone to produce realistic content\nvia publicly available interfaces. Especially for guided image generation,\ndiffusion models are changing the creator economy by producing high quality low\ncost content. In parallel, artists are rising against unruly AI, since their\nartwork are leveraged, distributed, and dissimulated by large generative\nmodels. Our approach, My Art My Choice (MAMC), aims to empower content owners\nby protecting their copyrighted materials from being utilized by diffusion\nmodels in an adversarial fashion. MAMC learns to generate adversarially\nperturbed \"protected\" versions of images which can in turn \"break\" diffusion\nmodels. The perturbation amount is decided by the artist to balance distortion\nvs. protection of the content. MAMC is designed with a simple UNet-based\ngenerator, attacking black box diffusion models, combining several losses to\ncreate adversarial twins of the original artwork. We experiment on three\ndatasets for various image-to-image tasks, with different user control values.\nBoth protected image and diffusion output results are evaluated in visual,\nnoise, structure, pixel, and generative spaces to validate our claims. We\nbelieve that MAMC is a crucial step for preserving ownership information for AI\ngenerated content in a flawless, based-on-need, and human-centric way.\n",
        "pdfLink": "http://arxiv.org/pdf/2309.03198.pdf",
        "authors": [
            "Rhodes Anthony",
            "Bhagat Ram",
            "Ciftci Umur Aybars",
            "Demir Ilke"
        ],
        "metaData": {
            "relevancy": 0.27726573348045347
        }
    }
]