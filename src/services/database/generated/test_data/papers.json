[
  {
    "id": "http://arxiv.org/abs/2307.08700v1",
    "title": "Fast model inference and training on-board of Satellites",
    "summary": "Artificial intelligence onboard satellites has the potential to reduce data\ntransmission requirements, enable real-time decision-making and collaboration\nwithin constellations. This study deploys a lightweight foundational model\ncalled RaVAEn on D-Orbit's ION SCV004 satellite. RaVAEn is a variational\nauto-encoder (VAE) that generates compressed latent vectors from small image\ntiles, enabling several downstream tasks. In this work we demonstrate the\nreliable use of RaVAEn onboard a satellite, achieving an encoding time of\n0.110s for tiles of a 4.8x4.8 km$^2$ area. In addition, we showcase fast\nfew-shot training onboard a satellite using the latent representation of data.\nWe compare the deployment of the model on the on-board CPU and on the available\nMyriad vision processing unit (VPU) accelerator. To our knowledge, this work\nshows for the first time the deployment of a multi-task model on-board a\nCubeSat and the on-board training of a machine learning model.",
    "authors": [
      [
        "Vít Růžička"
      ],
      [
        "Gonzalo Mateo-García"
      ],
      [
        "Chris Bridges"
      ],
      [
        "Chris Brunskill"
      ],
      [
        "Cormac Purcell"
      ],
      [
        "Nicolas Longépé"
      ],
      [
        "Andrew Markham"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08700v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08700v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T17:59:09Z",
    "updated": "2023-07-17T17:59:09Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08699v1",
    "title": "Pair then Relation: Pair-Net for Panoptic Scene Graph Generation",
    "summary": "Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation\n(SGG) that aims to create a more comprehensive scene graph representation using\npanoptic segmentation instead of boxes. However, current PSG methods have\nlimited performance, which can hinder downstream task development. To improve\nPSG methods, we conducted an in-depth analysis to identify the bottleneck of\nthe current PSG models, finding that inter-object pair-wise recall is a crucial\nfactor which was ignored by previous PSG methods. Based on this, we present a\nnovel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal\nNetwork (PPN) to learn and filter sparse pair-wise relationships between\nsubjects and objects. We also observed the sparse nature of object pairs and\nused this insight to design a lightweight Matrix Learner within the PPN.\nThrough extensive ablation and analysis, our approach significantly improves\nupon leveraging the strong segmenter baseline. Notably, our approach achieves\nnew state-of-the-art results on the PSG benchmark, with over 10% absolute gains\ncompared to PSGFormer. The code of this paper is publicly available at\nhttps://github.com/king159/Pair-Net.",
    "authors": [
      [
        "Jinghao Wang"
      ],
      [
        "Zhengyu Wen"
      ],
      [
        "Xiangtai Li"
      ],
      [
        "Zujin Guo"
      ],
      [
        "Jingkang Yang"
      ],
      [
        "Ziwei Liu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08699v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08699v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T17:58:37Z",
    "updated": "2023-07-17T17:58:37Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08689v1",
    "title": "COLLIE: Systematic Construction of Constrained Text Generation Tasks",
    "summary": "Text generation under constraints have seen increasing interests in natural\nlanguage processing, especially with the rapidly improving capabilities of\nlarge language models. However, existing benchmarks for constrained generation\nusually focus on fixed constraint types (e.g.,generate a sentence containing\ncertain words) that have proved to be easy for state-of-the-art models like\nGPT-4. We present COLLIE, a grammar-based framework that allows the\nspecification of rich, compositional constraints with diverse generation levels\n(word, sentence, paragraph, passage) and modeling challenges (e.g.,language\nunderstanding, logical reasoning, counting, semantic planning). We also develop\ntools for automatic extraction of task instances given a constraint structure\nand a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2080\ninstances comprising 13 constraint structures. We perform systematic\nexperiments across five state-of-the-art instruction-tuned language models and\nanalyze their performances to reveal shortcomings. COLLIE is designed to be\nextensible and lightweight, and we hope the community finds it useful to\ndevelop more complex constraints and evaluations in the future.",
    "authors": [
      [
        "Shunyu Yao"
      ],
      [
        "Howard Chen"
      ],
      [
        "Austin W. Hanjie"
      ],
      [
        "Runzhe Yang"
      ],
      [
        "Karthik Narasimhan"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08689v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08689v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T17:48:51Z",
    "updated": "2023-07-17T17:48:51Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08678v1",
    "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural\n  Language Explanations",
    "summary": "Large language models (LLMs) are trained to imitate humans to explain human\ndecisions. However, do LLMs explain themselves? Can they help humans build\nmental models of how LLMs process different inputs? To answer these questions,\nwe propose to evaluate $\\textbf{counterfactual simulatability}$ of natural\nlanguage explanations: whether an explanation can enable humans to precisely\ninfer the model's outputs on diverse counterfactuals of the explained input.\nFor example, if a model answers \"yes\" to the input question \"Can eagles fly?\"\nwith the explanation \"all birds can fly\", then humans would infer from the\nexplanation that it would also answer \"yes\" to the counterfactual input \"Can\npenguins fly?\". If the explanation is precise, then the model's answer should\nmatch humans' expectations.\n  We implemented two metrics based on counterfactual simulatability: precision\nand generality. We generated diverse counterfactuals automatically using LLMs.\nWe then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on\ntwo tasks: multi-hop factual reasoning and reward modeling. We found that LLM's\nexplanations have low precision and that precision does not correlate with\nplausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may\nnot be a sufficient solution.",
    "authors": [
      [
        "Yanda Chen"
      ],
      [
        "Ruiqi Zhong"
      ],
      [
        "Narutatsu Ri"
      ],
      [
        "Chen Zhao"
      ],
      [
        "He He"
      ],
      [
        "Jacob Steinhardt"
      ],
      [
        "Zhou Yu"
      ],
      [
        "Kathleen McKeown"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08678v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08678v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T17:41:47Z",
    "updated": "2023-07-17T17:41:47Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08674v1",
    "title": "TableGPT: Towards Unifying Tables, Nature Language and Commands into One\n  GPT",
    "summary": "Tables are prevalent in real-world databases, requiring significant time and\neffort for humans to analyze and manipulate. The advancements in large language\nmodels (LLMs) have made it possible to interact with tables using natural\nlanguage input, bringing this capability closer to reality. In this paper, we\npresent TableGPT, a unified fine-tuned framework that enables LLMs to\nunderstand and operate on tables using external functional commands. It\nintroduces the capability to seamlessly interact with tables, enabling a wide\nrange of functionalities such as question answering, data manipulation (e.g.,\ninsert, delete, query, and modify operations), data visualization, analysis\nreport generation, and automated prediction. TableGPT aims to provide\nconvenience and accessibility to users by empowering them to effortlessly\nleverage tabular data. At the core of TableGPT lies the novel concept of global\ntabular representations, which empowers LLMs to gain a comprehensive\nunderstanding of the entire table beyond meta-information. By jointly training\nLLMs on both table and text modalities, TableGPT achieves a deep understanding\nof tabular data and the ability to perform complex operations on tables through\nchain-of-command instructions. Importantly, TableGPT offers the advantage of\nbeing a self-contained system rather than relying on external API interfaces.\nMoreover, it supports efficient data process flow, query rejection (when\nappropriate) and private deployment, enabling faster domain data fine-tuning\nand ensuring data privacy, which enhances the framework's adaptability to\nspecific use cases.",
    "authors": [
      [
        "Liangyu Zha"
      ],
      [
        "Junlin Zhou"
      ],
      [
        "Liyao Li"
      ],
      [
        "Rui Wang"
      ],
      [
        "Qingyi Huang"
      ],
      [
        "Saisai Yang"
      ],
      [
        "Jing Yuan"
      ],
      [
        "Changbao Su"
      ],
      [
        "Xiang Li"
      ],
      [
        "Aofeng Su"
      ],
      [
        "Tao Zhang"
      ],
      [
        "Chen Zhou"
      ],
      [
        "Kaizhe Shou"
      ],
      [
        "Miao Wang"
      ],
      [
        "Wufang Zhu"
      ],
      [
        "Guoshan Lu"
      ],
      [
        "Chao Ye"
      ],
      [
        "Yali Ye"
      ],
      [
        "Wentao Ye"
      ],
      [
        "Yiming Zhang"
      ],
      [
        "Xinglong Deng"
      ],
      [
        "Jie Xu"
      ],
      [
        "Haobo Wang"
      ],
      [
        "Gang Chen"
      ],
      [
        "Junbo Zhao"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08674v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08674v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T17:36:09Z",
    "updated": "2023-07-17T17:36:09Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08663v1",
    "title": "Quaternion Convolutional Neural Networks: Current Advances and Future\n  Directions",
    "summary": "Since their first applications, Convolutional Neural Networks (CNNs) have\nsolved problems that have advanced the state-of-the-art in several domains.\nCNNs represent information using real numbers. Despite encouraging results,\ntheoretical analysis shows that representations such as hyper-complex numbers\ncan achieve richer representational capacities than real numbers, and that\nHamilton products can capture intrinsic interchannel relationships. Moreover,\nin the last few years, experimental research has shown that Quaternion-Valued\nCNNs (QCNNs) can achieve similar performance with fewer parameters than their\nreal-valued counterparts. This paper condenses research in the development of\nQCNNs from its very beginnings. We propose a conceptual organization of current\ntrends and analyze the main building blocks used in the design of QCNN models.\nBased on this conceptual organization, we propose future directions of\nresearch.",
    "authors": [
      [
        "Gerardo Altamirano-Gomez"
      ],
      [
        "Carlos Gershenson"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08663v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08663v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T17:27:06Z",
    "updated": "2023-07-17T17:27:06Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "I.2.0; I.4.0; I.2.7",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08598v1",
    "title": "Glamour muscles: why having a body is not what it means to be embodied",
    "summary": "Embodiment has recently enjoyed renewed consideration as a means to amplify\nthe faculties of smart machines. Proponents of embodiment seem to imply that\noptimizing for movement in physical space promotes something more than the\nacquisition of niche capabilities for solving problems in physical space.\nHowever, there is nothing in principle which should so distinguish the problem\nof action selection in physical space from the problem of action selection in\nmore abstract spaces, like that of language. Rather, what makes embodiment\npersuasive as a means toward higher intelligence is that it promises to\ncapture, but does not actually realize, contingent facts about certain bodies\n(living intelligence) and the patterns of activity associated with them. These\ninclude an active resistance to annihilation and revisable constraints on the\nprocesses that make the world intelligible. To be theoretically or practically\nuseful beyond the creation of niche tools, we argue that \"embodiment\" cannot be\nthe trivial fact of a body, nor its movement through space, but the perpetual\nnegotiation of the function, design, and integrity of that\nbody$\\unicode{x2013}$that is, to participate in what it means to\n$\\textit{constitute}$ a given body. It follows that computer programs which are\nstrictly incapable of traversing physical space might, under the right\nconditions, be more embodied than a walking, talking robot.",
    "authors": [
      [
        "Shawn L. Beaulieu"
      ],
      [
        "Sam Kriegman"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08598v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08598v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T16:09:24Z",
    "updated": "2023-07-17T16:09:24Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.RO",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08581v1",
    "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs",
    "summary": "LLMs have demonstrated remarkable abilities at interacting with humans\nthrough language, especially with the usage of instruction-following data.\nRecent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further\nenlarge their abilities by incorporating multi-modal inputs, including image,\nvideo, and speech. Despite their effectiveness at generating precise and\ndetailed language understanding of the given modality signal, these LLMs give\nup the ability to ground specific parts of inputs, thus only constructing a\ncoarse-grained mapping. However, explicit and informative correspondence\nbetween text and other modalities will not only improve the user experience but\nalso help to expand the application scenario of multi-modal LLMs. Therefore, we\npropose BuboGPT, a multi-modal LLM with visual grounding that can perform\ncross-modal interaction between vision, audio and language, providing\nfine-grained understanding of visual objects and other given modalities. As a\nresult, BuboGPT is able to point out the specific location of an object in the\nimage, when it is generating response or description for that object. Our\ncontributions are two-fold: 1) An off-the-shelf visual grounding module based\non SAM that extracts entities in a sentence and find corresponding masks in the\nimage. 2) A two-stage training scheme and instruction dataset to endow joint\ntext-image-audio understanding. Our experiments show that BuboGPT achieves\nimpressive multi-modality understanding and visual grounding abilities during\nthe interaction with human. It performs consistently well when provided by\narbitrary modality combinations (either aligned or unaligned). Our code, model\nand dataset are available at https://bubo-gpt.github.io .",
    "authors": [
      [
        "Yang Zhao"
      ],
      [
        "Zhijie Lin"
      ],
      [
        "Daquan Zhou"
      ],
      [
        "Zilong Huang"
      ],
      [
        "Jiashi Feng"
      ],
      [
        "Bingyi Kang"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08581v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08581v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T15:51:47Z",
    "updated": "2023-07-17T15:51:47Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08533v1",
    "title": "Nonlinear Processing with Linear Optics",
    "summary": "Deep neural networks have achieved remarkable breakthroughs by leveraging\nmultiple layers of data processing to extract hidden representations, albeit at\nthe cost of large electronic computing power. To enhance energy efficiency and\nspeed, the optical implementation of neural networks aims to harness the\nadvantages of optical bandwidth and the energy efficiency of optical\ninterconnections. In the absence of low-power optical nonlinearities, the\nchallenge in the implementation of multilayer optical networks lies in\nrealizing multiple optical layers without resorting to electronic components.\nIn this study, we present a novel framework that uses multiple scattering that\nis capable of synthesizing programmable linear and nonlinear transformations\nconcurrently at low optical power by leveraging the nonlinear relationship\nbetween the scattering potential, represented by data, and the scattered field.\nTheoretical and experimental investigations show that repeating the data by\nmultiple scattering enables non-linear optical computing at low power\ncontinuous wave light.",
    "authors": [
      [
        "Mustafa Yildirim"
      ],
      [
        "Niyazi Ulas Dinc"
      ],
      [
        "Ilker Oguz"
      ],
      [
        "Demetri Psaltis"
      ],
      [
        "Christophe Moser"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08533v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08533v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T14:49:06Z",
    "updated": "2023-07-17T14:49:06Z",
    "categories": [
      {
        "term": "physics.optics",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.ET",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08532v1",
    "title": "LuckyMera: a Modular AI Framework for Building Hybrid NetHack Agents",
    "summary": "In the last few decades we have witnessed a significant development in\nArtificial Intelligence (AI) thanks to the availability of a variety of\ntestbeds, mostly based on simulated environments and video games. Among those,\nroguelike games offer a very good trade-off in terms of complexity of the\nenvironment and computational costs, which makes them perfectly suited to test\nAI agents generalization capabilities. In this work, we present LuckyMera, a\nflexible, modular, extensible and configurable AI framework built around\nNetHack, a popular terminal-based, single-player roguelike video game. This\nlibrary is aimed at simplifying and speeding up the development of AI agents\ncapable of successfully playing the game and offering a high-level interface\nfor designing game strategies. LuckyMera comes with a set of off-the-shelf\nsymbolic and neural modules (called \"skills\"): these modules can be either\nhard-coded behaviors, or neural Reinforcement Learning approaches, with the\npossibility of creating compositional hybrid solutions. Additionally, LuckyMera\ncomes with a set of utility features to save its experiences in the form of\ntrajectories for further analysis and to use them as datasets to train neural\nmodules, with a direct interface to the NetHack Learning Environment and\nMiniHack. Through an empirical evaluation we validate our skills implementation\nand propose a strong baseline agent that can reach state-of-the-art\nperformances in the complete NetHack game. LuckyMera is open-source and\navailable at https://github.com/Pervasive-AI-Lab/LuckyMera.",
    "authors": [
      [
        "Luigi Quarantiello"
      ],
      [
        "Simone Marzeddu"
      ],
      [
        "Antonio Guzzi"
      ],
      [
        "Vincenzo Lomonaco"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08532v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08532v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T14:46:59Z",
    "updated": "2023-07-17T14:46:59Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08526v1",
    "title": "Image Captions are Natural Prompts for Text-to-Image Models",
    "summary": "With the rapid development of Artificial Intelligence Generated Content\n(AIGC), it has become common practice in many learning tasks to train or\nfine-tune large models on synthetic data due to the data-scarcity and privacy\nleakage problems. Albeit promising with unlimited data generation, owing to\nmassive and diverse information conveyed in real images, it is challenging for\ntext-to-image generative models to synthesize informative training data with\nhand-crafted prompts, which usually leads to inferior generalization\nperformance when training downstream models. In this paper, we theoretically\nanalyze the relationship between the training effect of synthetic data and the\nsynthetic data distribution induced by prompts. Then we correspondingly propose\na simple yet effective method that prompts text-to-image generative models to\nsynthesize more informative and diverse training data. Specifically, we caption\neach real image with the advanced captioning model to obtain informative and\nfaithful prompts that extract class-relevant information and clarify the\npolysemy of class names. The image captions and class names are concatenated to\nprompt generative models for training image synthesis. Extensive experiments on\nImageNette, ImageNet-100, and ImageNet-1K verify that our method significantly\nimproves the performance of models trained on synthetic training data, i.e.,\n10% classification accuracy improvements on average.",
    "authors": [
      [
        "Shiye Lei"
      ],
      [
        "Hao Chen"
      ],
      [
        "Sen Zhang"
      ],
      [
        "Bo Zhao"
      ],
      [
        "Dacheng Tao"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08526v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08526v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T14:38:11Z",
    "updated": "2023-07-17T14:38:11Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08506v1",
    "title": "Does Visual Pretraining Help End-to-End Reasoning?",
    "summary": "We aim to investigate whether end-to-end learning of visual reasoning can be\nachieved with general-purpose neural networks, with the help of visual\npretraining. A positive result would refute the common belief that explicit\nvisual abstraction (e.g. object detection) is essential for compositional\ngeneralization on visual reasoning, and confirm the feasibility of a neural\nnetwork \"generalist\" to solve visual recognition and reasoning tasks. We\npropose a simple and general self-supervised framework which \"compresses\" each\nvideo frame into a small set of tokens with a transformer network, and\nreconstructs the remaining frames based on the compressed temporal context. To\nminimize the reconstruction loss, the network must learn a compact\nrepresentation for each image, as well as capture temporal dynamics and object\npermanence from temporal context. We perform evaluation on two visual reasoning\nbenchmarks, CATER and ACRE. We observe that pretraining is essential to achieve\ncompositional generalization for end-to-end visual reasoning. Our proposed\nframework outperforms traditional supervised pretraining, including image\nclassification and explicit object detection, by large margins.",
    "authors": [
      [
        "Chen Sun"
      ],
      [
        "Calvin Luo"
      ],
      [
        "Xingyi Zhou"
      ],
      [
        "Anurag Arnab"
      ],
      [
        "Cordelia Schmid"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08506v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08506v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T14:08:38Z",
    "updated": "2023-07-17T14:08:38Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08496v1",
    "title": "Can We Trust Race Prediction?",
    "summary": "In the absence of sensitive race and ethnicity data, researchers, regulators,\nand firms alike turn to proxies. In this paper, I train a Bidirectional Long\nShort-Term Memory (BiLSTM) model on a novel dataset of voter registration data\nfrom all 50 US states and create an ensemble that achieves up to 36.8% higher\nout of sample (OOS) F1 scores than the best performing machine learning models\nin the literature. Additionally, I construct the most comprehensive database of\nfirst and surname distributions in the US in order to improve the coverage and\naccuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved\nFirstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality\nbenchmark dataset in order to fairly compare existing models and aid future\nmodel developers.",
    "authors": [
      [
        "Cangyuan Li"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08496v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08496v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T13:59:07Z",
    "updated": "2023-07-17T13:59:07Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CY",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "stat.ML",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08484v1",
    "title": "Navigating Fairness Measures and Trade-Offs",
    "summary": "In order to monitor and prevent bias in AI systems we can use a wide range of\n(statistical) fairness measures. However, it is mathematically impossible to\noptimize for all of these measures at the same time. In addition, optimizing a\nfairness measure often greatly reduces the accuracy of the system (Kozodoi et\nal, 2022). As a result, we need a substantive theory that informs us how to\nmake these decisions and for what reasons. I show that by using Rawls' notion\nof justice as fairness, we can create a basis for navigating fairness measures\nand the accuracy trade-off. In particular, this leads to a principled choice\nfocusing on both the most vulnerable groups and the type of fairness measure\nthat has the biggest impact on that group. This also helps to close part of the\ngap between philosophical accounts of distributive justice and the fairness\nliterature that has been observed (Kuppler et al, 2021) and to operationalise\nthe value of fairness.",
    "authors": [
      [
        "Stefan Buijsman"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08484v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08484v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T13:45:47Z",
    "updated": "2023-07-17T13:45:47Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08481v1",
    "title": "Derivation-Graph-Based Characterizations of Decidable Existential Rule\n  Sets",
    "summary": "This paper establishes alternative characterizations of very expressive\nclasses of existential rule sets with decidable query entailment. We consider\nthe notable class of greedy bounded-treewidth sets (gbts) and a new,\ngeneralized variant, called weakly gbts (wgbts). Revisiting and building on the\nnotion of derivation graphs, we define (weakly) cycle-free derivation graph\nsets ((w)cdgs) and employ elaborate proof-theoretic arguments to obtain that\ngbts and cdgs coincide, as do wgbts and wcdgs. These novel characterizations\nadvance our analytic proof-theoretic understanding of existential rules and\nwill likely be instrumental in practice.",
    "authors": [
      [
        "Tim S. Lyon"
      ],
      [
        "Sebastian Rudolph"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08481v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08481v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T13:39:08Z",
    "updated": "2023-07-17T13:39:08Z",
    "categories": [
      {
        "term": "cs.LO",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.DB",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "math.LO",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08471v1",
    "title": "Clarifying the Half Full or Half Empty Question: Multimodal Container\n  Classification",
    "summary": "Multimodal integration is a key component of allowing robots to perceive the\nworld. Multimodality comes with multiple challenges that have to be considered,\nsuch as how to integrate and fuse the data. In this paper, we compare different\npossibilities of fusing visual, tactile and proprioceptive data. The data is\ndirectly recorded on the NICOL robot in an experimental setup in which the\nrobot has to classify containers and their content. Due to the different nature\nof the containers, the use of the modalities can wildly differ between the\nclasses. We demonstrate the superiority of multimodal solutions in this use\ncase and evaluate three fusion strategies that integrate the data at different\ntime steps. We find that the accuracy of the best fusion strategy is 15% higher\nthan the best strategy using only one singular sense.",
    "authors": [
      [
        "Josua Spisak"
      ],
      [
        "Matthias Kerzel"
      ],
      [
        "Stefan Wermter"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08471v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08471v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T13:26:44Z",
    "updated": "2023-07-17T13:26:44Z",
    "categories": [
      {
        "term": "cs.RO",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08461v1",
    "title": "Towards eXplainable AI for Mobility Data Science",
    "summary": "This paper presents our ongoing work towards XAI for Mobility Data Science\napplications, focusing on explainable models that can learn from dense\ntrajectory data, such as GPS tracks of vehicles and vessels using temporal\ngraph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI\nstudies, argue the need for comprehensible explanations with human-centered\napproaches, and outline a research path toward XAI for Mobility Data Science.",
    "authors": [
      [
        "Anahid Jalali"
      ],
      [
        "Anita Graser"
      ],
      [
        "Clemens Heistracher"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08461v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08461v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T13:06:33Z",
    "updated": "2023-07-17T13:06:33Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "F.2.2",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08430v1",
    "title": "Long-range Dependency based Multi-Layer Perceptron for Heterogeneous\n  Information Networks",
    "summary": "Existing heterogeneous graph neural networks (HGNNs) have achieved great\nsuccess in utilizing the rich semantic information in heterogeneous information\nnetworks (HINs). However, few works have delved into the utilization of\nlong-range dependencies in HINs, which is extremely valuable as many real-world\nHINs are sparse, and each node has only a few directly connected neighbors.\nAlthough some HGNNs can utilize distant neighbors by stacking multiple layers\nor leveraging long meta-paths, the exponentially increased number of nodes in\nthe receptive field or the number of meta-paths incurs high computation and\nmemory costs. To address these issues, we investigate the importance of\ndifferent meta-paths and propose Long-range Dependency based Multi-Layer\nPerceptron (LDMLP). Specifically, to solve the high-cost problem of leveraging\nlong-range dependencies, LDMLP adopts a search stage to discover effective\nmeta-paths automatically, reducing the exponentially increased number of\nmeta-paths to a constant. To avoid the influence of specific modules on search\nresults, LDMLP utilizes a simple architecture with only multi-layer perceptions\nin the search stage, improving the generalization of searched meta-paths. As a\nresult, the searched meta-paths not only perform well in LDMLP but also enable\nother HGNNs like HAN and SeHGNN to perform better. Extensive experiments on\neight heterogeneous datasets demonstrate that LDMLP achieves state-of-the-art\nperformance while enjoying high efficiency and generalization, especially on\nsparse HINs.",
    "authors": [
      [
        "Chao Li"
      ],
      [
        "Zijie Guo"
      ],
      [
        "Qiuting He"
      ],
      [
        "Hao Xu"
      ],
      [
        "Kun He"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08430v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08430v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T12:20:07Z",
    "updated": "2023-07-17T12:20:07Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08424v1",
    "title": "An Indefensible Attack: Label-Only Model Inversion via Conditional\n  Diffusion Model",
    "summary": "Model inversion attacks (MIAs) are aimed at recovering private data from a\ntarget model's training set, which poses a threat to the privacy of deep\nlearning models. MIAs primarily focus on the white-box scenario where the\nattacker has full access to the structure and parameters of the target model.\nHowever, practical applications are black-box, it is not easy for adversaries\nto obtain model-related parameters, and various models only output predicted\nlabels. Existing black-box MIAs primarily focused on designing the optimization\nstrategy, and the generative model is only migrated from the GAN used in\nwhite-box MIA. Our research is the pioneering study of feasible attack models\nin label-only black-box scenarios, to the best of our knowledge.\n  In this paper, we develop a novel method of MIA using the conditional\ndiffusion model to recover the precise sample of the target without any extra\noptimization, as long as the target model outputs the label. Two primary\ntechniques are introduced to execute the attack. Firstly, select an auxiliary\ndataset that is relevant to the target model task, and the labels predicted by\nthe target model are used as conditions to guide the training process.\nSecondly, target labels and random standard normally distributed noise are\ninput into the trained conditional diffusion model, generating target samples\nwith pre-defined guidance strength. We then filter out the most robust and\nrepresentative samples. Furthermore, we propose for the first time to use\nLearned Perceptual Image Patch Similarity (LPIPS) as one of the evaluation\nmetrics for MIA, with systematic quantitative and qualitative evaluation in\nterms of attack accuracy, realism, and similarity. Experimental results show\nthat this method can generate similar and accurate data to the target without\noptimization and outperforms generators of previous approaches in the\nlabel-only scenario.",
    "authors": [
      [
        "Rongke Liu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08424v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08424v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T12:14:24Z",
    "updated": "2023-07-17T12:14:24Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08421v1",
    "title": "Systematic Comparison of Software Agents and Digital Twins: Differences,\n  Similarities, and Synergies in Industrial Production",
    "summary": "To achieve a highly agile and flexible production, it is envisioned that\nindustrial production systems gradually become more decentralized,\ninterconnected, and intelligent. Within this vision, production assets\ncollaborate with each other, exhibiting a high degree of autonomy. Furthermore,\nknowledge about individual production assets is readily available throughout\ntheir entire life-cycles. To realize this vision, adequate use of information\ntechnology is required. Two commonly applied software paradigms in this context\nare Software Agents (referred to as Agents) and Digital Twins (DTs). This work\npresents a systematic comparison of Agents and DTs in industrial applications.\nThe goal of the study is to determine the differences, similarities, and\npotential synergies between the two paradigms. The comparison is based on the\npurposes for which Agents and DTs are applied, the properties and capabilities\nexhibited by these software paradigms, and how they can be allocated within the\nReference Architecture Model Industry 4.0. The comparison reveals that Agents\nare commonly employed in the collaborative planning and execution of production\nprocesses, while DTs typically play a more passive role in monitoring\nproduction resources and processing information. Although these observations\nimply characteristic sets of capabilities and properties for both Agents and\nDTs, a clear and definitive distinction between the two paradigms cannot be\nmade. Instead, the analysis indicates that production assets utilizing a\ncombination of Agents and DTs would demonstrate high degrees of intelligence,\nautonomy, sociability, and fidelity. To achieve this, further standardization\nis required, particularly in the field of DTs.",
    "authors": [
      [
        "Lasse Matthias Reinpold"
      ],
      [
        "Lukas Peter Wagner"
      ],
      [
        "Felix Gehlhoff"
      ],
      [
        "Malte Ramonat"
      ],
      [
        "Maximilian Kilthau"
      ],
      [
        "Milapji Singh Gill"
      ],
      [
        "Jonathan Tobias Reif"
      ],
      [
        "Vincent Henkel"
      ],
      [
        "Lena Scholz"
      ],
      [
        "Alexander Fay"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08421v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08421v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T12:09:18Z",
    "updated": "2023-07-17T12:09:18Z",
    "categories": [
      {
        "term": "cs.SE",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08411v1",
    "title": "Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs",
    "summary": "Biomedical datasets are often modeled as knowledge graphs (KGs) because they\ncapture the multi-relational, heterogeneous, and dynamic natures of biomedical\nsystems. KG completion (KGC), can, therefore, help researchers make predictions\nto inform tasks like drug repositioning. While previous approaches for KGC were\neither rule-based or embedding-based, hybrid approaches based on neurosymbolic\nartificial intelligence are becoming more popular. Many of these methods\npossess unique characteristics which make them even better suited toward\nbiomedical challenges. Here, we survey such approaches with an emphasis on\ntheir utilities and prospective benefits for biomedicine.",
    "authors": [
      [
        "Lauren Nicole DeLong"
      ],
      [
        "Ramon Fernández Mir"
      ],
      [
        "Zonglin Ji"
      ],
      [
        "Fiona Niamh Coulter Smith"
      ],
      [
        "Jacques D. Fleuriot"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08411v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08411v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T11:47:05Z",
    "updated": "2023-07-17T11:47:05Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LO",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08401v1",
    "title": "A Novel Multiagent Flexibility Aggregation Framework",
    "summary": "The increasing number of Distributed Energy Resources (DERs) in the emerging\nSmart Grid, has created an imminent need for intelligent multiagent frameworks\nable to utilize these assets efficiently. In this paper, we propose a novel DER\naggregation framework, encompassing a multiagent architecture and various types\nof mechanisms for the effective management and efficient integration of DERs in\nthe Grid. One critical component of our architecture is the Local Flexibility\nEstimators (LFEs) agents, which are key for offloading the Aggregator from\nserious or resource-intensive responsibilities -- such as addressing privacy\nconcerns and predicting the accuracy of DER statements regarding their offered\ndemand response services. The proposed framework allows the formation of\nefficient LFE cooperatives. To this end, we developed and deployed a variety of\ncooperative member selection mechanisms, including (a) scoring rules, and (b)\n(deep) reinforcement learning. We use data from the well-known PowerTAC\nsimulator to systematically evaluate our framework. Our experiments verify its\neffectiveness for incorporating heterogeneous DERs into the Grid in an\nefficient manner. In particular, when using the well-known probabilistic\nprediction accuracy-incentivizing CRPS scoring rule as a selection mechanism,\nour framework results in increased average payments for participants, when\ncompared with traditional commercial aggregators.",
    "authors": [
      [
        "Stavros Orfanoudakis"
      ],
      [
        "Georgios Chalkiadakis"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08401v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08401v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T11:36:15Z",
    "updated": "2023-07-17T11:36:15Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08368v1",
    "title": "Gender mobility in the labor market with skills-based matching models",
    "summary": "Skills-based matching promises mobility of workers between different sectors\nand occupations in the labor market. In this case, job seekers can look for\njobs they do not yet have experience in, but for which they do have relevant\nskills. Currently, there are multiple occupations with a skewed gender\ndistribution. For skills-based matching, it is unclear if and how a shift in\nthe gender distribution, which we call gender mobility, between occupations\nwill be effected. It is expected that the skills-based matching approach will\nlikely be data-driven, including computational language models and supervised\nlearning methods.\n  This work, first, shows the presence of gender segregation in language\nmodel-based skills representation of occupations. Second, we assess the use of\nthese representations in a potential application based on simulated data, and\nshow that the gender segregation is propagated by various data-driven\nskills-based matching models.These models are based on different language\nrepresentations (bag of words, word2vec, and BERT), and distance metrics\n(static and machine learning-based). Accordingly, we show how skills-based\nmatching approaches can be evaluated and compared on matching performance as\nwell as on the risk of gender segregation. Making the gender segregation bias\nof models more explicit can help in generating healthy trust in the use of\nthese models in practice.",
    "authors": [
      [
        "Ajaya Adhikari"
      ],
      [
        "Steven Vethman"
      ],
      [
        "Daan Vos"
      ],
      [
        "Marc Lenz"
      ],
      [
        "Ioana Cocu"
      ],
      [
        "Ioannis Tolios"
      ],
      [
        "Cor J. Veenman"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08368v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08368v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T10:06:21Z",
    "updated": "2023-07-17T10:06:21Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08347v1",
    "title": "M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models\n  and Latent Space Geometry Optimization",
    "summary": "Medical vision-language models enable co-learning and integrating features\nfrom medical imaging and clinical text. However, these models are not easy to\ntrain and the latent representation space can be complex. Here we propose a\nnovel way for pre-training and regularising medical vision-language models. The\nproposed method, named Medical vision-language pre-training with Frozen\nlanguage models and Latent spAce Geometry optimization (M-FLAG), leverages a\nfrozen language model for training stability and efficiency and introduces a\nnovel orthogonality loss to harmonize the latent space geometry. We demonstrate\nthe potential of the pre-trained model on three downstream tasks: medical image\nclassification, segmentation, and object detection. Extensive experiments\nacross five public datasets demonstrate that M-FLAG significantly outperforms\nexisting medical vision-language pre-training approaches and reduces the number\nof parameters by 78\\%. Notably, M-FLAG achieves outstanding performance on the\nsegmentation task while using only 1\\% of the RSNA dataset, even outperforming\nImageNet pre-trained models that have been fine-tuned using 100\\% of the data.",
    "authors": [
      [
        "Che Liu"
      ],
      [
        "Sibo Cheng"
      ],
      [
        "Chen Chen"
      ],
      [
        "Mengyun Qiao"
      ],
      [
        "Weitong Zhang"
      ],
      [
        "Anand Shah"
      ],
      [
        "Wenjia Bai"
      ],
      [
        "Rossella Arcucci"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08347v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08347v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T09:38:41Z",
    "updated": "2023-07-17T09:38:41Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08339v1",
    "title": "Multi-Task Cross-Modality Attention-Fusion for 2D Object Detection",
    "summary": "Accurate and robust object detection is critical for autonomous driving.\nImage-based detectors face difficulties caused by low visibility in adverse\nweather conditions. Thus, radar-camera fusion is of particular interest but\npresents challenges in optimally fusing heterogeneous data sources. To approach\nthis issue, we propose two new radar preprocessing techniques to better align\nradar and camera data. In addition, we introduce a Multi-Task Cross-Modality\nAttention-Fusion Network (MCAF-Net) for object detection, which includes two\nnew fusion blocks. These allow for exploiting information from the feature maps\nmore comprehensively. The proposed algorithm jointly detects objects and\nsegments free space, which guides the model to focus on the more relevant part\nof the scene, namely, the occupied space. Our approach outperforms current\nstate-of-the-art radar-camera fusion-based object detectors in the nuScenes\ndataset and achieves more robust results in adverse weather conditions and\nnighttime scenarios.",
    "authors": [
      [
        "Huawei Sun"
      ],
      [
        "Hao Feng"
      ],
      [
        "Georg Stettinger"
      ],
      [
        "Lorenzo Servadei"
      ],
      [
        "Robert Wille"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08339v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08339v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T09:26:13Z",
    "updated": "2023-07-17T09:26:13Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.MM",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08327v1",
    "title": "Analyzing the Impact of Adversarial Examples on Explainable Machine\n  Learning",
    "summary": "Adversarial attacks are a type of attack on machine learning models where an\nattacker deliberately modifies the inputs to cause the model to make incorrect\npredictions. Adversarial attacks can have serious consequences, particularly in\napplications such as autonomous vehicles, medical diagnosis, and security\nsystems. Work on the vulnerability of deep learning models to adversarial\nattacks has shown that it is very easy to make samples that make a model\npredict things that it doesn't want to. In this work, we analyze the impact of\nmodel interpretability due to adversarial attacks on text classification\nproblems. We develop an ML-based classification model for text data. Then, we\nintroduce the adversarial perturbations on the text data to understand the\nclassification performance after the attack. Subsequently, we analyze and\ninterpret the model's explainability before and after the attack",
    "authors": [
      [
        "Prathyusha Devabhakthini"
      ],
      [
        "Sasmita Parida"
      ],
      [
        "Raj Mani Shukla"
      ],
      [
        "Suvendu Chandan Nayak"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08327v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08327v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T08:50:36Z",
    "updated": "2023-07-17T08:50:36Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08309v1",
    "title": "LogPrécis: Unleashing Language Models for Automated Shell Log Analysis",
    "summary": "The collection of security-related logs holds the key to understanding attack\nbehaviors and diagnosing vulnerabilities. Still, their analysis remains a\ndaunting challenge. Recently, Language Models (LMs) have demonstrated unmatched\npotential in understanding natural and programming languages. The question\narises whether and how LMs could be also useful for security experts since\ntheir logs contain intrinsically confused and obfuscated information. In this\npaper, we systematically study how to benefit from the state-of-the-art in LM\nto automatically analyze text-like Unix shell attack logs. We present a\nthorough design methodology that leads to LogPr\\'ecis. It receives as input raw\nshell sessions and automatically identifies and assigns the attacker tactic to\neach portion of the session, i.e., unveiling the sequence of the attacker's\ngoals. We demonstrate LogPr\\'ecis capability to support the analysis of two\nlarge datasets containing about 400,000 unique Unix shell attacks. LogPr\\'ecis\nreduces them into about 3,000 fingerprints, each grouping sessions with the\nsame sequence of tactics. The abstraction it provides lets the analyst better\nunderstand attacks, identify fingerprints, detect novelty, link similar\nattacks, and track families and mutations. Overall, LogPr\\'ecis, released as\nopen source, paves the way for better and more responsive defense against\ncyberattacks.",
    "authors": [
      [
        "Matteo Boffa"
      ],
      [
        "Rodolfo Vieira Valentim"
      ],
      [
        "Luca Vassio"
      ],
      [
        "Danilo Giordano"
      ],
      [
        "Idilio Drago"
      ],
      [
        "Marco Mellia"
      ],
      [
        "Zied Ben Houidi"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08309v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08309v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T08:09:40Z",
    "updated": "2023-07-17T08:09:40Z",
    "categories": [
      {
        "term": "cs.CR",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.NI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08308v1",
    "title": "A Novel Multi-Task Model Imitating Dermatologists for Accurate\n  Differential Diagnosis of Skin Diseases in Clinical Images",
    "summary": "Skin diseases are among the most prevalent health issues, and accurate\ncomputer-aided diagnosis methods are of importance for both dermatologists and\npatients. However, most of the existing methods overlook the essential domain\nknowledge required for skin disease diagnosis. A novel multi-task model, namely\nDermImitFormer, is proposed to fill this gap by imitating dermatologists'\ndiagnostic procedures and strategies. Through multi-task learning, the model\nsimultaneously predicts body parts and lesion attributes in addition to the\ndisease itself, enhancing diagnosis accuracy and improving diagnosis\ninterpretability. The designed lesion selection module mimics dermatologists'\nzoom-in action, effectively highlighting the local lesion features from noisy\nbackgrounds. Additionally, the presented cross-interaction module explicitly\nmodels the complicated diagnostic reasoning between body parts, lesion\nattributes, and diseases. To provide a more robust evaluation of the proposed\nmethod, a large-scale clinical image dataset of skin diseases with\nsignificantly more cases than existing datasets has been established. Extensive\nexperiments on three different datasets consistently demonstrate the\nstate-of-the-art recognition performance of the proposed approach.",
    "authors": [
      [
        "Yan-Jie Zhou"
      ],
      [
        "Wei Liu"
      ],
      [
        "Yuan Gao"
      ],
      [
        "Jing Xu"
      ],
      [
        "Le Lu"
      ],
      [
        "Yuping Duan"
      ],
      [
        "Hao Cheng"
      ],
      [
        "Na Jin"
      ],
      [
        "Xiaoyong Man"
      ],
      [
        "Shuang Zhao"
      ],
      [
        "Yu Wang"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08308v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08308v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T08:05:30Z",
    "updated": "2023-07-17T08:05:30Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08304v1",
    "title": "Efficient Computation of Counterfactual Bounds",
    "summary": "We assume to be given structural equations over discrete variables inducing a\ndirected acyclic graph, namely, a structural causal model, together with data\nabout its internal nodes. The question we want to answer is how we can compute\nbounds for partially identifiable counterfactual queries from such an input. We\nstart by giving a map from structural casual models to credal networks. This\nallows us to compute exact counterfactual bounds via algorithms for credal nets\non a subclass of structural causal models. Exact computation is going to be\ninefficient in general given that, as we show, causal inference is NP-hard even\non polytrees. We target then approximate bounds via a causal EM scheme. We\nevaluate their accuracy by providing credible intervals on the quality of the\napproximation; we show through a synthetic benchmark that the EM scheme\ndelivers accurate results in a fair number of runs. In the course of the\ndiscussion, we also point out what seems to be a neglected limitation to the\ntrending idea that counterfactual bounds can be computed without knowledge of\nthe structural equations. We also present a real case study on palliative care\nto show how our algorithms can readily be used for practical purposes.",
    "authors": [
      [
        "Marco Zaffalon"
      ],
      [
        "Alessandro Antonucci"
      ],
      [
        "Rafael Cabañas"
      ],
      [
        "David Huber"
      ],
      [
        "Dario Azzimonti"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08304v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08304v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T07:59:47Z",
    "updated": "2023-07-17T07:59:47Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08303v1",
    "title": "Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language\n  Models",
    "summary": "Dense retrieval (DR) converts queries and documents into dense embeddings and\nmeasures the similarity between queries and documents in vector space. One of\nthe challenges in DR is the lack of domain-specific training data. While DR\nmodels can learn from large-scale public datasets like MS MARCO through\ntransfer learning, evidence shows that not all DR models and domains can\nbenefit from transfer learning equally. Recently, some researchers have\nresorted to large language models (LLMs) to improve the zero-shot and few-shot\nDR models. However, the hard prompts or human-written prompts utilized in these\nworks cannot guarantee the good quality of generated weak queries. To tackle\nthis, we propose soft prompt tuning for augmenting DR (SPTAR): For each task,\nwe leverage soft prompt-tuning to optimize a task-specific soft prompt on\nlimited ground truth data and then prompt the LLMs to tag unlabeled documents\nwith weak queries, yielding enough weak document-query pairs to train\ntask-specific dense retrievers. We design a filter to select high-quality\nexample document-query pairs in the prompt to further improve the quality of\nweak tagged queries. To the best of our knowledge, there is no prior work\nutilizing soft prompt tuning to augment DR models. The experiments demonstrate\nthat SPTAR outperforms the unsupervised baselines BM25 and the recently\nproposed LLMs-based augmentation method for DR.",
    "authors": [
      [
        "Zhiyuan Peng"
      ],
      [
        "Xuyang Wu"
      ],
      [
        "Yi Fang"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08303v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08303v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T07:55:47Z",
    "updated": "2023-07-17T07:55:47Z",
    "categories": [
      {
        "term": "cs.IR",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08300v1",
    "title": "ShiftNAS: Improving One-shot NAS via Probability Shift",
    "summary": "One-shot Neural architecture search (One-shot NAS) has been proposed as a\ntime-efficient approach to obtain optimal subnet architectures and weights\nunder different complexity cases by training only once. However, the subnet\nperformance obtained by weight sharing is often inferior to the performance\nachieved by retraining. In this paper, we investigate the performance gap and\nattribute it to the use of uniform sampling, which is a common approach in\nsupernet training. Uniform sampling concentrates training resources on subnets\nwith intermediate computational resources, which are sampled with high\nprobability. However, subnets with different complexity regions require\ndifferent optimal training strategies for optimal performance. To address the\nproblem of uniform sampling, we propose ShiftNAS, a method that can adjust the\nsampling probability based on the complexity of subnets. We achieve this by\nevaluating the performance variation of subnets with different complexity and\ndesigning an architecture generator that can accurately and efficiently provide\nsubnets with the desired complexity. Both the sampling probability and the\narchitecture generator can be trained end-to-end in a gradient-based manner.\nWith ShiftNAS, we can directly obtain the optimal model architecture and\nparameters for a given computational complexity. We evaluate our approach on\nmultiple visual network models, including convolutional neural networks (CNNs)\nand vision transformers (ViTs), and demonstrate that ShiftNAS is\nmodel-agnostic. Experimental results on ImageNet show that ShiftNAS can improve\nthe performance of one-shot NAS without additional consumption. Source codes\nare available at https://github.com/bestfleer/ShiftNAS.",
    "authors": [
      [
        "Mingyang Zhang"
      ],
      [
        "Xinyi Yu"
      ],
      [
        "Haodong Zhao"
      ],
      [
        "Linlin Ou"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08300v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08300v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T07:53:23Z",
    "updated": "2023-07-17T07:53:23Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08286v1",
    "title": "Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature\n  Connectivity",
    "summary": "Recent work has revealed many intriguing empirical phenomena in neural\nnetwork training, despite the poorly understood and highly complex loss\nlandscapes and training dynamics. One of these phenomena, Linear Mode\nConnectivity (LMC), has gained considerable attention due to the intriguing\nobservation that different solutions can be connected by a linear path in the\nparameter space while maintaining near-constant training and test losses. In\nthis work, we introduce a stronger notion of linear connectivity, Layerwise\nLinear Feature Connectivity (LLFC), which says that the feature maps of every\nlayer in different trained networks are also linearly connected. We provide\ncomprehensive empirical evidence for LLFC across a wide range of settings,\ndemonstrating that whenever two trained networks satisfy LMC (via either\nspawning or permutation methods), they also satisfy LLFC in nearly all the\nlayers. Furthermore, we delve deeper into the underlying factors contributing\nto LLFC, which reveal new insights into the spawning and permutation\napproaches. The study of LLFC transcends and advances our understanding of LMC\nby adopting a feature-learning perspective.",
    "authors": [
      [
        "Zhanpeng Zhou"
      ],
      [
        "Yongyi Yang"
      ],
      [
        "Xiaojiang Yang"
      ],
      [
        "Junchi Yan"
      ],
      [
        "Wei Hu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08286v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08286v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T07:16:28Z",
    "updated": "2023-07-17T07:16:28Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08262v1",
    "title": "Team Badminseok at IJCAI CoachAI Badminton Challenge 2023: Multi-Layer\n  Multi-Input Transformer Network (MuLMINet) with Weighted Loss",
    "summary": "The increasing use of artificial intelligence (AI) technology in turn-based\nsports, such as badminton, has sparked significant interest in evaluating\nstrategies through the analysis of match video data. Predicting future shots\nbased on past ones plays a vital role in coaching and strategic planning. In\nthis study, we present a Multi-Layer Multi-Input Transformer Network (MuLMINet)\nthat leverages professional badminton player match data to accurately predict\nfuture shot types and area coordinates. Our approach resulted in achieving the\nrunner-up (2nd place) in the IJCAI CoachAI Badminton Challenge 2023, Track 2.\nTo facilitate further research, we have made our code publicly accessible\nonline, contributing to the broader research community's knowledge and\nadvancements in the field of AI-assisted sports analysis.",
    "authors": [
      [
        "Minwoo Seong"
      ],
      [
        "Jeongseok Oh"
      ],
      [
        "SeungJun Kim"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08262v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08262v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T06:10:03Z",
    "updated": "2023-07-17T06:10:03Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08242v1",
    "title": "Lifted Sequential Planning with Lazy Constraint Generation Solvers",
    "summary": "This paper studies the possibilities made open by the use of Lazy Clause\nGeneration (LCG) based approaches to Constraint Programming (CP) for tackling\nsequential classical planning. We propose a novel CP model based on seminal\nideas on so-called lifted causal encodings for planning as satisfiability, that\ndoes not require grounding, as choosing groundings for functions and action\nschemas becomes an integral part of the problem of designing valid plans. This\nencoding does not require encoding frame axioms, and does not explicitly\nrepresent states as decision variables for every plan step. We also present a\npropagator procedure that illustrates the possibilities of LCG to widen the\nkind of inference methods considered to be feasible in planning as (iterated)\nCSP solving. We test encodings and propagators over classic IPC and recently\nproposed benchmarks for lifted planning, and report that for planning problem\ninstances requiring fewer plan steps our methods compare very well with the\nstate-of-the-art in optimal sequential planning.",
    "authors": [
      [
        "Anubhav Singh"
      ],
      [
        "Miquel Ramirez"
      ],
      [
        "Nir Lipovetzky"
      ],
      [
        "Peter J. Stuckey"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08242v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08242v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T04:54:58Z",
    "updated": "2023-07-17T04:54:58Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "I.2.8; I.2.4",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08233v1",
    "title": "ROFusion: Efficient Object Detection using Hybrid Point-wise\n  Radar-Optical Fusion",
    "summary": "Radars, due to their robustness to adverse weather conditions and ability to\nmeasure object motions, have served in autonomous driving and intelligent\nagents for years. However, Radar-based perception suffers from its unintuitive\nsensing data, which lack of semantic and structural information of scenes. To\ntackle this problem, camera and Radar sensor fusion has been investigated as a\ntrending strategy with low cost, high reliability and strong maintenance. While\nmost recent works explore how to explore Radar point clouds and images, rich\ncontextual information within Radar observation are discarded. In this paper,\nwe propose a hybrid point-wise Radar-Optical fusion approach for object\ndetection in autonomous driving scenarios. The framework benefits from dense\ncontextual information from both the range-doppler spectrum and images which\nare integrated to learn a multi-modal feature representation. Furthermore, we\npropose a novel local coordinate formulation, tackling the object detection\ntask in an object-centric coordinate. Extensive results show that with the\ninformation gained from optical images, we could achieve leading performance in\nobject detection (97.69\\% recall) compared to recent state-of-the-art methods\nFFT-RadNet (82.86\\% recall). Ablation studies verify the key design choices and\npracticability of our approach given machine generated imperfect detections.\nThe code will be available at https://github.com/LiuLiu-55/ROFusion.",
    "authors": [
      [
        "Liu Liu"
      ],
      [
        "Shuaifeng Zhi"
      ],
      [
        "Zhenhua Du"
      ],
      [
        "Li Liu"
      ],
      [
        "Xinyu Zhang"
      ],
      [
        "Kai Huo"
      ],
      [
        "Weidong Jiang"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08233v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08233v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T04:25:46Z",
    "updated": "2023-07-17T04:25:46Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08225v1",
    "title": "Harnessing Scalable Transactional Stream Processing for Managing Large\n  Language Models [Vision]",
    "summary": "Large Language Models (LLMs) have demonstrated extraordinary performance\nacross a broad array of applications, from traditional language processing\ntasks to interpreting structured sequences like time-series data. Yet, their\neffectiveness in fast-paced, online decision-making environments requiring\nswift, accurate, and concurrent responses poses a significant challenge. This\npaper introduces TStreamLLM, a revolutionary framework integrating\nTransactional Stream Processing (TSP) with LLM management to achieve remarkable\nscalability and low latency. By harnessing the scalability, consistency, and\nfault tolerance inherent in TSP, TStreamLLM aims to manage continuous &\nconcurrent LLM updates and usages efficiently. We showcase its potential\nthrough practical use cases like real-time patient monitoring and intelligent\ntraffic management. The exploration of synergies between TSP and LLM management\ncan stimulate groundbreaking developments in AI and database research. This\npaper provides a comprehensive overview of challenges and opportunities in this\nemerging field, setting forth a roadmap for future exploration and development.",
    "authors": [
      [
        "Shuhao Zhang"
      ],
      [
        "Xianzhi Zeng"
      ],
      [
        "Yuhao Wu"
      ],
      [
        "Zhonghao Yang"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08225v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08225v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T04:01:02Z",
    "updated": "2023-07-17T04:01:02Z",
    "categories": [
      {
        "term": "cs.DB",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.DC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08197v1",
    "title": "Towards Self-Assembling Artificial Neural Networks through Neural\n  Developmental Programs",
    "summary": "Biological nervous systems are created in a fundamentally different way than\ncurrent artificial neural networks. Despite its impressive results in a variety\nof different domains, deep learning often requires considerable engineering\neffort to design high-performing neural architectures. By contrast, biological\nnervous systems are grown through a dynamic self-organizing process. In this\npaper, we take initial steps toward neural networks that grow through a\ndevelopmental process that mirrors key properties of embryonic development in\nbiological organisms. The growth process is guided by another neural network,\nwhich we call a Neural Developmental Program (NDP) and which operates through\nlocal communication alone. We investigate the role of neural growth on\ndifferent machine learning benchmarks and different optimization methods\n(evolutionary training, online RL, offline RL, and supervised learning).\nAdditionally, we highlight future research directions and opportunities enabled\nby having self-organization driving the growth of neural networks.",
    "authors": [
      [
        "Elias Najarro"
      ],
      [
        "Shyam Sudhakaran"
      ],
      [
        "Sebastian Risi"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08197v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08197v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T01:58:52Z",
    "updated": "2023-07-17T01:58:52Z",
    "categories": [
      {
        "term": "cs.NE",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08192v1",
    "title": "HOPE: High-order Polynomial Expansion of Black-box Neural Networks",
    "summary": "Despite their remarkable performance, deep neural networks remain mostly\n``black boxes'', suggesting inexplicability and hindering their wide\napplications in fields requiring making rational decisions. Here we introduce\nHOPE (High-order Polynomial Expansion), a method for expanding a network into a\nhigh-order Taylor polynomial on a reference input. Specifically, we derive the\nhigh-order derivative rule for composite functions and extend the rule to\nneural networks to obtain their high-order derivatives quickly and accurately.\nFrom these derivatives, we can then derive the Taylor polynomial of the neural\nnetwork, which provides an explicit expression of the network's local\ninterpretations. Numerical analysis confirms the high accuracy, low\ncomputational complexity, and good convergence of the proposed method.\nMoreover, we demonstrate HOPE's wide applications built on deep learning,\nincluding function discovery, fast inference, and feature selection. The code\nis available at https://github.com/HarryPotterXTX/HOPE.git.",
    "authors": [
      [
        "Tingxiong Xiao"
      ],
      [
        "Weihang Zhang"
      ],
      [
        "Yuxiao Cheng"
      ],
      [
        "Jinli Suo"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08192v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08192v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T01:46:15Z",
    "updated": "2023-07-17T01:46:15Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08189v1",
    "title": "Mini-Giants: \"Small\" Language Models and Open Source Win-Win",
    "summary": "ChatGPT is phenomenal. However, it is prohibitively expensive to train and\nrefine such giant models. Fortunately, small language models are flourishing\nand becoming more and more competent. We call them \"mini-giants\". We argue that\nopen source community like Kaggle and mini-giants will win-win in many ways,\ntechnically, ethically and socially. In this article, we present a brief yet\nrich background, discuss how to attain small language models, present a\ncomparative study of small language models and a brief discussion of evaluation\nmethods, discuss the application scenarios where small language models are most\nneeded in the real world, and conclude with discussion and outlook.",
    "authors": [
      [
        "Zhengping Zhou"
      ],
      [
        "Lezhi Li"
      ],
      [
        "Xinxi Chen"
      ],
      [
        "Andy Li"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08189v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08189v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T01:35:56Z",
    "updated": "2023-07-17T01:35:56Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08187v1",
    "title": "An Empirical Investigation of Pre-trained Model Selection for\n  Out-of-Distribution Generalization and Calibration",
    "summary": "In the realm of out-of-distribution generalization tasks, finetuning has\nrisen as a key strategy. While the most focus has been on optimizing learning\nalgorithms, our research highlights the influence of pre-trained model\nselection in finetuning on out-of-distribution performance and inference\nuncertainty. Balancing model size constraints of a single GPU, we examined the\nimpact of varying pre-trained datasets and model parameters on performance\nmetrics like accuracy and expected calibration error. Our findings underscore\nthe significant influence of pre-trained model selection, showing marked\nperformance improvements over algorithm choice. Larger models outperformed\nothers, though the balance between memorization and true generalization merits\nfurther investigation. Ultimately, our research emphasizes the importance of\npre-trained model selection for enhancing out-of-distribution generalization.",
    "authors": [
      [
        "Hiroki Naganuma"
      ],
      [
        "Ryuichiro Hataya"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08187v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08187v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T01:27:10Z",
    "updated": "2023-07-17T01:27:10Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08177v1",
    "title": "In-IDE Generation-based Information Support with a Large Language Model",
    "summary": "Developers often face challenges in code understanding, which is crucial for\nbuilding and maintaining high-quality software systems. Code comments and\ndocumentation can provide some context for the code, but are often scarce or\nmissing. This challenge has become even more pressing with the rise of large\nlanguage model (LLM) based code generation tools. To understand unfamiliar\ncode, most software developers rely on general-purpose search engines to search\nthrough various programming information resources, which often requires\nmultiple iterations of query rewriting and information foraging. More recently,\ndevelopers have turned to online chatbots powered by LLMs, such as ChatGPT,\nwhich can provide more customized responses but also incur more overhead as\ndevelopers need to communicate a significant amount of context to the LLM via a\ntextual interface. In this study, we provide the investigation of an LLM-based\nconversational UI in the IDE. We aim to understand the promises and obstacles\nfor tools powered by LLMs that are contextually aware, in that they\nautomatically leverage the developer's programming context to answer queries.\nTo this end, we develop an IDE Plugin that allows users to query back-ends such\nas OpenAI's GPT-3.5 and GPT-4 with high-level requests, like: explaining a\nhighlighted section of code, explaining key domain-specific terms, or providing\nusage examples for an API. We conduct an exploratory user study with 32\nparticipants to understand the usefulness and effectiveness, as well as\nindividual preferences in the usage of, this LLM-powered information support\ntool. The study confirms that this approach can aid code understanding more\neffectively than web search, but the degree of the benefit differed by\nparticipants' experience levels.",
    "authors": [
      [
        "Daye Nam"
      ],
      [
        "Andrew Macvean"
      ],
      [
        "Vincent Hellendoorn"
      ],
      [
        "Bogdan Vasilescu"
      ],
      [
        "Brad Myers"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08177v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08177v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-17T00:49:06Z",
    "updated": "2023-07-17T00:49:06Z",
    "categories": [
      {
        "term": "cs.SE",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.HC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08171v1",
    "title": "Credit Assignment: Challenges and Opportunities in Developing Human-like\n  AI Agents",
    "summary": "Temporal credit assignment is crucial for learning and skill development in\nnatural and artificial intelligence. While computational methods like the TD\napproach in reinforcement learning have been proposed, it's unclear if they\naccurately represent how humans handle feedback delays. Cognitive models intend\nto represent the mental steps by which humans solve problems and perform a\nnumber of tasks, but limited research in cognitive science has addressed the\ncredit assignment problem in humans and cognitive models. Our research uses a\ncognitive model based on a theory of decisions from experience, Instance-Based\nLearning Theory (IBLT), to test different credit assignment mechanisms in a\ngoal-seeking navigation task with varying levels of decision complexity.\nInstance-Based Learning (IBL) models simulate the process of making sequential\nchoices with different credit assignment mechanisms, including a new IBL-TD\nmodel that combines the IBL decision mechanism with the TD approach. We found\nthat (1) An IBL model that gives equal credit assignment to all decisions is\nable to match human performance better than other models, including IBL-TD and\nQ-learning; (2) IBL-TD and Q-learning models underperform compared to humans\ninitially, but eventually, they outperform humans; (3) humans are influenced by\ndecision complexity, while models are not. Our study provides insights into the\nchallenges of capturing human behavior and the potential opportunities to use\nthese models in future AI systems to support human activities.",
    "authors": [
      [
        "Thuy Ngoc Nguyen"
      ],
      [
        "Chase McDonald"
      ],
      [
        "Cleotilde Gonzalez"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08171v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08171v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T23:11:26Z",
    "updated": "2023-07-16T23:11:26Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.HC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08167v1",
    "title": "Computing the gradients with respect to all parameters of a quantum\n  neural network using a single circuit",
    "summary": "When computing the gradients of a quantum neural network using the\nparameter-shift rule, the cost function needs to be calculated twice for the\ngradient with respect to a single adjustable parameter of the network. When the\ntotal number of parameters is high, the quantum circuit for the computation has\nto be adjusted and run for many times. Here we propose an approach to compute\nall the gradients using a single circuit only, with a much reduced circuit\ndepth and less classical registers. We also demonstrate experimentally, on both\nreal quantum hardware and simulator, that our approach has the advantages that\nthe circuit takes a significantly shorter time to compile than the conventional\napproach, resulting in a speedup on the total runtime.",
    "authors": [
      [
        "Guang Ping He"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08167v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08167v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T22:35:52Z",
    "updated": "2023-07-16T22:35:52Z",
    "categories": [
      {
        "term": "quant-ph",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08161v1",
    "title": "Assessing the Quality of Multiple-Choice Questions Using GPT-4 and\n  Rule-Based Methods",
    "summary": "Multiple-choice questions with item-writing flaws can negatively impact\nstudent learning and skew analytics. These flaws are often present in\nstudent-generated questions, making it difficult to assess their quality and\nsuitability for classroom usage. Existing methods for evaluating\nmultiple-choice questions often focus on machine readability metrics, without\nconsidering their intended use within course materials and their pedagogical\nimplications. In this study, we compared the performance of a rule-based method\nwe developed to a machine-learning based method utilizing GPT-4 for the task of\nautomatically assessing multiple-choice questions based on 19 common\nitem-writing flaws. By analyzing 200 student-generated questions from four\ndifferent subject areas, we found that the rule-based method correctly detected\n91% of the flaws identified by human annotators, as compared to 79% by GPT-4.\nWe demonstrated the effectiveness of the two methods in identifying common\nitem-writing flaws present in the student-generated questions across different\nsubject areas. The rule-based method can accurately and efficiently evaluate\nmultiple-choice questions from multiple domains, outperforming GPT-4 and going\nbeyond existing metrics that do not account for the educational use of such\nquestions. Finally, we discuss the potential for using these automated methods\nto improve the quality of questions based on the identified flaws.",
    "authors": [
      [
        "Steven Moore"
      ],
      [
        "Huy A. Nguyen"
      ],
      [
        "Tianying Chen"
      ],
      [
        "John Stamper"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08161v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08161v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T22:12:10Z",
    "updated": "2023-07-16T22:12:10Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.HC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08141v1",
    "title": "POA: Passable Obstacles Aware Path-planning Algorithm for Navigation of\n  a Two-wheeled Robot in Highly Cluttered Environments",
    "summary": "This paper focuses on Passable Obstacles Aware (POA) planner - a novel\nnavigation method for two-wheeled robots in a highly cluttered environment. The\nnavigation algorithm detects and classifies objects to distinguish two types of\nobstacles - passable and unpassable. Our algorithm allows two-wheeled robots to\nfind a path through passable obstacles. Such a solution helps the robot working\nin areas inaccessible to standard path planners and find optimal trajectories\nin scenarios with a high number of objects in the robot's vicinity. The POA\nplanner can be embedded into other planning algorithms and enables them to\nbuild a path through obstacles. Our method decreases path length and the total\ntravel time to the final destination up to 43% and 39%, respectively, comparing\nto standard path planners such as GVD, A*, and RRT*",
    "authors": [
      [
        "Alexander Petrovsky"
      ],
      [
        "Yomna Youssef"
      ],
      [
        "Kirill Myasoedov"
      ],
      [
        "Artem Timoshenko"
      ],
      [
        "Vladimir Guneavoi"
      ],
      [
        "Ivan Kalinov"
      ],
      [
        "Dzmitry Tsetserukou"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08141v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08141v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T19:44:27Z",
    "updated": "2023-07-16T19:44:27Z",
    "categories": [
      {
        "term": "cs.RO",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08132v1",
    "title": "Heterogeneous graphs model spatial relationships between biological\n  entities for breast cancer diagnosis",
    "summary": "The heterogeneity of breast cancer presents considerable challenges for its\nearly detection, prognosis, and treatment selection. Convolutional neural\nnetworks often neglect the spatial relationships within histopathological\nimages, which can limit their accuracy. Graph neural networks (GNNs) offer a\npromising solution by coding the spatial relationships within images. Prior\nstudies have investigated the modeling of histopathological images as cell and\ntissue graphs, but they have not fully tapped into the potential of extracting\ninterrelationships between these biological entities. In this paper, we present\na novel approach using a heterogeneous GNN that captures the spatial and\nhierarchical relations between cell and tissue graphs to enhance the extraction\nof useful information from histopathological images. We also compare the\nperformance of a cross-attention-based network and a transformer architecture\nfor modeling the intricate relationships within tissue and cell graphs. Our\nmodel demonstrates superior efficiency in terms of parameter count and achieves\nhigher accuracy compared to the transformer-based state-of-the-art approach on\nthree publicly available breast cancer datasets -- BRIGHT, BreakHis, and BACH.",
    "authors": [
      [
        "Akhila Krishna K"
      ],
      [
        "Ravi Kant Gupta"
      ],
      [
        "Nikhil Cherian Kurian"
      ],
      [
        "Pranav Jeevan"
      ],
      [
        "Amit Sethi"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08132v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08132v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T19:06:29Z",
    "updated": "2023-07-16T19:06:29Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08131v1",
    "title": "INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks",
    "summary": "Leveraging network information for predictive modeling has become widespread\nin many domains. Within the realm of referral and targeted marketing,\ninfluencer detection stands out as an area that could greatly benefit from the\nincorporation of dynamic network representation due to the ongoing development\nof customer-brand relationships. To elaborate this idea, we introduce\nINFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph\nNeural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural\nNetworks (RNN) with weighted loss functions, the Synthetic Minority\nOversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted\nrolling-window strategy. To evaluate predictive performance, we utilize a\nunique corporate data set with networks of three cities and derive a\nprofit-driven evaluation methodology for influencer prediction. Our results\nshow how using RNN to encode temporal attributes alongside GNNs significantly\nimproves predictive performance. We compare the results of various models to\ndemonstrate the importance of capturing graph representation, temporal\ndependencies, and using a profit-driven methodology for evaluation.",
    "authors": [
      [
        "Elena Tiukhova"
      ],
      [
        "Emiliano Penaloza"
      ],
      [
        "María Óskarsdóttir"
      ],
      [
        "Bart Baesens"
      ],
      [
        "Monique Snoeck"
      ],
      [
        "Cristián Bravo"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08131v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08131v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T19:04:48Z",
    "updated": "2023-07-16T19:04:48Z",
    "categories": [
      {
        "term": "cs.SI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08087v1",
    "title": "A Recursive Bateson-Inspired Model for the Generation of Semantic Formal\n  Concepts from Spatial Sensory Data",
    "summary": "Neural-symbolic approaches to machine learning incorporate the advantages\nfrom both connectionist and symbolic methods. Typically, these models employ a\nfirst module based on a neural architecture to extract features from complex\ndata. Then, these features are processed as symbols by a symbolic engine that\nprovides reasoning, concept structures, composability, better generalization\nand out-of-distribution learning among other possibilities. However, neural\napproaches to the grounding of symbols in sensory data, albeit powerful, still\nrequire heavy training and tedious labeling for the most part. This paper\npresents a new symbolic-only method for the generation of hierarchical concept\nstructures from complex spatial sensory data. The approach is based on\nBateson's notion of difference as the key to the genesis of an idea or a\nconcept. Following his suggestion, the model extracts atomic features from raw\ndata by computing elemental sequential comparisons in a stream of multivariate\nnumerical values. Higher-level constructs are built from these features by\nsubjecting them to further comparisons in a recursive process. At any stage in\nthe recursion, a concept structure may be obtained from these constructs and\nfeatures by means of Formal Concept Analysis. Results show that the model is\nable to produce fairly rich yet human-readable conceptual representations\nwithout training. Additionally, the concept structures obtained through the\nmodel (i) present high composability, which potentially enables the generation\nof 'unseen' concepts, (ii) allow formal reasoning, and (iii) have inherent\nabilities for generalization and out-of-distribution learning. Consequently,\nthis method may offer an interesting angle to current neural-symbolic research.\nFuture work is required to develop a training methodology so that the model can\nbe tested against a larger dataset.",
    "authors": [
      [
        "Jaime de Miguel Rodriguez"
      ],
      [
        "Fernando Sancho Caparrini"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08087v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08087v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T15:59:13Z",
    "updated": "2023-07-16T15:59:13Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08086v1",
    "title": "Dataset Distillation Meets Provable Subset Selection",
    "summary": "Deep learning has grown tremendously over recent years, yielding\nstate-of-the-art results in various fields. However, training such models\nrequires huge amounts of data, increasing the computational time and cost. To\naddress this, dataset distillation was proposed to compress a large training\ndataset into a smaller synthetic one that retains its performance -- this is\nusually done by (1) uniformly initializing a synthetic set and (2) iteratively\nupdating/learning this set according to a predefined loss by uniformly sampling\ninstances from the full data. In this paper, we improve both phases of dataset\ndistillation: (1) we present a provable, sampling-based approach for\ninitializing the distilled set by identifying important and removing redundant\npoints in the data, and (2) we further merge the idea of data subset selection\nwith dataset distillation, by training the distilled set on ``important''\nsampled points during the training procedure instead of randomly sampling the\nnext batch. To do so, we define the notion of importance based on the relative\ncontribution of instances with respect to two different loss functions, i.e.,\none for the initialization phase (a kernel fitting function for kernel ridge\nregression and $K$-means based loss function for any other distillation\nmethod), and the relative cross-entropy loss (or any other predefined loss)\nfunction for the training phase. Finally, we provide experimental results\nshowing how our method can latch on to existing dataset distillation techniques\nand improve their performance.",
    "authors": [
      [
        "Murad Tukan"
      ],
      [
        "Alaa Maalouf"
      ],
      [
        "Margarita Osadchy"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08086v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08086v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T15:58:19Z",
    "updated": "2023-07-16T15:58:19Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08082v1",
    "title": "POMDP inference and robust solution via deep reinforcement learning: An\n  application to railway optimal maintenance",
    "summary": "Partially Observable Markov Decision Processes (POMDPs) can model complex\nsequential decision-making problems under stochastic and uncertain\nenvironments. A main reason hindering their broad adoption in real-world\napplications is the lack of availability of a suitable POMDP model or a\nsimulator thereof. Available solution algorithms, such as Reinforcement\nLearning (RL), require the knowledge of the transition dynamics and the\nobservation generating process, which are often unknown and non-trivial to\ninfer. In this work, we propose a combined framework for inference and robust\nsolution of POMDPs via deep RL. First, all transition and observation model\nparameters are jointly inferred via Markov Chain Monte Carlo sampling of a\nhidden Markov model, which is conditioned on actions, in order to recover full\nposterior distributions from the available data. The POMDP with uncertain\nparameters is then solved via deep RL techniques with the parameter\ndistributions incorporated into the solution via domain randomization, in order\nto develop solutions that are robust to model uncertainty. As a further\ncontribution, we compare the use of transformers and long short-term memory\nnetworks, which constitute model-free RL solutions, with a\nmodel-based/model-free hybrid approach. We apply these methods to the\nreal-world problem of optimal maintenance planning for railway assets.",
    "authors": [
      [
        "Giacomo Arcieri"
      ],
      [
        "Cyprien Hoelzl"
      ],
      [
        "Oliver Schwery"
      ],
      [
        "Daniel Straub"
      ],
      [
        "Konstantinos G. Papakonstantinou"
      ],
      [
        "Eleni Chatzi"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08082v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08082v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T15:44:58Z",
    "updated": "2023-07-16T15:44:58Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08074v1",
    "title": "Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language\n  Modelling",
    "summary": "Modeling discourse -- the linguistic phenomena that go beyond individual\nsentences, is a fundamental yet challenging aspect of natural language\nprocessing (NLP). However, existing evaluation benchmarks primarily focus on\nthe evaluation of inter-sentence properties and overlook critical discourse\nphenomena that cross sentences. To bridge the gap, we propose Disco-Bench, a\nbenchmark that can evaluate intra-sentence discourse properties across a\ndiverse set of NLP tasks, covering understanding, translation, and generation.\nDisco-Bench consists of 9 document-level testsets in the literature domain,\nwhich contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese\nand/or English. For linguistic analysis, we also design a diagnostic test suite\nthat can examine whether the target models learn discourse knowledge. We\ntotally evaluate 20 general-, in-domain and commercial models based on\nTransformer, advanced pretraining architectures and large language models\n(LLMs). Our results show (1) the challenge and necessity of our evaluation\nbenchmark; (2) fine-grained pretraining based on literary document-level\ntraining data consistently improves the modeling of discourse information. We\nwill release the datasets, pretrained models, and leaderboard, which we hope\ncan significantly facilitate research in this field:\nhttps://github.com/longyuewangdcu/Disco-Bench.",
    "authors": [
      [
        "Longyue Wang"
      ],
      [
        "Zefeng Du"
      ],
      [
        "Donghuai Liu"
      ],
      [
        "Cai Deng"
      ],
      [
        "Dian Yu"
      ],
      [
        "Haiyun Jiang"
      ],
      [
        "Yan Wang"
      ],
      [
        "Leyang Cui"
      ],
      [
        "Shuming Shi"
      ],
      [
        "Zhaopeng Tu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08074v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08074v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T15:18:25Z",
    "updated": "2023-07-16T15:18:25Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08072v1",
    "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An\n  Empirical Study",
    "summary": "Despite the superior performance, Large Language Models~(LLMs) require\nsignificant computational resources for deployment and use. To overcome this\nissue, quantization methods have been widely applied to reduce the memory\nfootprint of LLMs as well as increasing the inference rate. However, a major\nchallenge is that low-bit quantization methods often lead to performance\ndegradation. It is important to understand how quantization impacts the\ncapacity of LLMs. Different from previous studies focused on overall\nperformance, this work aims to investigate the impact of quantization on\n\\emph{emergent abilities}, which are important characteristics that distinguish\nLLMs from small language models. Specially, we examine the abilities of\nin-context learning, chain-of-thought reasoning, and instruction-following in\nquantized LLMs. Our empirical experiments show that these emergent abilities\nstill exist in 4-bit quantization models, while 2-bit models encounter severe\nperformance degradation on the test of these abilities. To improve the\nperformance of low-bit models, we conduct two special experiments: (1)\nfine-gained impact analysis that studies which components (or substructures)\nare more sensitive to quantization, and (2) performance compensation through\nmodel fine-tuning. Our work derives a series of important findings to\nunderstand the impact of quantization on emergent abilities, and sheds lights\non the possibilities of extremely low-bit quantization for LLMs.",
    "authors": [
      [
        "Peiyu Liu"
      ],
      [
        "Zikang Liu"
      ],
      [
        "Ze-Feng Gao"
      ],
      [
        "Dawei Gao"
      ],
      [
        "Wayne Xin Zhao"
      ],
      [
        "Yaliang Li"
      ],
      [
        "Bolin Ding"
      ],
      [
        "Ji-Rong Wen"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08072v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08072v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T15:11:01Z",
    "updated": "2023-07-16T15:11:01Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08044v1",
    "title": "Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via\n  Rank Regression",
    "summary": "Time-to-event analysis, also known as survival analysis, aims to predict the\ntime of occurrence of an event, given a set of features. One of the major\nchallenges in this area is dealing with censored data, which can make learning\nalgorithms more complex. Traditional methods such as Cox's proportional hazards\nmodel and the accelerated failure time (AFT) model have been popular in this\nfield, but they often require assumptions such as proportional hazards and\nlinearity. In particular, the AFT models often require pre-specified parametric\ndistributional assumptions. To improve predictive performance and alleviate\nstrict assumptions, there have been many deep learning approaches for\nhazard-based models in recent years. However, representation learning for AFT\nhas not been widely explored in the neural network literature, despite its\nsimplicity and interpretability in comparison to hazard-focused methods. In\nthis work, we introduce the Deep AFT Rank-regression model for Time-to-event\nprediction (DART). This model uses an objective function based on Gehan's rank\nstatistic, which is efficient and reliable for representation learning. On top\nof eliminating the requirement to establish a baseline event time distribution,\nDART retains the advantages of directly predicting event time in standard AFT\nmodels. The proposed method is a semiparametric approach to AFT modeling that\ndoes not impose any distributional assumptions on the survival time\ndistribution. This also eliminates the need for additional hyperparameters or\ncomplex model architectures, unlike existing neural network-based AFT models.\nThrough quantitative analysis on various benchmark datasets, we have shown that\nDART has significant potential for modeling high-throughput censored\ntime-to-event data.",
    "authors": [
      [
        "Hyunjun Lee"
      ],
      [
        "Junhyun Lee"
      ],
      [
        "Taehwa Choi"
      ],
      [
        "Jaewoo Kang"
      ],
      [
        "Sangbum Choi"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08044v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08044v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T13:58:28Z",
    "updated": "2023-07-16T13:58:28Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "stat.ML",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08036v1",
    "title": "A Neural-Symbolic Approach Towards Identifying Grammatically Correct\n  Sentences",
    "summary": "Textual content around us is growing on a daily basis. Numerous articles are\nbeing written as we speak on online newspapers, blogs, or social media.\nSimilarly, recent advances in the AI field, like language models or traditional\nclassic AI approaches, are utilizing all the above to improve their learned\nrepresentation to tackle NLP challenges with human-like accuracy. It is\ncommonly accepted that it is crucial to have access to well-written text from\nvalid sources to tackle challenges like text summarization, question-answering,\nmachine translation, or even pronoun resolution. For instance, to summarize\nwell, one needs to select the most important sentences in order to concatenate\nthem to form the summary. However, what happens if we do not have access to\nwell-formed English sentences or even non-valid sentences? Despite the\nimportance of having access to well-written sentences, figuring out ways to\nvalidate them is still an open area of research. To address this problem, we\npresent a simplified way to validate English sentences through a novel\nneural-symbolic approach. Lately, neural-symbolic approaches have triggered an\nincreasing interest towards tackling various NLP challenges, as they are\ndemonstrating their effectiveness as a central component in various AI systems.\nThrough combining Classic with Modern AI, which involves the blending of\ngrammatical and syntactical rules with language models, we effectively tackle\nthe Corpus of Linguistic Acceptability (COLA), a task that shows whether or not\na sequence of words is an English grammatical sentence. Among others,\nundertaken experiments effectively show that blending symbolic and non-symbolic\nsystems helps the former provide insights about the latter's accuracy results.",
    "authors": [
      [
        "Nicos Isaak"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08036v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08036v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T13:21:44Z",
    "updated": "2023-07-16T13:21:44Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "I.2.0; I.2.3; I.2.7; I.5.1",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08024v1",
    "title": "Bayesian inference for data-efficient, explainable, and safe robotic\n  motion planning: A review",
    "summary": "Bayesian inference has many advantages in robotic motion planning over four\nperspectives: The uncertainty quantification of the policy, safety (risk-aware)\nand optimum guarantees of robot motions, data-efficiency in training of\nreinforcement learning, and reducing the sim2real gap when the robot is applied\nto real-world tasks. However, the application of Bayesian inference in robotic\nmotion planning is lagging behind the comprehensive theory of Bayesian\ninference. Further, there are no comprehensive reviews to summarize the\nprogress of Bayesian inference to give researchers a systematic understanding\nin robotic motion planning. This paper first provides the probabilistic\ntheories of Bayesian inference which are the preliminary of Bayesian inference\nfor complex cases. Second, the Bayesian estimation is given to estimate the\nposterior of policies or unknown functions which are used to compute the\npolicy. Third, the classical model-based Bayesian RL and model-free Bayesian RL\nalgorithms for robotic motion planning are summarized, while these algorithms\nin complex cases are also analyzed. Fourth, the analysis of Bayesian inference\nin inverse RL is given to infer the reward functions in a data-efficient\nmanner. Fifth, we systematically present the hybridization of Bayesian\ninference and RL which is a promising direction to improve the convergence of\nRL for better motion planning. Sixth, given the Bayesian inference, we present\nthe interpretable and safe robotic motion plannings which are the hot research\ntopic recently. Finally, all algorithms reviewed in this paper are summarized\nanalytically as the knowledge graphs, and the future of Bayesian inference for\nrobotic motion planning is also discussed, to pave the way for data-efficient,\nexplainable, and safe robotic motion planning strategies for practical\napplications.",
    "authors": [
      [
        "Chengmin Zhou"
      ],
      [
        "Chao Wang"
      ],
      [
        "Haseeb Hassan"
      ],
      [
        "Himat Shah"
      ],
      [
        "Bingding Huang"
      ],
      [
        "Pasi Fränti"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08024v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08024v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T12:29:27Z",
    "updated": "2023-07-16T12:29:27Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08016v1",
    "title": "Breaking Down the Task: A Unit-Grained Hybrid Training Framework for\n  Vision and Language Decision Making",
    "summary": "Vision language decision making (VLDM) is a challenging multimodal task. The\nagent have to understand complex human instructions and complete compositional\ntasks involving environment navigation and object manipulation. However, the\nlong action sequences involved in VLDM make the task difficult to learn. From\nan environment perspective, we find that task episodes can be divided into\nfine-grained \\textit{units}, each containing a navigation phase and an\ninteraction phase. Since the environment within a unit stays unchanged, we\npropose a novel hybrid-training framework that enables active exploration in\nthe environment and reduces the exposure bias. Such framework leverages the\nunit-grained configurations and is model-agnostic. Specifically, we design a\nUnit-Transformer (UT) with an intrinsic recurrent state that maintains a\nunit-scale cross-modal memory. Through extensive experiments on the TEACH\nbenchmark, we demonstrate that our proposed framework outperforms existing\nstate-of-the-art methods in terms of all evaluation metrics. Overall, our work\nintroduces a novel approach to tackling the VLDM task by breaking it down into\nsmaller, manageable units and utilizing a hybrid-training framework. By doing\nso, we provide a more flexible and effective solution for multimodal decision\nmaking.",
    "authors": [
      [
        "Ruipu Luo"
      ],
      [
        "Jiwen Zhang"
      ],
      [
        "Zhongyu Wei"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08016v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08016v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T11:54:16Z",
    "updated": "2023-07-16T11:54:16Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08003v1",
    "title": "SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical\n  Significance Utilizing Local interpretability methods",
    "summary": "The interpretability of deep neural networks has become a subject of great\ninterest within the medical and healthcare domain. This attention stems from\nconcerns regarding transparency, legal and ethical considerations, and the\nmedical significance of predictions generated by these deep neural networks in\nclinical decision support systems. To address this matter, our study delves\ninto the application of four well-established interpretability methods: Local\nInterpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations\n(SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise\nRelevance Propagation (LRP). Leveraging the approach of transfer learning with\na multi-label-multi-class chest radiography dataset, we aim to interpret\npredictions pertaining to specific pathology classes. Our analysis encompasses\nboth single-label and multi-label predictions, providing a comprehensive and\nunbiased assessment through quantitative and qualitative investigations, which\nare compared against human expert annotation. Notably, Grad-CAM demonstrates\nthe most favorable performance in quantitative evaluation, while the LIME\nheatmap segmentation visualization exhibits the highest level of medical\nsignificance. Our research highlights the strengths and limitations of these\ninterpretability methods and suggests that a multimodal-based approach,\nincorporating diverse sources of information beyond chest radiography images,\ncould offer additional insights for enhancing interpretability in the medical\ndomain.",
    "authors": [
      [
        "Mahbub Ul Alam"
      ],
      [
        "Jaakko Hollmén"
      ],
      [
        "Jón Rúnar Baldvinsson"
      ],
      [
        "Rahim Rahmani"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.08003v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.08003v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T11:10:35Z",
    "updated": "2023-07-16T11:10:35Z",
    "categories": [
      {
        "term": "eess.IV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07997v1",
    "title": "MargCTGAN: A \"Marginally'' Better CTGAN for the Low Sample Regime",
    "summary": "The potential of realistic and useful synthetic data is significant. However,\ncurrent evaluation methods for synthetic tabular data generation predominantly\nfocus on downstream task usefulness, often neglecting the importance of\nstatistical properties. This oversight becomes particularly prominent in low\nsample scenarios, accompanied by a swift deterioration of these statistical\nmeasures. In this paper, we address this issue by conducting an evaluation of\nthree state-of-the-art synthetic tabular data generators based on their\nmarginal distribution, column-pair correlation, joint distribution and\ndownstream task utility performance across high to low sample regimes. The\npopular CTGAN model shows strong utility, but underperforms in low sample\nsettings in terms of utility. To overcome this limitation, we propose MargCTGAN\nthat adds feature matching of de-correlated marginals, which results in a\nconsistent improvement in downstream utility as well as statistical properties\nof the synthetic data.",
    "authors": [
      [
        "Tejumade Afonja"
      ],
      [
        "Dingfan Chen"
      ],
      [
        "Mario Fritz"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07997v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07997v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T10:28:49Z",
    "updated": "2023-07-16T10:28:49Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07956v1",
    "title": "Automated Polynomial Filter Learning for Graph Neural Networks",
    "summary": "Polynomial graph filters have been widely used as guiding principles in the\ndesign of Graph Neural Networks (GNNs). Recently, the adaptive learning of the\npolynomial graph filters has demonstrated promising performance for modeling\ngraph signals on both homophilic and heterophilic graphs, owning to their\nflexibility and expressiveness. In this work, we conduct a novel preliminary\nstudy to explore the potential and limitations of polynomial graph filter\nlearning approaches, revealing a severe overfitting issue. To improve the\neffectiveness of polynomial graph filters, we propose Auto-Polynomial, a novel\nand general automated polynomial graph filter learning framework that\nefficiently learns better filters capable of adapting to various complex graph\nsignals. Comprehensive experiments and ablation studies demonstrate significant\nand consistent performance improvements on both homophilic and heterophilic\ngraphs across multiple learning settings considering various labeling ratios,\nwhich unleashes the potential of polynomial filter learning.",
    "authors": [
      [
        "Wendi Yu"
      ],
      [
        "Zhichao Hou"
      ],
      [
        "Xiaorui Liu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07956v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07956v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T06:14:12Z",
    "updated": "2023-07-16T06:14:12Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07951v1",
    "title": "MinT: Boosting Generalization in Mathematical Reasoning via Multi-View\n  Fine-Tuning",
    "summary": "Reasoning in mathematical domains remains a significant challenge for\nrelatively small language models (LMs). Many current methods focus on\nspecializing LMs in mathematical reasoning and rely heavily on knowledge\ndistillation from powerful but inefficient large LMs (LLMs). In this work, we\nexplore a new direction that avoids over-reliance on LLM teachers, introducing\na multi-view fine-tuning method that efficiently exploits existing mathematical\nproblem datasets with diverse annotation styles. Our approach uniquely\nconsiders the various annotation formats as different \"views\" and leverages\nthem in training the model. By postpending distinct instructions to input\nquestions, models can learn to generate solutions in diverse formats in a\nflexible manner. Experimental results show that our strategy enables a LLaMA-7B\nmodel to outperform prior approaches that utilize knowledge distillation, as\nwell as carefully established baselines. Additionally, the proposed method\ngrants the models promising generalization ability across various views and\ndatasets, and the capability to learn from inaccurate or incomplete noisy data.\nWe hope our multi-view training paradigm could inspire future studies in other\nmachine reasoning domains.",
    "authors": [
      [
        "Zhenwen Liang"
      ],
      [
        "Dian Yu"
      ],
      [
        "Xiaoman Pan"
      ],
      [
        "Wenlin Yao"
      ],
      [
        "Qingkai Zeng"
      ],
      [
        "Xiangliang Zhang"
      ],
      [
        "Dong Yu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07951v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07951v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T05:41:53Z",
    "updated": "2023-07-16T05:41:53Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07944v1",
    "title": "Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and\n  Class-balanced Pseudo-Labeling",
    "summary": "Unsupervised domain adaptation (DA) with the aid of pseudo labeling\ntechniques has emerged as a crucial approach for domain-adaptive 3D object\ndetection. While effective, existing DA methods suffer from a substantial drop\nin performance when applied to a multi-class training setting, due to the\nco-existence of low-quality pseudo labels and class imbalance issues. In this\npaper, we address this challenge by proposing a novel ReDB framework tailored\nfor learning to detect all classes at once. Our approach produces Reliable,\nDiverse, and class-Balanced pseudo 3D boxes to iteratively guide the\nself-training on a distributionally different target domain. To alleviate\ndisruptions caused by the environmental discrepancy (e.g., beam numbers), the\nproposed cross-domain examination (CDE) assesses the correctness of pseudo\nlabels by copy-pasting target instances into a source environment and measuring\nthe prediction consistency. To reduce computational overhead and mitigate the\nobject shift (e.g., scales and point densities), we design an overlapped boxes\ncounting (OBC) metric that allows to uniformly downsample pseudo-labeled\nobjects across different geometric characteristics. To confront the issue of\ninter-class imbalance, we progressively augment the target point clouds with a\nclass-balanced set of pseudo-labeled target instances and source objects, which\nboosts recognition accuracies on both frequently appearing and rare classes.\nExperimental results on three benchmark datasets using both voxel-based (i.e.,\nSECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our\nproposed ReDB approach outperforms existing 3D domain adaptation methods by a\nlarge margin, improving 23.15% mAP on the nuScenes $\\rightarrow$ KITTI task.",
    "authors": [
      [
        "Zhuoxiao Chen"
      ],
      [
        "Yadan Luo"
      ],
      [
        "Zi Huang"
      ],
      [
        "Zheng Wang"
      ],
      [
        "Mahsa Baktashmotlagh"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07944v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07944v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T04:34:11Z",
    "updated": "2023-07-16T04:34:11Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07942v1",
    "title": "KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection",
    "summary": "Achieving a reliable LiDAR-based object detector in autonomous driving is\nparamount, but its success hinges on obtaining large amounts of precise 3D\nannotations. Active learning (AL) seeks to mitigate the annotation burden\nthrough algorithms that use fewer labels and can attain performance comparable\nto fully supervised learning. Although AL has shown promise, current approaches\nprioritize the selection of unlabeled point clouds with high uncertainty and/or\ndiversity, leading to the selection of more instances for labeling and reduced\ncomputational efficiency. In this paper, we resort to a novel kernel coding\nrate maximization (KECOR) strategy which aims to identify the most informative\npoint clouds to acquire labels through the lens of information theory. Greedy\nsearch is applied to seek desired point clouds that can maximize the minimal\nnumber of bits required to encode the latent features. To determine the\nuniqueness and informativeness of the selected samples from the model\nperspective, we construct a proxy network of the 3D detector head and compute\nthe outer product of Jacobians from all proxy layers to form the empirical\nneural tangent kernel (NTK) matrix. To accommodate both one-stage (i.e.,\nSECOND) and two-stage detectors (i.e., PVRCNN), we further incorporate the\nclassification entropy maximization and well trade-off between detection\nperformance and the total number of bounding boxes selected for annotation.\nExtensive experiments conducted on two 3D benchmarks and a 2D detection dataset\nevidence the superiority and versatility of the proposed approach. Our results\nshow that approximately 44% box-level annotation costs and 26% computational\ntime are reduced compared to the state-of-the-art AL method, without\ncompromising detection performance.",
    "authors": [
      [
        "Yadan Luo"
      ],
      [
        "Zhuoxiao Chen"
      ],
      [
        "Zhen Fang"
      ],
      [
        "Zheng Zhang"
      ],
      [
        "Zi Huang"
      ],
      [
        "Mahsa Baktashmotlagh"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07942v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07942v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T04:27:03Z",
    "updated": "2023-07-16T04:27:03Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07930v1",
    "title": "GeoGPT: Understanding and Processing Geospatial Tasks through An\n  Autonomous GPT",
    "summary": "Decision-makers in GIS need to combine a series of spatial algorithms and\noperations to solve geospatial tasks. For example, in the task of facility\nsiting, the Buffer tool is usually first used to locate areas close or away\nfrom some specific entities; then, the Intersect or Erase tool is used to\nselect candidate areas satisfied multiple requirements. Though professionals\ncan easily understand and solve these geospatial tasks by sequentially\nutilizing relevant tools, it is difficult for non-professionals to handle these\nproblems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents\nstrong performance in semantic understanding and reasoning. Especially, AutoGPT\ncan further extend the capabilities of large language models (LLMs) by\nautomatically reasoning and calling externally defined tools. Inspired by these\nstudies, we attempt to lower the threshold of non-professional users to solve\ngeospatial tasks by integrating the semantic understanding ability inherent in\nLLMs with mature tools within the GIS community. Specifically, we develop a new\nframework called GeoGPT that can conduct geospatial data collection,\nprocessing, and analysis in an autonomous manner with the instruction of only\nnatural language. In other words, GeoGPT is used to understand the demands of\nnon-professional users merely based on input natural language descriptions, and\nthen think, plan, and execute defined GIS tools to output final effective\nresults. Several cases including geospatial data crawling, spatial query,\nfacility siting, and mapping validate the effectiveness of our framework.\nThough limited cases are presented in this paper, GeoGPT can be further\nextended to various tasks by equipping with more GIS tools, and we think the\nparadigm of \"foundational plus professional\" implied in GeoGPT provides an\neffective way to develop next-generation GIS in this era of large foundation\nmodels.",
    "authors": [
      [
        "Yifan Zhang"
      ],
      [
        "Cheng Wei"
      ],
      [
        "Shangyou Wu"
      ],
      [
        "Zhengting He"
      ],
      [
        "Wenhao Yu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07930v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07930v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T03:03:59Z",
    "updated": "2023-07-16T03:03:59Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07919v1",
    "title": "Neural Architecture Retrieval",
    "summary": "With the increasing number of new neural architecture designs and substantial\nexisting neural architectures, it becomes difficult for the researchers to\nsituate their contributions compared with existing neural architectures or\nestablish the connections between their designs and other relevant ones. To\ndiscover similar neural architectures in an efficient and automatic manner, we\ndefine a new problem Neural Architecture Retrieval which retrieves a set of\nexisting neural architectures which have similar designs to the query neural\narchitecture. Existing graph pre-training strategies cannot address the\ncomputational graph in neural architectures due to the graph size and motifs.\nTo fulfill this potential, we propose to divide the graph into motifs which are\nused to rebuild the macro graph to tackle these issues, and introduce\nmulti-level contrastive learning to achieve accurate graph representation\nlearning. Extensive evaluations on both human-designed and synthesized neural\narchitectures demonstrate the superiority of our algorithm. Such a dataset\nwhich contains 12k real-world network architectures, as well as their\nembedding, is built for neural architecture retrieval.",
    "authors": [
      [
        "Xiaohuan Pei"
      ],
      [
        "Yanxi Li"
      ],
      [
        "Minjing Dong"
      ],
      [
        "Chang Xu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07919v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07919v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T01:56:41Z",
    "updated": "2023-07-16T01:56:41Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07909v1",
    "title": "Is Imitation All You Need? Generalized Decision-Making with Dual-Phase\n  Training",
    "summary": "We introduce DualMind, a generalist agent designed to tackle various\ndecision-making tasks that addresses challenges posed by current methods, such\nas overfitting behaviors and dependence on task-specific fine-tuning. DualMind\nuses a novel \"Dual-phase\" training strategy that emulates how humans learn to\nact in the world. The model first learns fundamental common knowledge through a\nself-supervised objective tailored for control tasks and then learns how to\nmake decisions based on different contexts through imitating behaviors\nconditioned on given prompts. DualMind can handle tasks across domains, scenes,\nand embodiments using just a single set of model weights and can execute\nzero-shot prompting without requiring task-specific fine-tuning. We evaluate\nDualMind on MetaWorld and Habitat through extensive experiments and demonstrate\nits superior generalizability compared to previous techniques, outperforming\nother generalist agents by over 50$\\%$ and 70$\\%$ on Habitat and MetaWorld,\nrespectively. On the 45 tasks in MetaWorld, DualMind achieves over 30 tasks at\na 90$\\%$ success rate.",
    "authors": [
      [
        "Yao Wei"
      ],
      [
        "Yanchao Sun"
      ],
      [
        "Ruijie Zheng"
      ],
      [
        "Sai Vemprala"
      ],
      [
        "Rogerio Bonatti"
      ],
      [
        "Shuhang Chen"
      ],
      [
        "Ratnesh Madaan"
      ],
      [
        "Zhongjie Ba"
      ],
      [
        "Ashish Kapoor"
      ],
      [
        "Shuang Ma"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07909v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07909v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-16T00:34:12Z",
    "updated": "2023-07-16T00:34:12Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07544v1",
    "title": "A Dialogue System for Assessing Activities of Daily Living: Improving\n  Consistency with Grounded Knowledge",
    "summary": "In healthcare, the ability to care for oneself is reflected in the\n\"Activities of Daily Living (ADL),\" which serve as a measure of functional\nability (functioning). A lack of functioning may lead to poor living conditions\nrequiring personal care and assistance. To accurately identify those in need of\nsupport, assistance programs continuously evaluate participants' functioning\nacross various domains. However, the assessment process may encounter\nconsistency issues when multiple assessors with varying levels of expertise are\ninvolved. Novice assessors, in particular, may lack the necessary preparation\nfor real-world interactions with participants. To address this issue, we\ndeveloped a dialogue system that simulates interactions between assessors and\nindividuals of varying functioning in a natural and reproducible way. The\ndialogue system consists of two major modules, one for natural language\nunderstanding (NLU) and one for natural language generation (NLG),\nrespectively. In order to generate responses consistent with the underlying\nknowledge base, the dialogue system requires both an understanding of the\nuser's query and of biographical details of an individual being simulated. To\nfulfill this requirement, we experimented with query classification and\ngenerated responses based on those biographical details using some recently\nreleased InstructGPT-like models.",
    "authors": [
      [
        "Zhecheng Sheng"
      ],
      [
        "Raymond Finzel"
      ],
      [
        "Michael Lucke"
      ],
      [
        "Sheena Dufresne"
      ],
      [
        "Maria Gini"
      ],
      [
        "Serguei Pakhomov"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07544v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07544v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T22:41:59Z",
    "updated": "2023-07-15T22:41:59Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07893v1",
    "title": "Anomaly Detection in Automated Fibre Placement: Learning with Data\n  Limitations",
    "summary": "Current defect detection systems for Automated Fibre Placement (AFP) are\nmostly based on end-to-end supervised learning methods requiring abundant\nlabelled defective samples, which are not easily generated in sufficient\nnumbers. To address this data scarcity problem, we introduce an\nautoencoder-based approach compatible with small datasets. Fortunately, the\nproblem from a foundational point of view can be simplified as a binary\nclassification between normal and abnormal samples. The proposed approach uses\na depth map of the fibre layup surface, split into small windows aligned to\neach composite strip (tow). A subset of these windows that do not contain\nanomalies is passed to an autoencoder to reconstruct the input. Because the\nautoencoder is trained with normal samples, it produces more accurate\nreconstructions for these samples than for abnormal ones. Therefore, the value\nof reconstruction error is used as a quantitative metric for whether there are\npotential anomalies. These values are combined to produce an anomaly map, which\ncan localize the manufacturing defects in the depth map. The results show that\nalthough the autoencoder is trained with a very limited number of scans, the\nproposed approach can produce sufficient binary classification accuracy and\nspecify the location of the defects.",
    "authors": [
      [
        "Assef Ghamisi"
      ],
      [
        "Todd Charter"
      ],
      [
        "Li Ji"
      ],
      [
        "Maxime Rivard"
      ],
      [
        "Gil Lund"
      ],
      [
        "Homayoun Najjaran"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07893v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07893v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T22:13:36Z",
    "updated": "2023-07-15T22:13:36Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "eess.IV",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07887v1",
    "title": "Handwritten and Printed Text Segmentation: A Signature Case Study",
    "summary": "While analyzing scanned documents, handwritten text can overlay printed text.\nThis causes difficulties during the optical character recognition (OCR) and\ndigitization process of documents, and subsequently, hurts downstream NLP\ntasks. Prior research either focuses only on the binary classification of\nhandwritten text, or performs a three-class segmentation of the document, i.e.,\nrecognition of handwritten, printed, and background pixels. This results in the\nassignment of the handwritten and printed overlapping pixels to only one of the\nclasses, and thus, they are not accounted for in the other class. Thus, in this\nresearch, we develop novel approaches for addressing the challenges of\nhandwritten and printed text segmentation with the goal of recovering text in\ndifferent classes in whole, especially improving the segmentation performance\non the overlapping parts. As such, to facilitate with this task, we introduce a\nnew dataset, SignaTR6K, collected from real legal documents, as well as a new\nmodel architecture for handwritten and printed text segmentation task. Our best\nconfiguration outperforms the prior work on two different datasets by 17.9% and\n7.3% on IoU scores.",
    "authors": [
      [
        "Sina Gholamian"
      ],
      [
        "Ali Vahdat"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07887v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07887v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T21:49:22Z",
    "updated": "2023-07-15T21:49:22Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07876v1",
    "title": "Online Goal Recognition in Discrete and Continuous Domains Using a\n  Vectorial Representation",
    "summary": "While recent work on online goal recognition efficiently infers goals under\nlow observability, comparatively less work focuses on online goal recognition\nthat works in both discrete and continuous domains. Online goal recognition\napproaches often rely on repeated calls to the planner at each new observation,\nincurring high computational costs. Recognizing goals online in continuous\nspace quickly and reliably is critical for any trajectory planning problem\nsince the real physical world is fast-moving, e.g. robot applications. We\ndevelop an efficient method for goal recognition that relies either on a single\ncall to the planner for each possible goal in discrete domains or a simplified\nmotion model that reduces the computational burden in continuous ones. The\nresulting approach performs the online component of recognition orders of\nmagnitude faster than the current state of the art, making it the first online\nmethod effectively usable for robotics applications that require sub-second\nrecognition.",
    "authors": [
      [
        "Douglas Tesch"
      ],
      [
        "Leonardo Rosa Amado"
      ],
      [
        "Felipe Meneguzzi"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07876v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07876v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T19:27:38Z",
    "updated": "2023-07-15T19:27:38Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07872v1",
    "title": "Does Double Descent Occur in Self-Supervised Learning?",
    "summary": "Most investigations into double descent have focused on supervised models\nwhile the few works studying self-supervised settings find a surprising lack of\nthe phenomenon. These results imply that double descent may not exist in\nself-supervised models. We show this empirically using a standard and linear\nautoencoder, two previously unstudied settings. The test loss is found to have\neither a classical U-shape or to monotonically decrease instead of exhibiting a\ndouble-descent curve. We hope that further work on this will help elucidate the\ntheoretical underpinnings of this phenomenon.",
    "authors": [
      [
        "Alisia Lupidi"
      ],
      [
        "Yonatan Gideoni"
      ],
      [
        "Dulhan Jayalath"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07872v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07872v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T19:09:35Z",
    "updated": "2023-07-15T19:09:35Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07871v1",
    "title": "The SocialAI School: Insights from Developmental Psychology Towards\n  Artificial Socio-Cultural Agents",
    "summary": "Developmental psychologists have long-established the importance of\nsocio-cognitive abilities in human intelligence. These abilities enable us to\nenter, participate and benefit from human culture. AI research on social\ninteractive agents mostly concerns the emergence of culture in a multi-agent\nsetting (often without a strong grounding in developmental psychology). We\nargue that AI research should be informed by psychology and study\nsocio-cognitive abilities enabling to enter a culture too. We discuss the\ntheories of Michael Tomasello and Jerome Bruner to introduce some of their\nconcepts to AI and outline key concepts and socio-cognitive abilities. We\npresent The SocialAI school - a tool including a customizable parameterized\nuite of procedurally generated environments, which simplifies conducting\nexperiments regarding those concepts. We show examples of such experiments with\nRL agents and Large Language Models. The main motivation of this work is to\nengage the AI community around the problem of social intelligence informed by\ndevelopmental psychology, and to provide a tool to simplify first steps in this\ndirection. Refer to the project website for code and additional information:\nhttps://sites.google.com/view/socialai-school.",
    "authors": [
      [
        "Grgur Kovač"
      ],
      [
        "Rémy Portelas"
      ],
      [
        "Peter Ford Dominey"
      ],
      [
        "Pierre-Yves Oudeyer"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07871v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07871v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T19:05:56Z",
    "updated": "2023-07-15T19:05:56Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "68T07",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "I.2.0",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07870v1",
    "title": "Large Language Models as Superpositions of Cultural Perspectives",
    "summary": "Large Language Models (LLMs) are often misleadingly recognized as having a\npersonality or a set of values. We argue that an LLM can be seen as a\nsuperposition of perspectives with different values and personality traits.\nLLMs exhibit context-dependent values and personality traits that change based\non the induced perspective (as opposed to humans, who tend to have more\ncoherent values and personality traits across contexts). We introduce the\nconcept of perspective controllability, which refers to a model's affordance to\nadopt various perspectives with differing values and personality traits. In our\nexperiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study\nhow exhibited values and personality traits change based on different\nperspectives. Through qualitative experiments, we show that LLMs express\ndifferent values when those are (implicitly or explicitly) implied in the\nprompt, and that LLMs express different values even when those are not\nobviously implied (demonstrating their context-dependent nature). We then\nconduct quantitative experiments to study the controllability of different\nmodels (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the\neffectiveness of various methods for inducing perspectives, and the smoothness\nof the models' drivability. We conclude by examining the broader implications\nof our work and outline a variety of associated scientific questions. The\nproject website is available at\nhttps://sites.google.com/view/llm-superpositions .",
    "authors": [
      [
        "Grgur Kovač"
      ],
      [
        "Masataka Sawayama"
      ],
      [
        "Rémy Portelas"
      ],
      [
        "Cédric Colas"
      ],
      [
        "Peter Ford Dominey"
      ],
      [
        "Pierre-Yves Oudeyer"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07870v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07870v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T19:04:33Z",
    "updated": "2023-07-15T19:04:33Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "68T07",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "I.2.7",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07863v1",
    "title": "Benchmarking the Effectiveness of Classification Algorithms and SVM\n  Kernels for Dry Beans",
    "summary": "Plant breeders and agricultural researchers can increase crop productivity by\nidentifying desirable features, disease resistance, and nutritional content by\nanalysing the Dry Bean dataset. This study analyses and compares different\nSupport Vector Machine (SVM) classification algorithms, namely linear,\npolynomial, and radial basis function (RBF), along with other popular\nclassification algorithms. The analysis is performed on the Dry Bean Dataset,\nwith PCA (Principal Component Analysis) conducted as a preprocessing step for\ndimensionality reduction. The primary evaluation metric used is accuracy, and\nthe RBF SVM kernel algorithm achieves the highest Accuracy of 93.34%, Precision\nof 92.61%, Recall of 92.35% and F1 Score as 91.40%. Along with adept\nvisualization and empirical analysis, this study offers valuable guidance by\nemphasizing the importance of considering different SVM algorithms for complex\nand non-linear structured datasets.",
    "authors": [
      [
        "Anant Mehta"
      ],
      [
        "Prajit Sengupta"
      ],
      [
        "Divisha Garg"
      ],
      [
        "Harpreet Singh"
      ],
      [
        "Yosi Shacham Diamand"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07863v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07863v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T18:13:29Z",
    "updated": "2023-07-15T18:13:29Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07857v1",
    "title": "A Multi-Heuristic Search-based Motion Planning for Automated Parking",
    "summary": "In unstructured environments like parking lots or construction sites, due to\nthe large search-space and kinodynamic constraints of the vehicle, it is\nchallenging to achieve real-time planning. Several state-of-the-art planners\nutilize heuristic search-based algorithms. However, they heavily rely on the\nquality of the single heuristic function, used to guide the search. Therefore,\nthey are not capable to achieve reasonable computational performance, resulting\nin unnecessary delays in the response of the vehicle. In this work, we are\nadopting a Multi-Heuristic Search approach, that enables the use of multiple\nheuristic functions and their individual advantages to capture different\ncomplexities of a given search space. Based on our knowledge, this approach was\nnot used previously for this problem. For this purpose, multiple admissible and\nnon-admissible heuristic functions are defined, the original Multi-Heuristic A*\nSearch was extended for bidirectional use and dealing with hybrid\ncontinuous-discrete search space, and a mechanism for adapting scale of motion\nprimitives is introduced. To demonstrate the advantage, the Multi-Heuristic A*\nalgorithm is benchmarked against a very popular heuristic search-based\nalgorithm, Hybrid A*. The Multi-Heuristic A* algorithm outperformed baseline in\nboth terms, computation efficiency and motion plan (path) quality.",
    "authors": [
      [
        "Bhargav Adabala"
      ],
      [
        "Zlatan Ajanović"
      ]
    ],
    "links": [
      {
        "title": "doi",
        "href": "http://dx.doi.org/10.1109/ICAT57854.2023.10171306",
        "rel": "related"
      },
      {
        "href": "http://arxiv.org/abs/2307.07857v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07857v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T17:33:06Z",
    "updated": "2023-07-15T17:33:06Z",
    "categories": [
      {
        "term": "cs.RO",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07851v1",
    "title": "AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual\n  Similarity using Contrastive Learning and Structured Knowledge",
    "summary": "Generic sentence embeddings provide a coarse-grained approximation of\nsemantic textual similarity but ignore specific aspects that make texts\nsimilar. Conversely, aspect-based sentence embeddings provide similarities\nbetween texts based on certain predefined aspects. Thus, similarity predictions\nof texts are more targeted to specific requirements and more easily\nexplainable. In this paper, we present AspectCSE, an approach for aspect-based\ncontrastive learning of sentence embeddings. Results indicate that AspectCSE\nachieves an average improvement of 3.97% on information retrieval tasks across\nmultiple aspects compared to the previous best results. We also propose using\nWikidata knowledge graph properties to train models of multi-aspect sentence\nembeddings in which multiple specific aspects are simultaneously considered\nduring similarity predictions. We demonstrate that multi-aspect embeddings\noutperform single-aspect embeddings on aspect-specific information retrieval\ntasks. Finally, we examine the aspect-based sentence embedding space and\ndemonstrate that embeddings of semantically similar aspect labels are often\nclose, even without explicit similarity training between different aspect\nlabels.",
    "authors": [
      [
        "Tim Schopf"
      ],
      [
        "Emanuel Gerber"
      ],
      [
        "Malte Ostendorff"
      ],
      [
        "Florian Matthes"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07851v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07851v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T17:01:56Z",
    "updated": "2023-07-15T17:01:56Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "I.2.7",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07846v1",
    "title": "AIOptimizer -- A reinforcement learning-based software performance\n  optimisation prototype for cost minimisation",
    "summary": "This research article introduces AIOptimizer, a prototype for a software\nperformance optimisation tool based on cost reduction. AIOptimizer uses a\nrecommendation system driven by reinforcement learning to improve software\nsystem efficiency and affordability. The paper highlights AIOptimizer's design\nfactors, such as accuracy, adaptability, scalability, and user-friendliness. To\nprovide effective and user-centric performance optimisation solutions, it\nemphasises the use of a modular design, data gathering techniques, continuous\nlearning, and resilient integration. The article also investigates AIOptimizer\nfeatures such as fault identification, cost optimisation recommendations,\nefficiency prediction, and cooperation. Furthermore, it explores several\nsoftware development life cycle models and introduces AIOptimizer uses a\nreinforcement learning-based recommendation engine for cost optimisation. The\npurpose of this research study is to highlight AIOptimizer as a prototype that\nuses advanced optimisation techniques and smart recommendation systems to\ncontinually enhance software performance and save expenses. The research\nfocuses on various software development life cycle models, such as the\nWaterfall model, Iterative model, Spiral model, V-Model, Big Bang model and\nAgile Model. Each model has advantages and disadvantages, and their usefulness\nis determined by the project's specifications and characteristics. The\nAIOptimizer tool is a theoretical prototype for such software performance\noptimizers.",
    "authors": [
      [
        "Noopur Zambare"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07846v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07846v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T16:38:20Z",
    "updated": "2023-07-15T16:38:20Z",
    "categories": [
      {
        "term": "cs.SE",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07840v1",
    "title": "RegExplainer: Generating Explanations for Graph Neural Networks in\n  Regression Task",
    "summary": "Graph regression is a fundamental task and has received increasing attention\nin a wide range of graph learning tasks. However, the inference process is\noften not interpretable. Most existing explanation techniques are limited to\nunderstanding GNN behaviors in classification tasks. In this work, we seek an\nexplanation to interpret the graph regression models (XAIG-R). We show that\nexisting methods overlook the distribution shifting and continuously ordered\ndecision boundary, which hinders them away from being applied in the regression\ntasks. To address these challenges, we propose a novel objective based on the\ninformation bottleneck theory and introduce a new mix-up framework, which could\nsupport various GNNs in a model-agnostic manner. We further present a\ncontrastive learning strategy to tackle the continuously ordered labels in\nregression task. To empirically verify the effectiveness of the proposed\nmethod, we introduce three benchmark datasets and a real-life dataset for\nevaluation. Extensive experiments show the effectiveness of the proposed method\nin interpreting GNN models in regression tasks.",
    "authors": [
      [
        "Jiaxing Zhang"
      ],
      [
        "Zhuomin Chen"
      ],
      [
        "Hao Mei"
      ],
      [
        "Dongsheng Luo"
      ],
      [
        "Hua Wei"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07840v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07840v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T16:16:22Z",
    "updated": "2023-07-15T16:16:22Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07832v1",
    "title": "MixupExplainer: Generalizing Explanations for Graph Neural Networks with\n  Data Augmentation",
    "summary": "Graph Neural Networks (GNNs) have received increasing attention due to their\nability to learn from graph-structured data. However, their predictions are\noften not interpretable. Post-hoc instance-level explanation methods have been\nproposed to understand GNN predictions. These methods seek to discover\nsubstructures that explain the prediction behavior of a trained GNN. In this\npaper, we shed light on the existence of the distribution shifting issue in\nexisting methods, which affects explanation quality, particularly in\napplications on real-life datasets with tight decision boundaries. To address\nthis issue, we introduce a generalized Graph Information Bottleneck (GIB) form\nthat includes a label-independent graph variable, which is equivalent to the\nvanilla GIB. Driven by the generalized GIB, we propose a graph mixup method,\nMixupExplainer, with a theoretical guarantee to resolve the distribution\nshifting issue. We conduct extensive experiments on both synthetic and\nreal-world datasets to validate the effectiveness of our proposed mixup\napproach over existing approaches. We also provide a detailed analysis of how\nour proposed approach alleviates the distribution shifting issue.",
    "authors": [
      [
        "Jiaxing Zhang"
      ],
      [
        "Dongsheng Luo"
      ],
      [
        "Hua Wei"
      ]
    ],
    "links": [
      {
        "title": "doi",
        "href": "http://dx.doi.org/10.1145/3580305.3599435",
        "rel": "related"
      },
      {
        "href": "http://arxiv.org/abs/2307.07832v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07832v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T15:46:38Z",
    "updated": "2023-07-15T15:46:38Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07781v1",
    "title": "Improving Trace Link Recommendation by Using Non-Isotropic Distances and\n  Combinations",
    "summary": "The existence of trace links between artifacts of the software development\nlife cycle can improve the efficiency of many activities during software\ndevelopment, maintenance and operations. Unfortunately, the creation and\nmaintenance of trace links is time-consuming and error-prone. Research efforts\nhave been spent to automatically compute trace links and lately gained\nmomentum, e.g., due to the availability of powerful tools in the area of\nnatural language processing. In this paper, we report on some observations that\nwe made during studying non-linear similarity measures for computing trace\nlinks. We argue, that taking a geometric viewpoint on semantic similarity can\nbe helpful for future traceability research. We evaluated our observations on a\ndataset of four open source projects and two industrial projects. We\nfurthermore point out that our findings are more general and can build the\nbasis for other information retrieval problems as well.",
    "authors": [
      [
        "Christof Tinnes"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07781v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07781v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T11:35:02Z",
    "updated": "2023-07-15T11:35:02Z",
    "categories": [
      {
        "term": "cs.SE",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.IR",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "math.DG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07764v1",
    "title": "Explainable AI with counterfactual paths",
    "summary": "Explainable AI (XAI) is an increasingly important area of research in machine\nlearning, which in principle aims to make black-box models transparent and\ninterpretable. In this paper, we propose a novel approach to XAI that uses\ncounterfactual paths generated by conditional permutations. Our method provides\ncounterfactual explanations by identifying alternative paths that could have\nled to different outcomes. The proposed method is particularly suitable for\ngenerating explanations based on counterfactual paths in knowledge graphs. By\nexamining hypothetical changes to the input data in the knowledge graph, we can\nsystematically validate the behaviour of the model and examine the features or\ncombination of features that are most important to the model's predictions. Our\napproach provides a more intuitive and interpretable explanation for the\nmodel's behaviour than traditional feature weighting methods and can help\nidentify and mitigate biases in the model.",
    "authors": [
      [
        "Bastian Pfeifer"
      ],
      [
        "Mateusz Krzyzinski"
      ],
      [
        "Hubert Baniecki"
      ],
      [
        "Anna Saranti"
      ],
      [
        "Andreas Holzinger"
      ],
      [
        "Przemyslaw Biecek"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07764v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07764v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T10:16:51Z",
    "updated": "2023-07-15T10:16:51Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07754v1",
    "title": "Bidirectionally Deformable Motion Modulation For Video-based Human Pose\n  Transfer",
    "summary": "Video-based human pose transfer is a video-to-video generation task that\nanimates a plain source human image based on a series of target human poses.\nConsidering the difficulties in transferring highly structural patterns on the\ngarments and discontinuous poses, existing methods often generate\nunsatisfactory results such as distorted textures and flickering artifacts. To\naddress these issues, we propose a novel Deformable Motion Modulation (DMM)\nthat utilizes geometric kernel offset with adaptive weight modulation to\nsimultaneously perform feature alignment and style transfer. Different from\nnormal style modulation used in style transfer, the proposed modulation\nmechanism adaptively reconstructs smoothed frames from style codes according to\nthe object shape through an irregular receptive field of view. To enhance the\nspatio-temporal consistency, we leverage bidirectional propagation to extract\nthe hidden motion information from a warped image sequence generated by noisy\nposes. The proposed feature propagation significantly enhances the motion\nprediction ability by forward and backward propagation. Both quantitative and\nqualitative experimental results demonstrate superiority over the\nstate-of-the-arts in terms of image fidelity and visual continuity. The source\ncode is publicly available at github.com/rocketappslab/bdmm.",
    "authors": [
      [
        "Wing-Yin Yu"
      ],
      [
        "Lai-Man Po"
      ],
      [
        "Ray Cheung"
      ],
      [
        "Yuzhi Zhao"
      ],
      [
        "Yu Xue"
      ],
      [
        "Kun Li"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07754v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07754v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T09:24:45Z",
    "updated": "2023-07-15T09:24:45Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07753v1",
    "title": "Learning Expressive Priors for Generalization and Uncertainty Estimation\n  in Neural Networks",
    "summary": "In this work, we propose a novel prior learning method for advancing\ngeneralization and uncertainty estimation in deep neural networks. The key idea\nis to exploit scalable and structured posteriors of neural networks as\ninformative priors with generalization guarantees. Our learned priors provide\nexpressive probabilistic representations at large scale, like Bayesian\ncounterparts of pre-trained models on ImageNet, and further produce non-vacuous\ngeneralization bounds. We also extend this idea to a continual learning\nframework, where the favorable properties of our priors are desirable. Major\nenablers are our technical contributions: (1) the sums-of-Kronecker-product\ncomputations, and (2) the derivations and optimizations of tractable objectives\nthat lead to improved generalization bounds. Empirically, we exhaustively show\nthe effectiveness of this method for uncertainty estimation and generalization.",
    "authors": [
      [
        "Dominik Schnaus"
      ],
      [
        "Jongseok Lee"
      ],
      [
        "Daniel Cremers"
      ],
      [
        "Rudolph Triebel"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07753v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07753v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T09:24:33Z",
    "updated": "2023-07-15T09:24:33Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "stat.ML",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07752v1",
    "title": "Combining model-predictive control and predictive reinforcement learning\n  for stable quadrupedal robot locomotion",
    "summary": "Stable gait generation is a crucial problem for legged robot locomotion as\nthis impacts other critical performance factors such as, e.g. mobility over an\nuneven terrain and power consumption. Gait generation stability results from\nthe efficient control of the interaction between the legged robot's body and\nthe environment where it moves. Here, we study how this can be achieved by a\ncombination of model-predictive and predictive reinforcement learning\ncontrollers. Model-predictive control (MPC) is a well-established method that\ndoes not utilize any online learning (except for some adaptive variations) as\nit provides a convenient interface for state constraints management.\nReinforcement learning (RL), in contrast, relies on adaptation based on pure\nexperience. In its bare-bone variants, RL is not always suitable for robots due\nto their high complexity and expensive simulation/experimentation. In this\nwork, we combine both control methods to address the quadrupedal robot stable\ngate generation problem. The hybrid approach that we develop and apply uses a\ncost roll-out algorithm with a tail cost in the form of a Q-function modeled by\na neural network; this allows to alleviate the computational complexity, which\ngrows exponentially with the prediction horizon in a purely MPC approach. We\ndemonstrate that our RL gait controller achieves stable locomotion at short\nhorizons, where a nominal MP controller fails. Further, our controller is\ncapable of live operation, meaning that it does not require previous training.\nOur results suggest that the hybridization of MPC with RL, as presented here,\nis beneficial to achieve a good balance between online control capabilities and\ncomputational complexity.",
    "authors": [
      [
        "Vyacheslav Kovalev"
      ],
      [
        "Anna Shkromada"
      ],
      [
        "Henni Ouerdane"
      ],
      [
        "Pavel Osinenko"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07752v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07752v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T09:22:37Z",
    "updated": "2023-07-15T09:22:37Z",
    "categories": [
      {
        "term": "cs.RO",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.SY",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "eess.SY",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07742v1",
    "title": "SINC: Self-Supervised In-Context Learning for Vision-Language Tasks",
    "summary": "Large Pre-trained Transformers exhibit an intriguing capacity for in-context\nlearning. Without gradient updates, these models can rapidly construct new\npredictors from demonstrations presented in the inputs. Recent works promote\nthis ability in the vision-language domain by incorporating visual information\ninto large language models that can already make in-context predictions.\nHowever, these methods could inherit issues in the language domain, such as\ntemplate sensitivity and hallucination. Also, the scale of these language\nmodels raises a significant demand for computations, making learning and\noperating these models resource-intensive. To this end, we raise a question:\n``How can we enable in-context learning for general models without being\nconstrained on large language models?\". To answer it, we propose a succinct and\ngeneral framework, Self-supervised IN-Context learning (SINC), that introduces\na meta-model to learn on self-supervised prompts consisting of tailored\ndemonstrations. The learned models can be transferred to downstream tasks for\nmaking in-context predictions on-the-fly. Extensive experiments show that SINC\noutperforms gradient-based methods in various vision-language tasks under\nfew-shot settings. Furthermore, the designs of SINC help us investigate the\nbenefits of in-context learning across different tasks, and the analysis\nfurther reveals the essential components for the emergence of in-context\nlearning in the vision-language domain.",
    "authors": [
      [
        "Yi-Syuan Chen"
      ],
      [
        "Yun-Zhu Song"
      ],
      [
        "Cheng Yu Yeo"
      ],
      [
        "Bei Liu"
      ],
      [
        "Jianlong Fu"
      ],
      [
        "Hong-Han Shuai"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07742v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07742v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T08:33:08Z",
    "updated": "2023-07-15T08:33:08Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07734v1",
    "title": "Abstracting Concept-Changing Rules for Solving Raven's Progressive\n  Matrix Problems",
    "summary": "The abstract visual reasoning ability in human intelligence benefits\ndiscovering underlying rules in the novel environment. Raven's Progressive\nMatrix (RPM) is a classic test to realize such ability in machine intelligence\nby selecting from candidates. Recent studies suggest that solving RPM in an\nanswer-generation way boosts a more in-depth understanding of rules. However,\nexisting generative solvers cannot discover the global concept-changing rules\nwithout auxiliary supervision (e.g., rule annotations and distractors in\ncandidate sets). To this end, we propose a deep latent variable model for\nConcept-changing Rule ABstraction (CRAB) by learning interpretable concepts and\nparsing concept-changing rules in the latent space. With the iterative learning\nprocess, CRAB can automatically abstract global rules shared on the dataset on\neach concept and form the learnable prior knowledge of global rules. CRAB\noutperforms the baselines trained without auxiliary supervision in the\narbitrary-position answer generation task and achieves comparable and even\nhigher accuracy than the compared models trained with auxiliary supervision.\nFinally, we conduct experiments to illustrate the interpretability of CRAB in\nconcept learning, answer selection, and global rule abstraction.",
    "authors": [
      [
        "Fan Shi"
      ],
      [
        "Bin Li"
      ],
      [
        "Xiangyang Xue"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07734v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07734v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T07:16:38Z",
    "updated": "2023-07-15T07:16:38Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07700v1",
    "title": "NeurASP: Embracing Neural Networks into Answer Set Programming",
    "summary": "We present NeurASP, a simple extension of answer set programs by embracing\nneural networks. By treating the neural network output as the probability\ndistribution over atomic facts in answer set programs, NeurASP provides a\nsimple and effective way to integrate sub-symbolic and symbolic computation. We\ndemonstrate how NeurASP can make use of a pre-trained neural network in\nsymbolic computation and how it can improve the neural network's perception\nresult by applying symbolic reasoning in answer set programming. Also, NeurASP\ncan be used to train a neural network better by training with ASP rules so that\na neural network not only learns from implicit correlations from the data but\nalso from the explicit complex semantic constraints expressed by the rules.",
    "authors": [
      [
        "Zhun Yang"
      ],
      [
        "Adam Ishay"
      ],
      [
        "Joohyung Lee"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07700v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07700v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T04:03:17Z",
    "updated": "2023-07-15T04:03:17Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.SC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07699v1",
    "title": "Leveraging Large Language Models to Generate Answer Set Programs",
    "summary": "Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated\nexceptional performance in various natural language processing tasks and have\nshown the ability to solve certain reasoning problems. However, their reasoning\ncapabilities are limited and relatively shallow, despite the application of\nvarious prompting techniques. In contrast, formal logic is adept at handling\ncomplex reasoning, but translating natural language descriptions into formal\nlogic is a challenging task that non-experts struggle with. This paper proposes\na neuro-symbolic method that combines the strengths of large language models\nand answer set programming. Specifically, we employ an LLM to transform natural\nlanguage descriptions of logic puzzles into answer set programs. We carefully\ndesign prompts for an LLM to convert natural language descriptions into answer\nset programs in a step by step manner. Surprisingly, with just a few in-context\nlearning examples, LLMs can generate reasonably complex answer set programs.\nThe majority of errors made are relatively simple and can be easily corrected\nby humans, thus enabling LLMs to effectively assist in the creation of answer\nset programs.",
    "authors": [
      [
        "Adam Ishay"
      ],
      [
        "Zhun Yang"
      ],
      [
        "Joohyung Lee"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07699v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07699v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T03:40:55Z",
    "updated": "2023-07-15T03:40:55Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.SC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07696v1",
    "title": "Coupling Large Language Models with Logic Programming for Robust and\n  General Reasoning from Text",
    "summary": "While large language models (LLMs), such as GPT-3, appear to be robust and\ngeneral, their reasoning ability is not at a level to compete with the best\nmodels trained for specific natural language reasoning problems. In this study,\nwe observe that a large language model can serve as a highly effective few-shot\nsemantic parser. It can convert natural language sentences into a logical form\nthat serves as input for answer set programs, a logic-based declarative\nknowledge representation formalism. The combination results in a robust and\ngeneral system that can handle multiple question-answering tasks without\nrequiring retraining for each new task. It only needs a few examples to guide\nthe LLM's adaptation to a specific task, along with reusable ASP knowledge\nmodules that can be applied to multiple tasks. We demonstrate that this method\nachieves state-of-the-art performance on several NLP benchmarks, including\nbAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot\nplanning tasks that an LLM alone fails to solve.",
    "authors": [
      [
        "Zhun Yang"
      ],
      [
        "Adam Ishay"
      ],
      [
        "Joohyung Lee"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07696v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07696v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T03:29:59Z",
    "updated": "2023-07-15T03:29:59Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.SC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07691v1",
    "title": "A Survey on Change Detection Techniques in Document Images",
    "summary": "The problem of change detection in images finds application in different\ndomains like diagnosis of diseases in the medical field, detecting growth\npatterns of cities through remote sensing, and finding changes in legal\ndocuments and contracts. However, this paper presents a survey on core\ntechniques and rules to detect changes in different versions of a document\nimage. Our discussions on change detection focus on two categories --\ncontent-based and layout-based. The content-based techniques intelligently\nextract and analyze the image contents (text or non-text) to show the possible\ndifferences, whereas the layout-based techniques use structural information to\npredict document changes. We also summarize the existing datasets and\nevaluation metrics used in change detection experiments. The shortcomings and\nchallenges the existing methods face are reported, along with some pointers for\nfuture research work.",
    "authors": [
      [
        "Abhinandan Kumar Pun"
      ],
      [
        "Mohammed Javed"
      ],
      [
        "David S. Doermann"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07691v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07691v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T03:04:35Z",
    "updated": "2023-07-15T03:04:35Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07686v1",
    "title": "Creating a Dataset Supporting Translation Between OpenMP Fortran and C++\n  Code",
    "summary": "In this study, we present a novel dataset for training machine learning\nmodels translating between OpenMP Fortran and C++ code. To ensure reliability\nand applicability, the dataset is initially refined using a meticulous code\nsimilarity test. The effectiveness of our dataset is assessed using both\nquantitative (CodeBLEU) and qualitative (human evaluation) methods. We\ndemonstrate how this dataset can significantly improve the translation\ncapabilities of large-scale language models, with improvements of \\times 5.1\nfor models with no prior coding knowledge and \\times 9.9 for models with some\ncoding familiarity. Our work highlights the potential of this dataset to\nadvance the field of code translation for high-performance computing.",
    "authors": [
      [
        "Bin Lei"
      ],
      [
        "Caiwen Ding"
      ],
      [
        "Le Chen"
      ],
      [
        "Pei-Hung Lin"
      ],
      [
        "Chunhua Liao"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07686v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07686v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T02:35:51Z",
    "updated": "2023-07-15T02:35:51Z",
    "categories": [
      {
        "term": "cs.SE",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07670v1",
    "title": "Efficient Adversarial Attacks on Online Multi-agent Reinforcement\n  Learning",
    "summary": "Due to the broad range of applications of multi-agent reinforcement learning\n(MARL), understanding the effects of adversarial attacks against MARL model is\nessential for the safe applications of this model. Motivated by this, we\ninvestigate the impact of adversarial attacks on MARL. In the considered setup,\nthere is an exogenous attacker who is able to modify the rewards before the\nagents receive them or manipulate the actions before the environment receives\nthem. The attacker aims to guide each agent into a target policy or maximize\nthe cumulative rewards under some specific reward function chosen by the\nattacker, while minimizing the amount of manipulation on feedback and action.\nWe first show the limitations of the action poisoning only attacks and the\nreward poisoning only attacks. We then introduce a mixed attack strategy with\nboth the action poisoning and the reward poisoning. We show that the mixed\nattack strategy can efficiently attack MARL agents even if the attacker has no\nprior information about the underlying environment and the agents' algorithms.",
    "authors": [
      [
        "Guanlin Liu"
      ],
      [
        "Lifeng Lai"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07670v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07670v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T00:38:55Z",
    "updated": "2023-07-15T00:38:55Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CR",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "math.OC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07666v1",
    "title": "Efficient Action Robust Reinforcement Learning with Probabilistic Policy\n  Execution Uncertainty",
    "summary": "Robust reinforcement learning (RL) aims to find a policy that optimizes the\nworst-case performance in the face of uncertainties. In this paper, we focus on\naction robust RL with the probabilistic policy execution uncertainty, in which,\ninstead of always carrying out the action specified by the policy, the agent\nwill take the action specified by the policy with probability $1-\\rho$ and an\nalternative adversarial action with probability $\\rho$. We establish the\nexistence of an optimal policy on the action robust MDPs with probabilistic\npolicy execution uncertainty and provide the action robust Bellman optimality\nequation for its solution. Furthermore, we develop Action Robust Reinforcement\nLearning with Certificates (ARRLC) algorithm that achieves minimax optimal\nregret and sample complexity. Furthermore, we conduct numerical experiments to\nvalidate our approach's robustness, demonstrating that ARRLC outperforms\nnon-robust RL algorithms and converges faster than the robust TD algorithm in\nthe presence of action perturbations.",
    "authors": [
      [
        "Guanin Liu"
      ],
      [
        "Zhihan Zhou"
      ],
      [
        "Han Liu"
      ],
      [
        "Lifeng Lai"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07666v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07666v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-15T00:26:51Z",
    "updated": "2023-07-15T00:26:51Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "math.OC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07662v1",
    "title": "MPDIoU: A Loss for Efficient and Accurate Bounding Box Regression",
    "summary": "Bounding box regression (BBR) has been widely used in object detection and\ninstance segmentation, which is an important step in object localization.\nHowever, most of the existing loss functions for bounding box regression cannot\nbe optimized when the predicted box has the same aspect ratio as the\ngroundtruth box, but the width and height values are exactly different. In\norder to tackle the issues mentioned above, we fully explore the geometric\nfeatures of horizontal rectangle and propose a novel bounding box similarity\ncomparison metric MPDIoU based on minimum point distance, which contains all of\nthe relevant factors considered in the existing loss functions, namely\noverlapping or non-overlapping area, central points distance, and deviation of\nwidth and height, while simplifying the calculation process. On this basis, we\npropose a bounding box regression loss function based on MPDIoU, called LMPDIoU\n. Experimental results show that the MPDIoU loss function is applied to\nstate-of-the-art instance segmentation (e.g., YOLACT) and object detection\n(e.g., YOLOv7) model trained on PASCAL VOC, MS COCO, and IIIT5k outperforms\nexisting loss functions.",
    "authors": [
      [
        "Ma Siliang"
      ],
      [
        "Xu Yong"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07662v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07662v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T23:54:49Z",
    "updated": "2023-07-14T23:54:49Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07650v1",
    "title": "SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying\n  Indoor Localization",
    "summary": "Wireless indoor localization has attracted significant amount of attention in\nrecent years. Using received signal strength (RSS) obtained from WiFi access\npoints (APs) for establishing fingerprinting database is a widely utilized\nmethod in indoor localization. However, the time-variant problem for indoor\npositioning systems is not well-investigated in existing literature. Compared\nto conventional static fingerprinting, the dynamicallyreconstructed database\ncan adapt to a highly-changing environment, which achieves sustainability of\nlocalization accuracy. To deal with the time-varying issue, we propose a\nskeleton-assisted learning-based clustering localization (SALC) system,\nincluding RSS-oriented map-assisted clustering (ROMAC), cluster-based online\ndatabase establishment (CODE), and cluster-scaled location estimation (CsLE).\nThe SALC scheme jointly considers similarities from the skeleton-based shortest\npath (SSP) and the time-varying RSS measurements across the reference points\n(RPs). ROMAC clusters RPs into different feature sets and therefore selects\nsuitable monitor points (MPs) for enhancing location estimation. Moreover, the\nCODE algorithm aims for establishing adaptive fingerprint database to alleviate\nthe timevarying problem. Finally, CsLE is adopted to acquire the target\nposition by leveraging the benefits of clustering information and estimated\nsignal variations in order to rescale the weights fromweighted k-nearest\nneighbors (WkNN) method. Both simulation and experimental results demonstrate\nthat the proposed SALC system can effectively reconstruct the fingerprint\ndatabase with an enhanced location estimation accuracy, which outperforms the\nother existing schemes in the open literature.",
    "authors": [
      [
        "An-Hung Hsiao"
      ],
      [
        "Li-Hsiang Shen"
      ],
      [
        "Chen-Yi Chang"
      ],
      [
        "Chun-Jie Chiu"
      ],
      [
        "Kai-Ten Feng"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07650v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07650v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T22:55:52Z",
    "updated": "2023-07-14T22:55:52Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "eess.SP",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07645v1",
    "title": "Othering and low prestige framing of immigrant cuisines in US restaurant\n  reviews and large language models",
    "summary": "Identifying and understanding implicit attitudes toward food can help efforts\nto mitigate social prejudice due to food's pervasive role as a marker of\ncultural and ethnic identity. Stereotypes about food are a form of\nmicroaggression that contribute to harmful public discourse that may in turn\nperpetuate prejudice toward ethnic groups and negatively impact economic\noutcomes for restaurants. Through careful linguistic analyses, we evaluate\nsocial theories about attitudes toward immigrant cuisine in a large-scale study\nof framing differences in 2.1M English language Yelp reviews of restaurants in\n14 US states. Controlling for factors such as restaurant price and neighborhood\nracial diversity, we find that immigrant cuisines are more likely to be framed\nin objectifying and othering terms of authenticity (e.g., authentic,\ntraditional), exoticism (e.g., exotic, different), and prototypicality (e.g.,\ntypical, usual), but that non-Western immigrant cuisines (e.g., Indian,\nMexican) receive more othering than European cuisines (e.g., French, Italian).\nWe further find that non-Western immigrant cuisines are framed less positively\nand as lower status, being evaluated in terms of affordability and hygiene.\nFinally, we show that reviews generated by large language models (LLMs)\nreproduce many of the same framing tendencies. Our results empirically\ncorroborate social theories of taste and gastronomic stereotyping, and reveal\nlinguistic processes by which such attitudes are reified.",
    "authors": [
      [
        "Yiwei Luo"
      ],
      [
        "Kristina Gligorić"
      ],
      [
        "Dan Jurafsky"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07645v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07645v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T22:25:39Z",
    "updated": "2023-07-14T22:25:39Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07636v1",
    "title": "Dissenting Explanations: Leveraging Disagreement to Reduce Model\n  Overreliance",
    "summary": "While explainability is a desirable characteristic of increasingly complex\nblack-box models, modern explanation methods have been shown to be inconsistent\nand contradictory. The semantics of explanations is not always fully understood\n- to what extent do explanations \"explain\" a decision and to what extent do\nthey merely advocate for a decision? Can we help humans gain insights from\nexplanations accompanying correct predictions and not over-rely on incorrect\npredictions advocated for by explanations? With this perspective in mind, we\nintroduce the notion of dissenting explanations: conflicting predictions with\naccompanying explanations. We first explore the advantage of dissenting\nexplanations in the setting of model multiplicity, where multiple models with\nsimilar performance may have different predictions. In such cases, providing\ndissenting explanations could be done by invoking the explanations of\ndisagreeing models. Through a pilot study, we demonstrate that dissenting\nexplanations reduce overreliance on model predictions, without reducing overall\naccuracy. Motivated by the utility of dissenting explanations we present both\nglobal and local methods for their generation.",
    "authors": [
      [
        "Omer Reingold"
      ],
      [
        "Judy Hanwen Shen"
      ],
      [
        "Aditi Talati"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07636v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07636v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T21:27:00Z",
    "updated": "2023-07-14T21:27:00Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "68",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "I.2",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07628v1",
    "title": "Value-based Fast and Slow AI Nudging",
    "summary": "Nudging is a behavioral strategy aimed at influencing people's thoughts and\nactions. Nudging techniques can be found in many situations in our daily lives,\nand these nudging techniques can targeted at human fast and unconscious\nthinking, e.g., by using images to generate fear or the more careful and\neffortful slow thinking, e.g., by releasing information that makes us reflect\non our choices. In this paper, we propose and discuss a value-based AI-human\ncollaborative framework where AI systems nudge humans by proposing decision\nrecommendations. Three different nudging modalities, based on when\nrecommendations are presented to the human, are intended to stimulate human\nfast thinking, slow thinking, or meta-cognition. Values that are relevant to a\nspecific decision scenario are used to decide when and how to use each of these\nnudging modalities. Examples of values are decision quality, speed, human\nupskilling and learning, human agency, and privacy. Several values can be\npresent at the same time, and their priorities can vary over time. The\nframework treats values as parameters to be instantiated in a specific decision\nenvironment.",
    "authors": [
      [
        "Marianna B. Ganapini"
      ],
      [
        "Francesco Fabiano"
      ],
      [
        "Lior Horesh"
      ],
      [
        "Andrea Loreggia"
      ],
      [
        "Nicholas Mattei"
      ],
      [
        "Keerthiram Murugesan"
      ],
      [
        "Vishal Pallagani"
      ],
      [
        "Francesca Rossi"
      ],
      [
        "Biplav Srivastava"
      ],
      [
        "Brent Venable"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07628v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07628v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T20:57:27Z",
    "updated": "2023-07-14T20:57:27Z",
    "categories": [
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CY",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.HC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07469v1",
    "title": "Interactive Spatiotemporal Token Attention Network for Skeleton-based\n  General Interactive Action Recognition",
    "summary": "Recognizing interactive action plays an important role in human-robot\ninteraction and collaboration. Previous methods use late fusion and\nco-attention mechanism to capture interactive relations, which have limited\nlearning capability or inefficiency to adapt to more interacting entities. With\nassumption that priors of each entity are already known, they also lack\nevaluations on a more general setting addressing the diversity of subjects. To\naddress these problems, we propose an Interactive Spatiotemporal Token\nAttention Network (ISTA-Net), which simultaneously model spatial, temporal, and\ninteractive relations. Specifically, our network contains a tokenizer to\npartition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to\nrepresent motions of multiple diverse entities. By extending the entity\ndimension, ISTs provide better interactive representations. To jointly learn\nalong three dimensions in ISTs, multi-head self-attention blocks integrated\nwith 3D convolutions are designed to capture inter-token correlations. When\nmodeling correlations, a strict entity ordering is usually irrelevant for\nrecognizing interactive actions. To this end, Entity Rearrangement is proposed\nto eliminate the orderliness in ISTs for interchangeable entities. Extensive\nexperiments on four datasets verify the effectiveness of ISTA-Net by\noutperforming state-of-the-art methods. Our code is publicly available at\nhttps://github.com/Necolizer/ISTA-Net",
    "authors": [
      [
        "Yuhang Wen"
      ],
      [
        "Zixuan Tang"
      ],
      [
        "Yunsheng Pang"
      ],
      [
        "Beichen Ding"
      ],
      [
        "Mengyuan Liu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07469v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07469v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T16:51:25Z",
    "updated": "2023-07-14T16:51:25Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.RO",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07457v1",
    "title": "Structured Pruning of Neural Networks for Constraints Learning",
    "summary": "In recent years, the integration of Machine Learning (ML) models with\nOperation Research (OR) tools has gained popularity across diverse\napplications, including cancer treatment, algorithmic configuration, and\nchemical process optimization. In this domain, the combination of ML and OR\noften relies on representing the ML model output using Mixed Integer\nProgramming (MIP) formulations. Numerous studies in the literature have\ndeveloped such formulations for many ML predictors, with a particular emphasis\non Artificial Neural Networks (ANNs) due to their significant interest in many\napplications. However, ANNs frequently contain a large number of parameters,\nresulting in MIP formulations that are impractical to solve, thereby impeding\nscalability. In fact, the ML community has already introduced several\ntechniques to reduce the parameter count of ANNs without compromising their\nperformance, since the substantial size of modern ANNs presents challenges for\nML applications as it significantly impacts computational efforts during\ntraining and necessitates significant memory resources for storage. In this\npaper, we showcase the effectiveness of pruning, one of these techniques, when\napplied to ANNs prior to their integration into MIPs. By pruning the ANN, we\nachieve significant improvements in the speed of the solution process. We\ndiscuss why pruning is more suitable in this context compared to other ML\ncompression techniques, and we identify the most appropriate pruning\nstrategies. To highlight the potential of this approach, we conduct experiments\nusing feed-forward neural networks with multiple layers to construct\nadversarial examples. Our results demonstrate that pruning offers remarkable\nreductions in solution times without hindering the quality of the final\ndecision, enabling the resolution of previously unsolvable instances.",
    "authors": [
      [
        "Matteo Cacciola"
      ],
      [
        "Antonio Frangioni"
      ],
      [
        "Andrea Lodi"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07457v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07457v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T16:36:49Z",
    "updated": "2023-07-14T16:36:49Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "math.OC",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07443v1",
    "title": "Can Large Language Models Empower Molecular Property Prediction?",
    "summary": "Molecular property prediction has gained significant attention due to its\ntransformative potential in multiple scientific disciplines. Conventionally, a\nmolecule graph can be represented either as a graph-structured data or a SMILES\ntext. Recently, the rapid development of Large Language Models (LLMs) has\nrevolutionized the field of NLP. Although it is natural to utilize LLMs to\nassist in understanding molecules represented by SMILES, the exploration of how\nLLMs will impact molecular property prediction is still in its early stage. In\nthis work, we advance towards this objective through two perspectives:\nzero/few-shot molecular classification, and using the new explanations\ngenerated by LLMs as representations of molecules. To be specific, we first\nprompt LLMs to do in-context molecular classification and evaluate their\nperformance. After that, we employ LLMs to generate semantically enriched\nexplanations for the original SMILES and then leverage that to fine-tune a\nsmall-scale LM model for multiple downstream tasks. The experimental results\nhighlight the superiority of text explanations as molecular representations\nacross multiple benchmark datasets, and confirm the immense potential of LLMs\nin molecular property prediction tasks. Codes are available at\n\\url{https://github.com/ChnQ/LLM4Mol}.",
    "authors": [
      [
        "Chen Qian"
      ],
      [
        "Huayi Tang"
      ],
      [
        "Zhirui Yang"
      ],
      [
        "Hong Liang"
      ],
      [
        "Yong Liu"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07443v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07443v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T16:06:42Z",
    "updated": "2023-07-14T16:06:42Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "q-bio.QM",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07413v1",
    "title": "Exploiting Counter-Examples for Active Learning with Partial labels",
    "summary": "This paper studies a new problem, \\emph{active learning with partial labels}\n(ALPL). In this setting, an oracle annotates the query samples with partial\nlabels, relaxing the oracle from the demanding accurate labeling process. To\naddress ALPL, we first build an intuitive baseline that can be seamlessly\nincorporated into existing AL frameworks. Though effective, this baseline is\nstill susceptible to the \\emph{overfitting}, and falls short of the\nrepresentative partial-label-based samples during the query process. Drawing\ninspiration from human inference in cognitive science, where accurate\ninferences can be explicitly derived from \\emph{counter-examples} (CEs), our\nobjective is to leverage this human-like learning pattern to tackle the\n\\emph{overfitting} while enhancing the process of selecting representative\nsamples in ALPL. Specifically, we construct CEs by reversing the partial labels\nfor each instance, and then we propose a simple but effective WorseNet to\ndirectly learn from this complementary pattern. By leveraging the distribution\ngap between WorseNet and the predictor, this adversarial evaluation manner\ncould enhance both the performance of the predictor itself and the sample\nselection process, allowing the predictor to capture more accurate patterns in\nthe data. Experimental results on five real-world datasets and four benchmark\ndatasets show that our proposed method achieves comprehensive improvements over\nten representative AL frameworks, highlighting the superiority of WorseNet. The\nsource code will be available at \\url{https://github.com/Ferenas/APLL}.",
    "authors": [
      [
        "Fei Zhang"
      ],
      [
        "Yunjie Ye"
      ],
      [
        "Lei Feng"
      ],
      [
        "Zhongwen Rao"
      ],
      [
        "Jieming Zhu"
      ],
      [
        "Marcus Kalander"
      ],
      [
        "Chen Gong"
      ],
      [
        "Jianye Hao"
      ],
      [
        "Bo Han"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07413v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07413v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T15:41:53Z",
    "updated": "2023-07-14T15:41:53Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07392v1",
    "title": "Rank Your Summaries: Enhancing Bengali Text Summarization via\n  Ranking-based Approach",
    "summary": "With the increasing need for text summarization techniques that are both\nefficient and accurate, it becomes crucial to explore avenues that enhance the\nquality and precision of pre-trained models specifically tailored for\nsummarizing Bengali texts. When it comes to text summarization tasks, there are\nnumerous pre-trained transformer models at one's disposal. Consequently, it\nbecomes quite a challenge to discern the most informative and relevant summary\nfor a given text among the various options generated by these pre-trained\nsummarization models. This paper aims to identify the most accurate and\ninformative summary for a given text by utilizing a simple but effective\nranking-based approach that compares the output of four different pre-trained\nBengali text summarization models. The process begins by carrying out\npreprocessing of the input text that involves eliminating unnecessary elements\nsuch as special characters and punctuation marks. Next, we utilize four\npre-trained summarization models to generate summaries, followed by applying a\ntext ranking algorithm to identify the most suitable summary. Ultimately, the\nsummary with the highest ranking score is chosen as the final one. To evaluate\nthe effectiveness of this approach, the generated summaries are compared\nagainst human-annotated summaries using standard NLG metrics such as BLEU,\nROUGE, BERTScore, WIL, WER, and METEOR. Experimental results suggest that by\nleveraging the strengths of each pre-trained transformer model and combining\nthem using a ranking-based approach, our methodology significantly improves the\naccuracy and effectiveness of the Bengali text summarization.",
    "authors": [
      [
        "G. M. Shahariar"
      ],
      [
        "Tonmoy Talukder"
      ],
      [
        "Rafin Alam Khan Sotez"
      ],
      [
        "Md. Tanvir Rouf Shawon"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07392v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07392v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T15:07:20Z",
    "updated": "2023-07-14T15:07:20Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07370v1",
    "title": "AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention\n  and Text Attributes",
    "summary": "Image captioning is a significant field across computer vision and natural\nlanguage processing. We propose and present AIC-AB NET, a novel\nAttribute-Information-Combined Attention-Based Network that combines spatial\nattention architecture and text attributes in an encoder-decoder. For caption\ngeneration, adaptive spatial attention determines which image region best\nrepresents the image and whether to attend to the visual features or the visual\nsentinel. Text attribute information is synchronously fed into the decoder to\nhelp image recognition and reduce uncertainty. We have tested and evaluated our\nAICAB NET on the MS COCO dataset and a new proposed Fashion dataset. The\nFashion dataset is employed as a benchmark of single-object images. The results\nshow the superior performance of the proposed model compared to the\nstate-of-the-art baseline and ablated models on both the images from MSCOCO and\nour single-object images. Our AIC-AB NET outperforms the baseline adaptive\nattention network by 0.017 (CIDEr score) on the MS COCO dataset and 0.095\n(CIDEr score) on the Fashion dataset.",
    "authors": [
      [
        "Guoyun Tu"
      ],
      [
        "Ying Liu"
      ],
      [
        "Vladimir Vlassov"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07370v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07370v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T14:25:26Z",
    "updated": "2023-07-14T14:25:26Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07367v1",
    "title": "Are Large Language Models a Threat to Digital Public Goods? Evidence\n  from Activity on Stack Overflow",
    "summary": "Large language models like ChatGPT efficiently provide users with information\nabout various topics, presenting a potential substitute for searching the web\nand asking people for help online. But since users interact privately with the\nmodel, these models may drastically reduce the amount of publicly available\nhuman-generated data and knowledge resources. This substitution can present a\nsignificant problem in securing training data for future models. In this work,\nwe investigate how the release of ChatGPT changed human-generated open data on\nthe web by analyzing the activity on Stack Overflow, the leading online Q\\&A\nplatform for computer programming. We find that relative to its Russian and\nChinese counterparts, where access to ChatGPT is limited, and to similar forums\nfor mathematics, where ChatGPT is less capable, activity on Stack Overflow\nsignificantly decreased. A difference-in-differences model estimates a 16\\%\ndecrease in weekly posts on Stack Overflow. This effect increases in magnitude\nover time, and is larger for posts related to the most widely used programming\nlanguages. Posts made after ChatGPT get similar voting scores than before,\nsuggesting that ChatGPT is not merely displacing duplicate or low-quality\ncontent. These results suggest that more users are adopting large language\nmodels to answer questions and they are better substitutes for Stack Overflow\nfor languages for which they have more training data. Using models like ChatGPT\nmay be more efficient for solving certain programming problems, but its\nwidespread adoption and the resulting shift away from public exchange on the\nweb will limit the open data people and models can learn from in the future.",
    "authors": [
      [
        "Maria del Rio-Chanona"
      ],
      [
        "Nadzeya Laurentsyeva"
      ],
      [
        "Johannes Wachs"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07367v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07367v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T14:22:12Z",
    "updated": "2023-07-14T14:22:12Z",
    "categories": [
      {
        "term": "cs.SI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CY",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07542v1",
    "title": "Source-Free Domain Adaptation with Temporal Imputation for Time Series\n  Data",
    "summary": "Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a\nlabeled source domain to an unlabeled target domain without access to the\nsource domain data, preserving source domain privacy. Despite its prevalence in\nvisual applications, SFDA is largely unexplored in time series applications.\nThe existing SFDA methods that are mainly designed for visual applications may\nfail to handle the temporal dynamics in time series, leading to impaired\nadaptation performance. To address this challenge, this paper presents a simple\nyet effective approach for source-free domain adaptation on time series data,\nnamely MAsk and imPUte (MAPU). First, to capture temporal information of the\nsource domain, our method performs random masking on the time series signals\nwhile leveraging a novel temporal imputer to recover the original signal from a\nmasked version in the embedding space. Second, in the adaptation step, the\nimputer network is leveraged to guide the target model to produce target\nfeatures that are temporally consistent with the source features. To this end,\nour MAPU can explicitly account for temporal dependency during the adaptation\nwhile avoiding the imputation in the noisy input space. Our method is the first\nto handle temporal consistency in SFDA for time series data and can be\nseamlessly equipped with other existing SFDA methods. Extensive experiments\nconducted on three real-world time series datasets demonstrate that our MAPU\nachieves significant performance gain over existing methods. Our code is\navailable at \\url{https://github.com/mohamedr002/MAPU_SFDA_TS}.",
    "authors": [
      [
        "Mohamed Ragab"
      ],
      [
        "Emadeldeen Eldele"
      ],
      [
        "Min Wu"
      ],
      [
        "Chuan-Sheng Foo"
      ],
      [
        "Xiaoli Li"
      ],
      [
        "Zhenghua Chen"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07542v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07542v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T14:22:03Z",
    "updated": "2023-07-14T14:22:03Z",
    "categories": [
      {
        "term": "eess.SP",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07325v1",
    "title": "Representation Learning With Hidden Unit Clustering For Low Resource\n  Speech Applications",
    "summary": "The representation learning of speech, without textual resources, is an area\nof significant interest for many low resource speech applications. In this\npaper, we describe an approach to self-supervised representation learning from\nraw audio using a hidden unit clustering (HUC) framework. The input to the\nmodel consists of audio samples that are windowed and processed with 1-D\nconvolutional layers. The learned \"time-frequency\" representations from the\nconvolutional neural network (CNN) module are further processed with long short\nterm memory (LSTM) layers which generate a contextual vector representation for\nevery windowed segment. The HUC framework, allowing the categorization of the\nrepresentations into a small number of phoneme-like units, is used to train the\nmodel for learning semantically rich speech representations. The targets\nconsist of phoneme-like pseudo labels for each audio segment and these are\ngenerated with an iterative k-means algorithm. We explore techniques that\nimprove the speaker invariance of the learned representations and illustrate\nthe effectiveness of the proposed approach on two settings, i) completely\nunsupervised speech applications on the sub-tasks described as part of the\nZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition\n(ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi\ndataset. In these experiments, we achieve state-of-art results for various\nZeroSpeech tasks. Further, on the ASR experiments, the HUC representations are\nshown to improve significantly over other established benchmarks based on\nWav2vec, HuBERT and Best-RQ.",
    "authors": [
      [
        "Varun Krishna"
      ],
      [
        "Tarun Sai"
      ],
      [
        "Sriram Ganapathy"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07325v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07325v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T13:02:10Z",
    "updated": "2023-07-14T13:02:10Z",
    "categories": [
      {
        "term": "eess.AS",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07306v1",
    "title": "C3: Zero-shot Text-to-SQL with ChatGPT",
    "summary": "This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3,\nwhich achieves 82.3\\% in terms of execution accuracy on the holdout test set of\nSpider and becomes the state-of-the-art zero-shot Text-to-SQL method on the\nSpider Challenge. C3 consists of three key components: Clear Prompting (CP),\nCalibration with Hints (CH), and Consistent Output (CO), which are\ncorresponding to the model input, model bias and model output respectively. It\nprovides a systematic treatment for zero-shot Text-to-SQL. Extensive\nexperiments have been conducted to verify the effectiveness and efficiency of\nour proposed method.",
    "authors": [
      [
        "Xuemei Dong"
      ],
      [
        "Chao Zhang"
      ],
      [
        "Yuhang Ge"
      ],
      [
        "Yuren Mao"
      ],
      [
        "Yunjun Gao"
      ],
      [
        "lu Chen"
      ],
      [
        "Jinshu Lin"
      ],
      [
        "Dongfang Lou"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07306v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07306v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T12:30:41Z",
    "updated": "2023-07-14T12:30:41Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07286v1",
    "title": "One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton\n  Matching",
    "summary": "One-shot skeleton action recognition, which aims to learn a skeleton action\nrecognition model with a single training sample, has attracted increasing\ninterest due to the challenge of collecting and annotating large-scale skeleton\naction data. However, most existing studies match skeleton sequences by\ncomparing their feature vectors directly which neglects spatial structures and\ntemporal orders of skeleton data. This paper presents a novel one-shot skeleton\naction recognition technique that handles skeleton action recognition via\nmulti-scale spatial-temporal feature matching. We represent skeleton data at\nmultiple spatial and temporal scales and achieve optimal feature matching from\ntwo perspectives. The first is multi-scale matching which captures the\nscale-wise semantic relevance of skeleton data at multiple spatial and temporal\nscales simultaneously. The second is cross-scale matching which handles\ndifferent motion magnitudes and speeds by capturing sample-wise relevance\nacross multiple scales. Extensive experiments over three large-scale datasets\n(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior\none-shot skeleton action recognition, and it outperforms the state-of-the-art\nconsistently by large margins.",
    "authors": [
      [
        "Siyuan Yang"
      ],
      [
        "Jun Liu"
      ],
      [
        "Shijian Lu"
      ],
      [
        "Er Meng Hwa"
      ],
      [
        "Alex C. Kot"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07286v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07286v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T11:52:10Z",
    "updated": "2023-07-14T11:52:10Z",
    "categories": [
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07265v1",
    "title": "AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND\n  Audio-Based-Interaction-Recognition Challenge 2023",
    "summary": "This report presents the technical details of our submission to the 2023\nEpic-Kitchen EPIC-SOUNDS Audio-Based Interaction Recognition Challenge. The\ntask is to learn the mapping from audio samples to their corresponding action\nlabels. To achieve this goal, we propose a simple yet effective single-stream\nCNN-based architecture called AudioInceptionNeXt that operates on the\ntime-frequency log-mel-spectrogram of the audio samples. Motivated by the\ndesign of the InceptionNeXt, we propose parallel multi-scale depthwise\nseparable convolutional kernels in the AudioInceptionNeXt block, which enable\nthe model to learn the time and frequency information more effectively. The\nlarge-scale separable kernels capture the long duration of activities and the\nglobal frequency semantic information, while the small-scale separable kernels\ncapture the short duration of activities and local details of frequency\ninformation. Our approach achieved 55.43% of top-1 accuracy on the challenge\ntest set, ranked as 1st on the public leaderboard. Codes are available\nanonymously at https://github.com/StevenLauHKHK/AudioInceptionNeXt.git.",
    "authors": [
      [
        "Kin Wai Lau"
      ],
      [
        "Yasar Abbas Ur Rehman"
      ],
      [
        "Yuyang Xie"
      ],
      [
        "Lan Ma"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07265v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07265v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T10:39:05Z",
    "updated": "2023-07-14T10:39:05Z",
    "categories": [
      {
        "term": "cs.SD",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "eess.AS",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07260v1",
    "title": "A Dynamic Points Removal Benchmark in Point Cloud Maps",
    "summary": "In the field of robotics, the point cloud has become an essential map\nrepresentation. From the perspective of downstream tasks like localization and\nglobal path planning, points corresponding to dynamic objects will adversely\naffect their performance. Existing methods for removing dynamic points in point\nclouds often lack clarity in comparative evaluations and comprehensive\nanalysis. Therefore, we propose an easy-to-extend unified benchmarking\nframework for evaluating techniques for removing dynamic points in maps. It\nincludes refactored state-of-art methods and novel metrics to analyze the\nlimitations of these approaches. This enables researchers to dive deep into the\nunderlying reasons behind these limitations. The benchmark makes use of several\ndatasets with different sensor types. All the code and datasets related to our\nstudy are publicly available for further development and utilization.",
    "authors": [
      [
        "Qingwen Zhang"
      ],
      [
        "Daniel Duberg"
      ],
      [
        "Ruoyu Geng"
      ],
      [
        "Mingkai Jia"
      ],
      [
        "Lujia Wang"
      ],
      [
        "Patric Jensfelt"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07260v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07260v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T10:21:26Z",
    "updated": "2023-07-14T10:21:26Z",
    "categories": [
      {
        "term": "cs.RO",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07255v1",
    "title": "Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for\n  Designing Effective Conversational Systems",
    "summary": "Sharing ideas through communication with peers is the primary mode of human\ninteraction. Consequently, extensive research has been conducted in the area of\nconversational AI, leading to an increase in the availability and diversity of\nconversational tasks, datasets, and methods. However, with numerous tasks being\nexplored simultaneously, the current landscape of conversational AI becomes\nfragmented. Therefore, initiating a well-thought-out model for a dialogue agent\ncan pose significant challenges for a practitioner. Towards highlighting the\ncritical ingredients needed for a practitioner to design a dialogue agent from\nscratch, the current study provides a comprehensive overview of the primary\ncharacteristics of a dialogue agent, the supporting tasks, their corresponding\nopen-domain datasets, and the methods used to benchmark these datasets. We\nobserve that different methods have been used to tackle distinct dialogue\ntasks. However, building separate models for each task is costly and does not\nleverage the correlation among the several tasks of a dialogue agent. As a\nresult, recent trends suggest a shift towards building unified foundation\nmodels. To this end, we propose UNIT, a UNified dIalogue dataseT constructed\nfrom conversations of existing datasets for different dialogue tasks capturing\nthe nuances for each of them. We also examine the evaluation strategies used to\nmeasure the performance of dialogue agents and highlight the scope for future\nresearch in the area of conversational AI.",
    "authors": [
      [
        "Shivani Kumar"
      ],
      [
        "Sumit Bhatia"
      ],
      [
        "Milan Aggarwal"
      ],
      [
        "Tanmoy Chakraborty"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07255v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07255v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T10:05:47Z",
    "updated": "2023-07-14T10:05:47Z",
    "categories": [
      {
        "term": "cs.CL",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07250v1",
    "title": "Mitigating Adversarial Vulnerability through Causal Parameter Estimation\n  by Adversarial Double Machine Learning",
    "summary": "Adversarial examples derived from deliberately crafted perturbations on\nvisual inputs can easily harm decision process of deep neural networks. To\nprevent potential threats, various adversarial training-based defense methods\nhave grown rapidly and become a de facto standard approach for robustness.\nDespite recent competitive achievements, we observe that adversarial\nvulnerability varies across targets and certain vulnerabilities remain\nprevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with\ndeeper architectures and advanced defense methods. To address this issue, in\nthis paper, we introduce a causal approach called Adversarial Double Machine\nLearning (ADML), which allows us to quantify the degree of adversarial\nvulnerability for network predictions and capture the effect of treatments on\noutcome of interests. ADML can directly estimate causal parameter of\nadversarial perturbations per se and mitigate negative effects that can\npotentially damage robustness, bridging a causal perspective into the\nadversarial vulnerability. Through extensive experiments on various CNN and\nTransformer architectures, we corroborate that ADML improves adversarial\nrobustness with large margins and relieve the empirical observation.",
    "authors": [
      [
        "Byung-Kwan Lee"
      ],
      [
        "Junho Kim"
      ],
      [
        "Yong Man Ro"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07250v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07250v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T09:51:26Z",
    "updated": "2023-07-14T09:51:26Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.CV",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07248v1",
    "title": "Rigorous Runtime Analysis of Diversity Optimization with GSEMO on\n  OneMinMax",
    "summary": "The evolutionary diversity optimization aims at finding a diverse set of\nsolutions which satisfy some constraint on their fitness. In the context of\nmulti-objective optimization this constraint can require solutions to be\nPareto-optimal. In this paper we study how the GSEMO algorithm with additional\ndiversity-enhancing heuristic optimizes a diversity of its population on a\nbi-objective benchmark problem OneMinMax, for which all solutions are\nPareto-optimal.\n  We provide a rigorous runtime analysis of the last step of the optimization,\nwhen the algorithm starts with a population with a second-best diversity, and\nprove that it finds a population with optimal diversity in expected time\n$O(n^2)$, when the problem size $n$ is odd. For reaching our goal, we analyse\nthe random walk of the population, which reflects the frequency of changes in\nthe population and their outcomes.",
    "authors": [
      [
        "Denis Antipov"
      ],
      [
        "Aneta Neumann"
      ],
      [
        "Frank Neumann"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07248v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07248v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T09:43:29Z",
    "updated": "2023-07-14T09:43:29Z",
    "categories": [
      {
        "term": "cs.NE",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.07189v1",
    "title": "Multiplicative update rules for accelerating deep learning training and\n  increasing robustness",
    "summary": "Even nowadays, where Deep Learning (DL) has achieved state-of-the-art\nperformance in a wide range of research domains, accelerating training and\nbuilding robust DL models remains a challenging task. To this end, generations\nof researchers have pursued to develop robust methods for training DL\narchitectures that can be less sensitive to weight distributions, model\narchitectures and loss landscapes. However, such methods are limited to\nadaptive learning rate optimizers, initialization schemes, and clipping\ngradients without investigating the fundamental rule of parameters update.\nAlthough multiplicative updates have contributed significantly to the early\ndevelopment of machine learning and hold strong theoretical claims, to best of\nour knowledge, this is the first work that investigate them in context of DL\ntraining acceleration and robustness. In this work, we propose an optimization\nframework that fits to a wide range of optimization algorithms and enables one\nto apply alternative update rules. To this end, we propose a novel\nmultiplicative update rule and we extend their capabilities by combining it\nwith a traditional additive update term, under a novel hybrid update method. We\nclaim that the proposed framework accelerates training, while leading to more\nrobust models in contrast to traditionally used additive update rule and we\nexperimentally demonstrate their effectiveness in a wide range of task and\noptimization methods. Such tasks ranging from convex and non-convex\noptimization to difficult image classification benchmarks applying a wide range\nof traditionally used optimization methods and Deep Neural Network (DNN)\narchitectures.",
    "authors": [
      [
        "Manos Kirtas"
      ],
      [
        "Nikolaos Passalis"
      ],
      [
        "Anastasios Tefas"
      ]
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2307.07189v1",
        "rel": "alternate",
        "type": "text/html"
      },
      {
        "title": "pdf",
        "href": "http://arxiv.org/pdf/2307.07189v1",
        "rel": "related",
        "type": "application/pdf"
      }
    ],
    "published": "2023-07-14T06:44:43Z",
    "updated": "2023-07-14T06:44:43Z",
    "categories": [
      {
        "term": "cs.LG",
        "scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "term": "cs.AI",
        "scheme": "http://arxiv.org/schemas/atom"
      }
    ]
  }
]