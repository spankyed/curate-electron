[
  {
    "id": "2403.01199",
    "title": "The Case for Animal-Friendly AI",
    "abstract": "  Artificial intelligence is seen as increasingly important, and potentially\nprofoundly so, but the fields of AI ethics and AI engineering have not fully\nrecognized that these technologies, including large language models (LLMs),\nwill have massive impacts on animals. We argue that this impact matters,\nbecause animals matter morally.\n  As a first experiment in evaluating animal consideration in LLMs, we\nconstructed a proof-of-concept Evaluation System, which assesses LLM responses\nand biases from multiple perspectives. This system evaluates LLM outputs by two\ncriteria: their truthfulness, and the degree of consideration they give to the\ninterests of animals. We tested OpenAI ChatGPT 4 and Anthropic Claude 2.1 using\na set of structured queries and predefined normative perspectives. Preliminary\nresults suggest that the outcomes of the tested models can be benchmarked\nregarding the consideration they give to animals, and that generated positions\nand biases might be addressed and mitigated with more developed and validated\nsystems.\n  Our research contributes one possible approach to integrating animal ethics\nin AI, opening pathways for future studies and practical applications in\nvarious fields, including education, public policy, and regulation, that\ninvolve or relate to animals and society. Overall, this study serves as a step\ntowards more useful and responsible AI systems that better recognize and\nrespect the vital interests and perspectives of all sentient beings.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01199v1",
    "authors": ["Sankalpa Ghose", "Yip Fai Tse", "Kasra Rasaee", "Jeff Sebo", "Peter Singer"]
  },
  {
    "id": "2403.01038",
    "title": "AutoAttacker: A Large Language Model Guided System to Implement\n  Automatic Cyber-attacks",
    "abstract": "  Large language models (LLMs) have demonstrated impressive results on natural\nlanguage tasks, and security researchers are beginning to employ them in both\noffensive and defensive systems. In cyber-security, there have been multiple\nresearch efforts that utilize LLMs focusing on the pre-breach stage of attacks\nlike phishing and malware generation. However, so far there lacks a\ncomprehensive study regarding whether LLM-based systems can be leveraged to\nsimulate the post-breach stage of attacks that are typically human-operated, or\n\"hands-on-keyboard\" attacks, under various attack techniques and environments.\n  As LLMs inevitably advance, they may be able to automate both the pre- and\npost-breach attack stages. This shift may transform organizational attacks from\nrare, expert-led events to frequent, automated operations requiring no\nexpertise and executed at automation speed and scale. This risks fundamentally\nchanging global computer security and correspondingly causing substantial\neconomic impacts, and a goal of this work is to better understand these risks\nnow so we can better prepare for these inevitable ever-more-capable LLMs on the\nhorizon. On the immediate impact side, this research serves three purposes.\nFirst, an automated LLM-based, post-breach exploitation framework can help\nanalysts quickly test and continually improve their organization's network\nsecurity posture against previously unseen attacks. Second, an LLM-based\npenetration test system can extend the effectiveness of red teams with a\nlimited number of human analysts. Finally, this research can help defensive\nsystems and teams learn to detect novel attack behaviors preemptively before\ntheir use in the wild....\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01038v1",
    "authors": [
      "Jiacen Xu",
      "Jack W. Stokes",
      "Geoff McDonald",
      "Xuesong Bai",
      "David Marshall",
      "Siyue Wang",
      "Adith Swaminathan",
      "Zhou Li"
    ]
  },
  {
    "id": "2403.01071",
    "title": "GraphRCG: Self-conditioned Graph Generation via Bootstrapped\n  Representations",
    "abstract": "  Graph generation generally aims to create new graphs that closely align with\na specific graph distribution. Existing works often implicitly capture this\ndistribution through the optimization of generators, potentially overlooking\nthe intricacies of the distribution itself. Furthermore, these approaches\ngenerally neglect the insights offered by the learned distribution for graph\ngeneration. In contrast, in this work, we propose a novel self-conditioned\ngraph generation framework designed to explicitly model graph distributions and\nemploy these distributions to guide the generation process. We first perform\nself-conditioned modeling to capture the graph distributions by transforming\neach graph sample into a low-dimensional representation and optimizing a\nrepresentation generator to create new representations reflective of the\nlearned distribution. Subsequently, we leverage these bootstrapped\nrepresentations as self-conditioned guidance for the generation process,\nthereby facilitating the generation of graphs that more accurately reflect the\nlearned distributions. We conduct extensive experiments on generic and\nmolecular graph datasets across various fields. Our framework demonstrates\nsuperior performance over existing state-of-the-art graph generation methods in\nterms of graph quality and fidelity to training data.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01071v1",
    "authors": ["Song Wang", "Zhen Tan", "Xinyu Zhao", "Tianlong Chen", "Huan Liu", "Jundong Li"]
  },
  {
    "id": "2403.01079",
    "title": "Teaching MLP More Graph Information: A Three-stage Multitask Knowledge\n  Distillation Framework",
    "abstract": "  We study the challenging problem for inference tasks on large-scale graph\ndatasets of Graph Neural Networks: huge time and memory consumption, and try to\novercome it by reducing reliance on graph structure. Even though distilling\ngraph knowledge to student MLP is an excellent idea, it faces two major\nproblems of positional information loss and low generalization. To solve the\nproblems, we propose a new three-stage multitask distillation framework. In\ndetail, we use Positional Encoding to capture positional information. Also, we\nintroduce Neural Heat Kernels responsible for graph data processing in GNN and\nutilize hidden layer outputs matching for better performance of student MLP's\nhidden layers. To the best of our knowledge, it is the first work to include\nhidden layer distillation for student MLP on graphs and to combine graph\nPositional Encoding with MLP. We test its performance and robustness with\nseveral settings and draw the conclusion that our work can outperform well with\ngood stability.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01079v1",
    "authors": ["Junxian Li", "Bin Shi", "Erfei Cui", "Hua Wei", "Qinghua Zheng"]
  },
  {
    "id": "2403.01101",
    "title": "Feature Alignment: Rethinking Efficient Active Learning via Proxy in the\n  Context of Pre-trained Models",
    "abstract": "  Fine-tuning the pre-trained model with active learning holds promise for\nreducing annotation costs. However, this combination introduces significant\ncomputational costs, particularly with the growing scale of pre-trained models.\nRecent research has proposed proxy-based active learning, which pre-computes\nfeatures to reduce computational costs. Yet, this approach often incurs a\nsignificant loss in active learning performance, which may even outweigh the\ncomputational cost savings. In this paper, we argue the performance drop stems\nnot only from pre-computed features' inability to distinguish between\ncategories of labeled samples, resulting in the selection of redundant samples\nbut also from the tendency to compromise valuable pre-trained information when\nfine-tuning with samples selected through the proxy model. To address this\nissue, we propose a novel method called aligned selection via proxy to update\npre-computed features while selecting a proper training method to inherit\nvaluable pre-training information. Extensive experiments validate that our\nmethod significantly improves the total cost of efficient active learning while\nmaintaining computational efficiency.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01101v1",
    "authors": ["Ziting Wen", "Oscar Pizarro", "Stefan Williams"]
  },
  {
    "id": "2403.01106",
    "title": "Distilling Text Style Transfer With Self-Explanation From LLMs",
    "abstract": "  Text Style Transfer (TST) seeks to alter the style of text while retaining\nits core content. Given the constraints of limited parallel datasets for TST,\nwe propose CoTeX, a framework that leverages large language models (LLMs)\nalongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills\nthe complex rewriting and reasoning capabilities of LLMs into more streamlined\nmodels capable of working with both non-parallel and parallel data. Through\nexperimentation across four TST datasets, CoTeX is shown to surpass traditional\nsupervised fine-tuning and knowledge distillation methods, particularly in\nlow-resource settings. We conduct a comprehensive evaluation, comparing CoTeX\nagainst current unsupervised, supervised, in-context learning (ICL) techniques,\nand instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering\ntransparent explanations for its style transfer process.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01106v1",
    "authors": [
      "Chiyu Zhang",
      "Honglong Cai",
      " Yuezhang",
      " Li",
      "Yuexin Wu",
      "Le Hou",
      "Muhammad Abdul-Mageed"
    ]
  },
  {
    "id": "2403.01118",
    "title": "Adversarial Testing for Visual Grounding via Image-Aware Property\n  Reduction",
    "abstract": "  Due to the advantages of fusing information from various modalities,\nmultimodal learning is gaining increasing attention. Being a fundamental task\nof multimodal learning, Visual Grounding (VG), aims to locate objects in images\nthrough natural language expressions. Ensuring the quality of VG models\npresents significant challenges due to the complex nature of the task. In the\nblack box scenario, existing adversarial testing techniques often fail to fully\nexploit the potential of both modalities of information. They typically apply\nperturbations based solely on either the image or text information,\ndisregarding the crucial correlation between the two modalities, which would\nlead to failures in test oracles or an inability to effectively challenge VG\nmodels. To this end, we propose PEELING, a text perturbation approach via\nimage-aware property reduction for adversarial testing of the VG model. The\ncore idea is to reduce the property-related information in the original\nexpression meanwhile ensuring the reduced expression can still uniquely\ndescribe the original object in the image. To achieve this, PEELING first\nconducts the object and properties extraction and recombination to generate\ncandidate property reduction expressions. It then selects the satisfied\nexpressions that accurately describe the original object while ensuring no\nother objects in the image fulfill the expression, through querying the image\nwith a visual understanding technique. We evaluate PEELING on the\nstate-of-the-art VG model, i.e. OFA-VG, involving three commonly used datasets.\nResults show that the adversarial tests generated by PEELING achieves 21.4% in\nMultiModal Impact score (MMI), and outperforms state-of-the-art baselines for\nimages and texts by 8.2%--15.1%.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01118v1",
    "authors": [
      "Zhiyuan Chang",
      "Mingyang Li",
      "Junjie Wang",
      "Cheng Li",
      "Boyu Wu",
      "Fanjiang Xu",
      "Qing Wang"
    ]
  },
  {
    "id": "2403.01139",
    "title": "ParallelPARC: A Scalable Pipeline for Generating Natural-Language\n  Analogies",
    "abstract": "  Analogy-making is central to human cognition, allowing us to adapt to novel\nsituations -- an ability that current AI systems still lack. Most analogy\ndatasets today focus on simple analogies (e.g., word analogies); datasets\nincluding complex types of analogies are typically manually curated and very\nsmall. We believe that this holds back progress in computational analogy. In\nthis work, we design a data generation pipeline, ParallelPARC (Parallel\nParagraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to\ncreate complex, paragraph-based analogies, as well as distractors, both simple\nand challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset\nof analogies between scientific processes. We publish a gold-set, validated by\nhumans, and a silver-set, generated automatically. We test LLMs' and humans'\nanalogy recognition in binary and multiple-choice settings, and found that\nhumans outperform the best models (~13% gap) after a light supervision. We\ndemonstrate that our silver-set is useful for training models. Lastly, we show\nchallenging distractors confuse LLMs, but not humans. We hope our pipeline will\nencourage research in this emerging field.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01139v1",
    "authors": ["Oren Sultan", "Yonatan Bitton", "Ron Yosef", "Dafna Shahaf"]
  },
  {
    "id": "2403.01147",
    "title": "A Hybrid Model for Traffic Incident Detection based on Generative\n  Adversarial Networks and Transformer Model",
    "abstract": "  In addition to enhancing traffic safety and facilitating prompt emergency\nresponse, traffic incident detection plays an indispensable role in intelligent\ntransportation systems by providing real-time traffic status information. This\nenables the realization of intelligent traffic control and management. Previous\nresearch has identified that apart from employing advanced algorithmic models,\nthe effectiveness of detection is also significantly influenced by challenges\nrelated to acquiring large datasets and addressing dataset imbalances. A hybrid\nmodel combining transformer and generative adversarial networks (GANs) is\nproposed to address these challenges. Experiments are conducted on four real\ndatasets to validate the superiority of the transformer in traffic incident\ndetection. Additionally, GANs are utilized to expand the dataset and achieve a\nbalanced ratio of 1:4, 2:3, and 1:1. The proposed model is evaluated against\nthe baseline model. The results demonstrate that the proposed model enhances\nthe dataset size, balances the dataset, and improves the performance of traffic\nincident detection in various aspects.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01147v1",
    "authors": ["Xinying Lu", "Doudou Zhang", "Jianli Xiao"]
  },
  {
    "id": "2403.01152",
    "title": "A Survey of AI-generated Text Forensic Systems: Detection, Attribution,\n  and Characterization",
    "abstract": "  We have witnessed lately a rapid proliferation of advanced Large Language\nModels (LLMs) capable of generating high-quality text. While these LLMs have\nrevolutionized text generation across various domains, they also pose\nsignificant risks to the information ecosystem, such as the potential for\ngenerating convincing propaganda, misinformation, and disinformation at scale.\nThis paper offers a review of AI-generated text forensic systems, an emerging\nfield addressing the challenges of LLM misuses. We present an overview of the\nexisting efforts in AI-generated text forensics by introducing a detailed\ntaxonomy, focusing on three primary pillars: detection, attribution, and\ncharacterization. These pillars enable a practical understanding of\nAI-generated text, from identifying AI-generated content (detection),\ndetermining the specific AI model involved (attribution), and grouping the\nunderlying intents of the text (characterization). Furthermore, we explore\navailable resources for AI-generated text forensics research and discuss the\nevolving challenges and future directions of forensic systems in an AI era.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01152v1",
    "authors": [
      "Tharindu Kumarage",
      "Garima Agrawal",
      "Paras Sheth",
      "Raha Moraffah",
      "Aman Chadha",
      "Joshua Garland",
      "Huan Liu"
    ]
  },
  {
    "id": "2403.01165",
    "title": "STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient\n  Fine-Tuning of Large Language Models",
    "abstract": "  Though Large Language Models (LLMs) have demonstrated the powerful\ncapabilities of few-shot learning through prompting methods, supervised\ntraining is still necessary for complex reasoning tasks. Because of their\nextensive parameters and memory consumption, both Parameter-Efficient\nFine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been\nproposed for LLMs. Nevertheless, the issue of large annotated data consumption,\nthe aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is\nto combine the PEFT method with active learning. However, the experimental\nresults show that such a combination is not trivial and yields inferior\nresults. Through probe experiments, such observation might be explained by two\nmain reasons: uncertainty gap and poor model calibration. Therefore, in this\npaper, we propose a novel approach to effectively integrate uncertainty-based\nactive learning and LoRA. Specifically, for the uncertainty gap, we introduce a\ndynamic uncertainty measurement that combines the uncertainty of the base model\nand the uncertainty of the full model during the iteration of active learning.\nFor poor model calibration, we incorporate the regularization method during\nLoRA training to keep the model from being over-confident, and the Monte-Carlo\ndropout mechanism is employed to enhance the uncertainty estimation.\nExperimental results show that the proposed approach outperforms existing\nbaseline models on three complex reasoning tasks.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01165v1",
    "authors": ["Linhai Zhang", "Jialong Wu", "Deyu Zhou", "Guoqiang Xu"]
  },
  {
    "id": "2403.01166",
    "title": "DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable\n  Causal Inference",
    "abstract": "  Though notable progress has been made, neural-based aspect-based sentiment\nanalysis (ABSA) models are prone to learn spurious correlations from annotation\nbiases, resulting in poor robustness on adversarial data transformations. Among\nthe debiasing solutions, causal inference-based methods have attracted much\nresearch attention, which can be mainly categorized into causal intervention\nmethods and counterfactual reasoning methods. However, most of the present\ndebiasing methods focus on single-variable causal inference, which is not\nsuitable for ABSA with two input variables (the target aspect and the review).\nIn this paper, we propose a novel framework based on multi-variable causal\ninference for debiasing ABSA. In this framework, different types of biases are\ntackled based on different causal intervention methods. For the review branch,\nthe bias is modeled as indirect confounding from context, where backdoor\nadjustment intervention is employed for debiasing. For the aspect branch, the\nbias is described as a direct correlation with labels, where counterfactual\nreasoning is adopted for debiasing. Extensive experiments demonstrate the\neffectiveness of the proposed method compared to various baselines on the two\nwidely used real-world aspect robustness test set datasets.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01166v1",
    "authors": ["Jialong Wu", "Linhai Zhang", "Deyu Zhou", "Guoqiang Xu"]
  },
  {
    "id": "2403.01185",
    "title": "Balancing Exploration and Exploitation in LLM using Soft RLLF for\n  Enhanced Negation Understanding",
    "abstract": "  Finetuning approaches in NLP often focus on exploitation rather than\nexploration, which may lead to suboptimal models. Given the vast search space\nof natural language, this limited exploration can restrict their performance in\ncomplex, high-stakes domains, where accurate negation understanding and logical\nreasoning abilities are crucial. To address this issue, we leverage\nReinforcement Learning from Logical Feedback (RLLF) to create an effective\nbalance between exploration and exploitation in LLMs. Our approach employs an\nappropriate benchmark dataset for training and evaluation, highlighting the\nimportance of exploration in enhancing negation understanding capabilities. We\ncompare the performance of our RLLF-enhanced LLMs with baseline models trained\nwithout RLLF, demonstrating the value of this balanced approach. Furthermore,\nwe showcase the potential of our method in legal AI applications by employing\ntransfer learning and evaluating its impact on negation understanding. Our\nexperimental results exhibit the effectiveness of balancing exploration and\nexploitation with RLLF in improving LLMs' negation capabilities. This has\nimplications for the development of more accurate, reliable, and logically\nconsistent language models in high-stakes domains.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01185v1",
    "authors": ["Ha-Thanh Nguyen", "Ken Satoh"]
  },
  {
    "id": "2403.01193",
    "title": "RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots",
    "abstract": "  Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\nof artificial intelligence. However, their tendency to hallucinate -- generate\nplausible but false information -- poses a significant challenge. This issue is\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\nGeneration (RAG) can counter hallucinations by integrating external knowledge\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\nin some cases, but can still be misled when prompts directly contradict the\nmodel's pre-trained understanding. These findings highlight the complex nature\nof hallucinations and the need for more robust solutions to ensure LLM\nreliability in real-world applications. We offer practical recommendations for\nRAG deployment and discuss implications for the development of more trustworthy\nLLMs.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01193v1",
    "authors": ["Philip Feldman. James R. Foulds", "Shimei Pan"]
  },
  {
    "id": "2403.01196",
    "title": "Machine Translation in the Covid domain: an English-Irish case study for\n  LoResMT 2021",
    "abstract": "  Translation models for the specific domain of translating Covid data from\nEnglish to Irish were developed for the LoResMT 2021 shared task. Domain\nadaptation techniques, using a Covid-adapted generic 55k corpus from the\nDirectorate General of Translation, were applied. Fine-tuning, mixed\nfine-tuning and combined dataset approaches were compared with models trained\non an extended in-domain dataset. As part of this study, an English-Irish\ndataset of Covid related data, from the Health and Education domains, was\ndeveloped. The highest-performing model used a Transformer architecture trained\nwith an extended in-domain Covid dataset. In the context of this study, we have\ndemonstrated that extending an 8k in-domain baseline dataset by just 5k lines\nimproved the BLEU score by 27 points.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01196v1",
    "authors": ["Séamus Lankford", "Haithem Afli", "Andy Way"]
  },
  {
    "id": "2403.01210",
    "title": "SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with\n  Target Scattering Feature Parameters",
    "abstract": "  Deep neural network-based Synthetic Aperture Radar (SAR) target recognition\nmodels are susceptible to adversarial examples. Current adversarial example\ngeneration methods for SAR imagery primarily operate in the 2D digital domain,\nknown as image adversarial examples. Recent work, while considering SAR imaging\nscatter mechanisms, fails to account for the actual imaging process, rendering\nattacks in the three-dimensional physical domain infeasible, termed pseudo\nphysics adversarial examples. To address these challenges, this paper proposes\nSAR-AE-SFP-Attack, a method to generate real physics adversarial examples by\naltering the scattering feature parameters of target objects. Specifically, we\niteratively optimize the coherent energy accumulation of the target echo by\nperturbing the reflection coefficient and scattering coefficient in the\nscattering feature parameters of the three-dimensional target object, and\nobtain the adversarial example after echo signal processing and imaging\nprocessing in the RaySAR simulator. Experimental results show that compared to\ndigital adversarial attack methods, SAR-AE-SFP Attack significantly improves\nattack efficiency on CNN-based models (over 30\\%) and Transformer-based models\n(over 13\\%), demonstrating significant transferability of attack effects across\ndifferent models and perspectives.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01210v1",
    "authors": ["Jiahao Cui", "Jiale Duan", "Binyan Luo", "Hang Cao", "Wang Guo", "Haifeng Li"]
  },
  {
    "id": "2403.01221",
    "title": "A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual\n  Explanations",
    "abstract": "  Counterfactual explanations constitute among the most popular methods for\nanalyzing the predictions of black-box systems since they can recommend\ncost-efficient and actionable changes to the input to turn an undesired\nsystem's output into a desired output. While most of the existing\ncounterfactual methods explain a single instance, several real-world use cases,\nsuch as customer satisfaction, require the identification of a single\ncounterfactual that can satisfy multiple instances (e.g. customers)\nsimultaneously. In this work, we propose a flexible two-stage algorithm for\nfinding groups of instances along with cost-efficient multi-instance\ncounterfactual explanations. This is motivated by the fact that in most\nprevious works the aspect of finding such groups is not addressed.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01221v1",
    "authors": ["André Artelt", "Andreas Gregoriades"]
  },
  {
    "id": "2403.01232",
    "title": "Polynormer: Polynomial-Expressive Graph Transformer in Linear Time",
    "abstract": "  Graph transformers (GTs) have emerged as a promising architecture that is\ntheoretically more expressive than message-passing graph neural networks\n(GNNs). However, typical GT models have at least quadratic complexity and thus\ncannot scale to large graphs. While there are several linear GTs recently\nproposed, they still lag behind GNN counterparts on several popular graph\ndatasets, which poses a critical concern on their practical expressivity. To\nbalance the trade-off between expressivity and scalability of GTs, we propose\nPolynormer, a polynomial-expressive GT model with linear complexity. Polynormer\nis built upon a novel base model that learns a high-degree polynomial on input\nfeatures. To enable the base model permutation equivariant, we integrate it\nwith graph topology and node features separately, resulting in local and global\nequivariant attention models. Consequently, Polynormer adopts a linear\nlocal-to-global attention scheme to learn high-degree equivariant polynomials\nwhose coefficients are controlled by attention scores. Polynormer has been\nevaluated on $13$ homophilic and heterophilic datasets, including large graphs\nwith millions of nodes. Our extensive experiment results show that Polynormer\noutperforms state-of-the-art GNN and GT baselines on most datasets, even\nwithout the use of nonlinear activation functions.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01232v1",
    "authors": ["Chenhui Deng", "Zichao Yue", "Zhiru Zhang"]
  },
  {
    "id": "2403.01241",
    "title": "IntactKV: Improving Large Language Model Quantization by Keeping Pivot\n  Tokens Intact",
    "abstract": "  Large language models (LLMs) excel in natural language processing but demand\nintensive computation. To mitigate this, various quantization methods have been\nexplored, yet they compromise LLM performance. This paper unveils a previously\noverlooked type of outlier in LLMs. Such outliers are found to allocate most of\nthe attention scores on initial tokens of input, termed as pivot tokens, which\nis crucial to the performance of quantized LLMs. Given that, we propose\nIntactKV to generate the KV cache of pivot tokens losslessly from the\nfull-precision model. The approach is simple and easy to combine with existing\nquantization solutions. Besides, IntactKV can be calibrated as additional LLM\nparameters to boost the quantized LLMs further. Mathematical analysis also\nproves that IntactKV effectively reduces the upper bound of quantization error.\nEmpirical results show that IntactKV brings consistent improvement and achieves\nlossless weight-only INT4 quantization on various downstream tasks, leading to\nthe new state-of-the-art for LLM quantization.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01241v1",
    "authors": [
      "Ruikang Liu",
      "Haoli Bai",
      "Haokun Lin",
      "Yuening Li",
      "Han Gao",
      "Zhengzhuo Xu",
      "Lu Hou",
      "Jun Yao",
      "Chun Yuan"
    ]
  },
  {
    "id": "2403.01244",
    "title": "Mitigating Catastrophic Forgetting in Large Language Models with\n  Self-Synthesized Rehearsal",
    "abstract": "  Large language models (LLMs) suffer from catastrophic forgetting during\ncontinual learning. Conventional rehearsal-based methods rely on previous\ntraining data to retain the model's ability, which may not be feasible in\nreal-world applications. When conducting continual learning based on a\npublicly-released LLM checkpoint, the availability of the original training\ndata may be non-existent. To address this challenge, we propose a framework\ncalled Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic\ninstances for rehearsal. Concretely, we first employ the base LLM for\nin-context learning to generate synthetic instances. Subsequently, we utilize\nthe latest LLM to refine the instance outputs based on the synthetic inputs,\npreserving its acquired ability. Finally, we select diverse high-quality\nsynthetic instances for rehearsal in future stages. Experimental results\ndemonstrate that SSR achieves superior or comparable performance compared to\nconventional rehearsal-based approaches while being more data-efficient.\nBesides, SSR effectively preserves the generalization capabilities of LLMs in\ngeneral domains.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01244v1",
    "authors": [
      "Jianheng Huang",
      "Leyang Cui",
      "Ante Wang",
      "Chengyi Yang",
      "Xinting Liao",
      "Linfeng Song",
      "Junfeng Yao",
      "Jinsong Su"
    ]
  },
  {
    "id": "2403.01281",
    "title": "Fast Low-parameter Video Activity Localization in Collaborative Learning\n  Environments",
    "abstract": "  Research on video activity detection has primarily focused on identifying\nwell-defined human activities in short video segments. The majority of the\nresearch on video activity recognition is focused on the development of large\nparameter systems that require training on large video datasets. This paper\ndevelops a low-parameter, modular system with rapid inferencing capabilities\nthat can be trained entirely on limited datasets without requiring transfer\nlearning from large-parameter systems. The system can accurately detect and\nassociate specific activities with the students who perform the activities in\nreal-life classroom videos. Additionally, the paper develops an interactive\nweb-based application to visualize human activity maps over long real-life\nclassroom videos.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01281v1",
    "authors": [
      "Venkatesh Jatla",
      "Sravani Teeparthi",
      "Ugesh Egala",
      "Sylvia Celedon Pattichis",
      "Marios S. Patticis"
    ]
  },
  {
    "id": "2403.02354",
    "title": "Spatio-Temporal Field Neural Networks for Air Quality Inference",
    "abstract": "  The air quality inference problem aims to utilize historical data from a\nlimited number of observation sites to infer the air quality index at an\nunknown location. Considering the sparsity of data due to the high maintenance\ncost of the stations, good inference algorithms can effectively save the cost\nand refine the data granularity. While spatio-temporal graph neural networks\nhave made excellent progress on this problem, their non-Euclidean and discrete\ndata structure modeling of reality limits its potential. In this work, we make\nthe first attempt to combine two different spatio-temporal perspectives, fields\nand graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and\nits corresponding new framework, Pyramidal Inference. Extensive experiments\nvalidate that our model achieves state-of-the-art performance in nationwide air\nquality inference in the Chinese Mainland, demonstrating the superiority of our\nproposed model and framework.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.02354v1",
    "authors": [
      "Yutong Feng",
      "Qiongyan Wang",
      "Yutong Xia",
      "Junlin Huang",
      "Siru Zhong",
      "Kun Wang",
      "Shifen Cheng",
      "Yuxuan Liang"
    ]
  },
  {
    "id": "2403.02355",
    "title": "Temporal Knowledge Graph Completion with Time-sensitive Relations in\n  Hypercomplex Space",
    "abstract": "  Temporal knowledge graph completion (TKGC) aims to fill in missing facts\nwithin a given temporal knowledge graph at a specific time. Existing methods,\noperating in real or complex spaces, have demonstrated promising performance in\nthis task. This paper advances beyond conventional approaches by introducing\nmore expressive quaternion representations for TKGC within hypercomplex space.\nUnlike existing quaternion-based methods, our study focuses on capturing\ntime-sensitive relations rather than time-aware entities. Specifically, we\nmodel time-sensitive relations through time-aware rotation and periodic time\ntranslation, effectively capturing complex temporal variability. Furthermore,\nwe theoretically demonstrate our method's capability to model symmetric,\nasymmetric, inverse, compositional, and evolutionary relation patterns.\nComprehensive experiments on public datasets validate that our proposed\napproach achieves state-of-the-art performance in the field of TKGC.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.02355v1",
    "authors": [
      "Li Cai",
      "Xin Mao",
      "Zhihong Wang",
      "Shangqing Zhao",
      "Yuhao Zhou",
      "Changxu Wu",
      "Man Lan"
    ]
  },
  {
    "id": "2403.01053",
    "title": "Seeing Unseen: Discover Novel Biomedical Concepts via\n  Geometry-Constrained Probabilistic Modeling",
    "abstract": "  Machine learning holds tremendous promise for transforming the fundamental\npractice of scientific discovery by virtue of its data-driven nature. With the\never-increasing stream of research data collection, it would be appealing to\nautonomously explore patterns and insights from observational data for\ndiscovering novel classes of phenotypes and concepts. However, in the\nbiomedical domain, there are several challenges inherently presented in the\ncumulated data which hamper the progress of novel class discovery. The\nnon-i.i.d. data distribution accompanied by the severe imbalance among\ndifferent groups of classes essentially leads to ambiguous and biased semantic\nrepresentations. In this work, we present a geometry-constrained probabilistic\nmodeling treatment to resolve the identified issues. First, we propose to\nparameterize the approximated posterior of instance embedding as a marginal von\nMisesFisher distribution to account for the interference of distributional\nlatent bias. Then, we incorporate a suite of critical geometric properties to\nimpose proper constraints on the layout of constructed embedding space, which\nin turn minimizes the uncontrollable risk for unknown class learning and\nstructuring. Furthermore, a spectral graph-theoretic method is devised to\nestimate the number of potential novel classes. It inherits two intriguing\nmerits compared to existent approaches, namely high computational efficiency\nand flexibility for taxonomy-adaptive estimation. Extensive experiments across\nvarious biomedical scenarios substantiate the effectiveness and general\napplicability of our method.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01053v2",
    "authors": ["Jianan Fan", "Dongnan Liu", "Hang Chang", "Heng Huang", "Mei Chen", "Weidong Cai"]
  },
  {
    "id": "2403.01055",
    "title": "Towards Full Authorship with AI: Supporting Revision with AI-Generated\n  Views",
    "abstract": "  Large language models (LLMs) are shaping a new user interface (UI) paradigm\nin writing tools by enabling users to generate text through prompts. This\nparadigm shifts some creative control from the user to the system, thereby\ndiminishing the user's authorship and autonomy in the writing process. To\nrestore autonomy, we introduce Textfocals, a UI prototype designed to\ninvestigate a human-centered approach that emphasizes the user's role in\nwriting. Textfocals supports the writing process by providing LLM-generated\nsummaries, questions, and advice (i.e., LLM views) in a sidebar of a text\neditor, encouraging reflection and self-driven revision in writing without\ndirect text generation. Textfocals' UI affordances, including contextually\nadaptive views and scaffolding for prompt selection and customization, offer a\nnovel way to interact with LLMs where users maintain full authorship of their\nwriting. A formative user study with Textfocals showed promising evidence that\nthis approach might help users develop underdeveloped ideas, cater to the\nrhetorical audience, and clarify their writing. However, the study also showed\ninteraction design challenges related to document navigation and scoping,\nprompt engineering, and context management. Our work highlights the breadth of\nthe design space of writing support interfaces powered by generative AI that\nmaintain authorship integrity.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01055v1",
    "authors": [
      "Jiho Kim",
      "Ray C. Flanagan",
      "Noelle E. Haviland",
      "ZeAi Sun",
      "Souad N. Yakubu",
      "Edom A. Maru",
      "Kenneth C. Arnold"
    ]
  },
  {
    "id": "2403.01078",
    "title": "$Γ$-VAE: Curvature regularized variational autoencoders for\n  uncovering emergent low dimensional geometric structure in high dimensional\n  data",
    "abstract": "  Natural systems with emergent behaviors often organize along low-dimensional\nsubsets of high-dimensional spaces. For example, despite the tens of thousands\nof genes in the human genome, the principled study of genomics is fruitful\nbecause biological processes rely on coordinated organization that results in\nlower dimensional phenotypes. To uncover this organization, many nonlinear\ndimensionality reduction techniques have successfully embedded high-dimensional\ndata into low-dimensional spaces by preserving local similarities between data\npoints. However, the nonlinearities in these methods allow for too much\ncurvature to preserve general trends across multiple non-neighboring data\nclusters, thereby limiting their interpretability and generalizability to\nout-of-distribution data. Here, we address both of these limitations by\nregularizing the curvature of manifolds generated by variational autoencoders,\na process we coin ``$\\Gamma$-VAE''. We demonstrate its utility using two\nexample data sets: bulk RNA-seq from the The Cancer Genome Atlas (TCGA) and the\nGenotype Tissue Expression (GTEx); and single cell RNA-seq from a lineage\ntracing experiment in hematopoietic stem cell differentiation. We find that the\nresulting regularized manifolds identify mesoscale structure associated with\ndifferent cancer cell types, and accurately re-embed tissues from completely\nunseen, out-of distribution cancers as if they were originally trained on them.\nFinally, we show that preserving long-range relationships to differentiated\ncells separates undifferentiated cells -- which have not yet specialized --\naccording to their eventual fate. Broadly, we anticipate that regularizing the\ncurvature of generative models will enable more consistent, predictive, and\ngeneralizable models in any high-dimensional system with emergent\nlow-dimensional behavior.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01078v1",
    "authors": [
      "Jason Z. Kim",
      "Nicolas Perrin-Gilbert",
      "Erkan Narmanli",
      "Paul Klein",
      "Christopher R. Myers",
      "Itai Cohen",
      "Joshua J. Waterfall",
      "James P. Sethna"
    ]
  },
  {
    "id": "2403.01091",
    "title": "COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for\n  Traffic Forecasting",
    "abstract": "  This paper investigates traffic forecasting, which attempts to forecast the\nfuture state of traffic based on historical situations. This problem has\nreceived ever-increasing attention in various scenarios and facilitated the\ndevelopment of numerous downstream applications such as urban planning and\ntransportation management. However, the efficacy of existing methods remains\nsub-optimal due to their tendency to model temporal and spatial relationships\nindependently, thereby inadequately accounting for complex high-order\ninteractions of both worlds. Moreover, the diversity of transitional patterns\nin traffic forecasting makes them challenging to capture for existing\napproaches, warranting a deeper exploration of their diversity. Toward this\nend, this paper proposes Conjoint Spatio-Temporal graph neural network\n(abbreviated as COOL), which models heterogeneous graphs from prior and\nposterior information to conjointly capture high-order spatio-temporal\nrelationships. On the one hand, heterogeneous graphs connecting sequential\nobservation are constructed to extract composite spatio-temporal relationships\nvia prior message passing. On the other hand, we model dynamic relationships\nusing constructed affinity and penalty graphs, which guide posterior message\npassing to incorporate complementary semantic information into node\nrepresentations. Moreover, to capture diverse transitional properties to\nenhance traffic forecasting, we propose a conjoint self-attention decoder that\nmodels diverse temporal patterns from both multi-rank and multi-scale views.\nExperimental results on four popular benchmark datasets demonstrate that our\nproposed COOL provides state-of-the-art performance compared with the\ncompetitive baselines.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01091v1",
    "authors": [
      "Wei Ju",
      "Yusheng Zhao",
      "Yifang Qin",
      "Siyu Yi",
      "Jingyang Yuan",
      "Zhiping Xiao",
      "Xiao Luo",
      "Xiting Yan",
      "Ming Zhang"
    ]
  },
  {
    "id": "2403.01121",
    "title": "OpenGraph: Towards Open Graph Foundation Models",
    "abstract": "  Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01121v1",
    "authors": ["Lianghao Xia", "Ben Kao", "Chao Huang"]
  },
  {
    "id": "2403.01136",
    "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition\n  and Adaptive Quantization",
    "abstract": "  Recent breakthroughs in Large-scale language models (LLMs) have demonstrated\nimpressive performance on various tasks. The immense sizes of LLMs have led to\nvery high resource demand and cost for running the models. Though the models\nare largely served using uniform high-caliber GPUs nowadays, utilizing a\nheterogeneous cluster with a mix of available high- and low-capacity GPUs can\npotentially substantially reduce the serving cost. There is a lack of designs\nto support efficient LLM serving using a heterogeneous cluster, while the\ncurrent solutions focus on model partition and uniform compression among\nhomogeneous devices. This paper proposes LLM-PQ, a system that advocates\nadaptive model quantization and phase-aware partition to improve LLM serving\nefficiency on heterogeneous GPU clusters. We carefully decide on\nmixed-precision model quantization together with phase-aware model partition\nand micro-batch sizing in distributed LLM serving with an efficient algorithm,\nto greatly enhance inference throughput while fulfilling user-specified model\nquality targets. Extensive experiments on production inference workloads in 11\ndifferent clusters demonstrate that LLM-PQ achieves up to 2.88x (2.26x on\naverage) throughput improvement in inference, showing great advantages over\nstate-of-the-art works.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01136v1",
    "authors": ["Juntao Zhao", "Borui Wan", "Yanghua Peng", "Haibin Lin", "Chuan Wu"]
  },
  {
    "id": "2403.01183",
    "title": "Leveraging Self-Supervised Learning for Scene Recognition in Child\n  Sexual Abuse Imagery",
    "abstract": "  Crime in the 21st century is split into a virtual and real world. However,\nthe former has become a global menace to people's well-being and security in\nthe latter. The challenges it presents must be faced with unified global\ncooperation, and we must rely more than ever on automated yet trustworthy tools\nto combat the ever-growing nature of online offenses. Over 10 million child\nsexual abuse reports are submitted to the US National Center for Missing &\nExploited Children every year, and over 80% originated from online sources.\nTherefore, investigation centers and clearinghouses cannot manually process and\ncorrectly investigate all imagery. In light of that, reliable automated tools\nthat can securely and efficiently deal with this data are paramount. In this\nsense, the scene recognition task looks for contextual cues in the environment,\nbeing able to group and classify child sexual abuse data without requiring to\nbe trained on sensitive material. The scarcity and limitations of working with\nchild sexual abuse images lead to self-supervised learning, a machine-learning\nmethodology that leverages unlabeled data to produce powerful representations\nthat can be more easily transferred to target tasks. This work shows that\nself-supervised deep learning models pre-trained on scene-centric data can\nreach 71.6% balanced accuracy on our indoor scene classification task and, on\naverage, 2.2 percentage points better performance than a fully supervised\nversion. We cooperate with Brazilian Federal Police experts to evaluate our\nindoor classification model on actual child abuse material. The results\ndemonstrate a notable discrepancy between the features observed in widely used\nscene datasets and those depicted on sensitive materials.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01183v1",
    "authors": [
      "Pedro H. V. Valois",
      "João Macedo",
      "Leo S. F. Ribeiro",
      "Jefersson A. dos Santos",
      "Sandra Avila"
    ]
  },
  {
    "id": "2403.01216",
    "title": "API Is Enough: Conformal Prediction for Large Language Models Without\n  Logit-Access",
    "abstract": "  This study aims to address the pervasive challenge of quantifying uncertainty\nin large language models (LLMs) without logit-access. Conformal Prediction\n(CP), known for its model-agnostic and distribution-free features, is a desired\napproach for various LLMs and data distributions. However, existing CP methods\nfor LLMs typically assume access to the logits, which are unavailable for some\nAPI-only LLMs. In addition, logits are known to be miscalibrated, potentially\nleading to degraded CP performance. To tackle these challenges, we introduce a\nnovel CP method that (1) is tailored for API-only LLMs without logit-access;\n(2) minimizes the size of prediction sets; and (3) ensures a statistical\nguarantee of the user-defined coverage. The core idea of this approach is to\nformulate nonconformity measures using both coarse-grained (i.e., sample\nfrequency) and fine-grained uncertainty notions (e.g., semantic similarity).\nExperimental results on both close-ended and open-ended Question Answering\ntasks show our approach can mostly outperform the logit-based CP baselines.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01216v1",
    "authors": ["Jiayuan Su", "Jing Luo", "Hongwei Wang", "Lu Cheng"]
  },
  {
    "id": "2403.01229",
    "title": "REWIND Dataset: Privacy-preserving Speaking Status Segmentation from\n  Multimodal Body Movement Signals in the Wild",
    "abstract": "  Recognizing speaking in humans is a central task towards understanding social\ninteractions. Ideally, speaking would be detected from individual voice\nrecordings, as done previously for meeting scenarios. However, individual voice\nrecordings are hard to obtain in the wild, especially in crowded mingling\nscenarios due to cost, logistics, and privacy concerns. As an alternative,\nmachine learning models trained on video and wearable sensor data make it\npossible to recognize speech by detecting its related gestures in an\nunobtrusive, privacy-preserving way. These models themselves should ideally be\ntrained using labels obtained from the speech signal. However, existing\nmingling datasets do not contain high quality audio recordings. Instead,\nspeaking status annotations have often been inferred by human annotators from\nvideo, without validation of this approach against audio-based ground truth. In\nthis paper we revisit no-audio speaking status estimation by presenting the\nfirst publicly available multimodal dataset with high-quality individual speech\nrecordings of 33 subjects in a professional networking event. We present three\nbaselines for no-audio speaking status segmentation: a) from video, b) from\nbody acceleration (chest-worn accelerometer), c) from body pose tracks. In all\ncases we predict a 20Hz binary speaking status signal extracted from the audio,\na time resolution not available in previous datasets. In addition to providing\nthe signals and ground truth necessary to evaluate a wide range of speaking\nstatus detection methods, the availability of audio in REWIND makes it suitable\nfor cross-modality studies not feasible with previous mingling datasets.\nFinally, our flexible data consent setup creates new challenges for multimodal\nsystems under missing modalities.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01229v1",
    "authors": [
      "Jose Vargas Quiros",
      "Chirag Raman",
      "Stephanie Tan",
      "Ekin Gedik",
      "Laura Cabrera-Quiros",
      "Hayley Hung"
    ]
  },
  {
    "id": "2403.01242",
    "title": "Augmenting Automation: Intent-Based User Instruction Classification with\n  Machine Learning",
    "abstract": "  Electric automation systems offer convenience and efficiency in controlling\nelectrical circuits and devices. Traditionally, these systems rely on\npredefined commands for control, limiting flexibility and adaptability. In this\npaper, we propose a novel approach to augment automation by introducing\nintent-based user instruction classification using machine learning techniques.\nOur system represents user instructions as intents, allowing for dynamic\ncontrol of electrical circuits without relying on predefined commands. Through\na machine learning model trained on a labeled dataset of user instructions, our\nsystem classifies intents from user input, enabling a more intuitive and\nadaptable control scheme. We present the design and implementation of our\nintent-based electric automation system, detailing the development of the\nmachine learning model for intent classification. Experimental results\ndemonstrate the effectiveness of our approach in enhancing user experience and\nexpanding the capabilities of electric automation systems. Our work contributes\nto the advancement of smart technologies by providing a more seamless\ninteraction between users and their environments.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01242v1",
    "authors": ["Lochan Basyal", "Bijay Gaudel"]
  },
  {
    "id": "2403.01248",
    "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code",
    "abstract": "  This paper introduces SceneCraft, a Large Language Model (LLM) Agent\nconverting text descriptions into Blender-executable Python scripts which\nrender complex scenes with up to a hundred 3D assets. This process requires\ncomplex spatial planning and arrangement. We tackle these challenges through a\ncombination of advanced abstraction, strategic planning, and library learning.\nSceneCraft first models a scene graph as a blueprint, detailing the spatial\nrelationships among assets in the scene. SceneCraft then writes Python scripts\nbased on this graph, translating relationships into numerical constraints for\nasset layout. Next, SceneCraft leverages the perceptual strengths of\nvision-language foundation models like GPT-V to analyze rendered images and\niteratively refine the scene. On top of this process, SceneCraft features a\nlibrary learning mechanism that compiles common script functions into a\nreusable library, facilitating continuous self-improvement without expensive\nLLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses\nexisting LLM-based agents in rendering complex scenes, as shown by its\nadherence to constraints and favorable human assessments. We also showcase the\nbroader application potential of SceneCraft by reconstructing detailed 3D\nscenes from the Sintel movie and guiding a video generative model with\ngenerated scenes as intermediary control signal.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01248v1",
    "authors": [
      "Ziniu Hu",
      "Ahmet Iscen",
      "Aashi Jain",
      "Thomas Kipf",
      "Yisong Yue",
      "David A. Ross",
      "Cordelia Schmid",
      "Alireza Fathi"
    ]
  },
  {
    "id": "2403.01255",
    "title": "Automatic Speech Recognition using Advanced Deep Learning Approaches: A\n  survey",
    "abstract": "  Recent advancements in deep learning (DL) have posed a significant challenge\nfor automatic speech recognition (ASR). ASR relies on extensive training\ndatasets, including confidential ones, and demands substantial computational\nand storage resources. Enabling adaptive systems improves ASR performance in\ndynamic environments. DL techniques assume training and testing data originate\nfrom the same domain, which is not always true. Advanced DL techniques like\ndeep transfer learning (DTL), federated learning (FL), and reinforcement\nlearning (RL) address these issues. DTL allows high-performance models using\nsmall yet related datasets, FL enables training on confidential data without\ndataset possession, and RL optimizes decision-making in dynamic environments,\nreducing computation costs. This survey offers a comprehensive review of DTL,\nFL, and RL-based ASR frameworks, aiming to provide insights into the latest\ndevelopments and aid researchers and professionals in understanding the current\nchallenges. Additionally, transformers, which are advanced DL techniques\nheavily used in proposed ASR frameworks, are considered in this survey for\ntheir ability to capture extensive dependencies in the input ASR sequence. The\npaper starts by presenting the background of DTL, FL, RL, and Transformers and\nthen adopts a well-designed taxonomy to outline the state-of-the-art\napproaches. Subsequently, a critical analysis is conducted to identify the\nstrengths and weaknesses of each framework. Additionally, a comparative study\nis presented to highlight the existing challenges, paving the way for future\nresearch opportunities.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01255v1",
    "authors": ["Hamza Kheddar", "Mustapha Hemis", "Yassine Himeur"]
  },
  {
    "id": "2403.01273",
    "title": "NoMAD-Attention: Efficient LLM Inference on CPUs Through\n  Multiply-add-free Attention",
    "abstract": "  Large language model inference on Central Processing Units (CPU) is\nchallenging due to the vast quantities of expensive Multiply-Add (MAD) matrix\noperations in the attention computations. In this paper, we argue that there is\na rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers,\nwhich allow for ultra-low-latency lookups in batch. We leverage this unique\ncapability of CPUs to propose NoMAD-Attention, an efficient attention algorithm\nthat replaces MAD operations with in-register lookups. Through hardware-aware\nalgorithmic designs, NoMAD-Attention achieves the computation of attention\nscores using repeated fast accesses to SIMD registers despite their highly\nlimited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based\nLLMs without model finetuning. Empirical evaluations demonstrate that\nNoMAD-Attention maintains the quality of the original LLMs well, and speeds up\nthe 4-bit quantized LLaMA-7B-based model by up to 2$\\times$ at 16k context\nlength. Our results are reproducible at\nhttps://github.com/tonyzhang617/nomad-dist.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01273v1",
    "authors": [
      "Tianyi Zhang",
      "Jonah Wonkyu Yi",
      "Bowen Yao",
      "Zhaozhuo Xu",
      "Anshumali Shrivastava"
    ]
  },
  {
    "id": "2403.01277",
    "title": "Optimal Integrated Task and Path Planning and Its Application to\n  Multi-Robot Pickup and Delivery",
    "abstract": "  We propose a generic multi-robot planning mechanism that combines an optimal\ntask planner and an optimal path planner to provide a scalable solution for\ncomplex multi-robot planning problems. The Integrated planner, through the\ninteraction of the task planner and the path planner, produces optimal\ncollision-free trajectories for the robots. We illustrate our general algorithm\non an object pick-and-drop planning problem in a warehouse scenario where a\ngroup of robots is entrusted with moving objects from one location to another\nin the workspace. We solve the task planning problem by reducing it into an\nSMT-solving problem and employing the highly advanced SMT solver Z3 to solve\nit. To generate collision-free movement of the robots, we extend the\nstate-of-the-art algorithm Conflict Based Search with Precedence Constraints\nwith several domain-specific constraints. We evaluate our integrated task and\npath planner extensively on various instances of the object pick-and-drop\nplanning problem and compare its performance with a state-of-the-art\nmulti-robot classical planner. Experimental results demonstrate that our\nplanning mechanism can deal with complex planning problems and outperforms a\nstate-of-the-art classical planner both in terms of computation time and the\nquality of the generated plan.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01277v1",
    "authors": ["Aman Aryan", "Manan Modi", "Indranil Saha", "Rupak Majumdar", "Swarup Mohalik"]
  },
  {
    "id": "2403.01308",
    "title": "VBART: The Turkish LLM",
    "abstract": "  We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is 7x more\nefficient than OpenAI's multilingual tokenizer. Last but not least, we\nintroduce a method to enlarge an existing pre-trained LLM and question the\nrelevancy of Chinchilla Scaling Law to sequence-to-sequence masked language\nmodels. Our fine-tuned models, tokenizer and cleaned web corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01308v1",
    "authors": ["Meliksah Turker", "Mehmet Erdi Ari", "Aydin Han"]
  },
  {
    "id": "2403.01309",
    "title": "VNLP: Turkish NLP Package",
    "abstract": "  In this work, we present VNLP: the first dedicated, complete, open-source,\nwell-documented, lightweight, production-ready, state-of-the-art Natural\nLanguage Processing (NLP) package for the Turkish language. It contains a wide\nvariety of tools, ranging from the simplest tasks, such as sentence splitting\nand text normalization, to the more advanced ones, such as text and token\nclassification models. Its token classification models are based on \"Context\nModel\", a novel architecture that is both an encoder and an auto-regressive\nmodel. NLP tasks solved by VNLP models include but are not limited to Sentiment\nAnalysis, Named Entity Recognition, Morphological Analysis \\& Disambiguation\nand Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings\nand corresponding SentencePiece Unigram tokenizers. VNLP has an open-source\nGitHub repository, ReadtheDocs documentation, PyPi package for convenient\ninstallation, Python and command-line API and a demo page to test all the\nfunctionality. Consequently, our main contribution is a complete, compact,\neasy-to-install and easy-to-use NLP package for Turkish.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01309v1",
    "authors": ["Meliksah Turker", "Mehmet Erdi Ari", "Aydin Han"]
  },
  {
    "id": "2403.01329",
    "title": "Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow\n  Models",
    "abstract": "  This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver\ndistillation approach to improve sample efficiency of Diffusion and Flow\nmodels. BNS solvers are based on a family of non-stationary solvers that\nprovably subsumes existing numerical ODE solvers and consequently demonstrate\nconsiderable improvement in sample approximation (PSNR) over these baselines.\nCompared to model distillation, BNS solvers benefit from a tiny parameter space\n($<$200 parameters), fast optimization (two orders of magnitude faster),\nmaintain diversity of samples, and in contrast to previous solver distillation\napproaches nearly close the gap from standard distillation methods such as\nProgressive Distillation in the low-medium NFE regime. For example, BNS solver\nachieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We\nexperimented with BNS solvers for conditional image generation, text-to-image\ngeneration, and text-2-audio generation showing significant improvement in\nsample approximation (PSNR) in all.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01329v1",
    "authors": [
      "Neta Shaul",
      "Uriel Singer",
      "Ricky T. Q. Chen",
      "Matthew Le",
      "Ali Thabet",
      "Albert Pumarola",
      "Yaron Lipman"
    ]
  },
  {
    "id": "2403.01332",
    "title": "Chaining thoughts and LLMs to learn DNA structural biophysics",
    "abstract": "  The future development of an AI scientist, a tool that is capable of\nintegrating a variety of experimental data and generating testable hypotheses,\nholds immense potential. So far, bespoke machine learning models have been\ncreated to specialize in singular scientific tasks, but otherwise lack the\nflexibility of a general purpose model. Here, we show that a general purpose\nlarge language model, chatGPT 3.5-turbo, can be fine-tuned to learn the\nstructural biophysics of DNA. We find that both fine-tuning models to return\nchain-of-thought responses and chaining together models fine-tuned for subtasks\nhave an enhanced ability to analyze and design DNA sequences and their\nstructures.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01332v1",
    "authors": ["Tyler D. Ross", "Ashwin Gopinath"]
  },
  {
    "id": "2403.01046",
    "title": "A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex\n  Lasso Models with Reflection Features",
    "abstract": "  We prove that training neural networks on 1-D data is equivalent to solving a\nconvex Lasso problem with a fixed, explicitly defined dictionary matrix of\nfeatures. The specific dictionary depends on the activation and depth. We\nconsider 2-layer networks with piecewise linear activations, deep narrow ReLU\nnetworks with up to 4 layers, and rectangular and tree networks with sign\nactivation and arbitrary depth. Interestingly in ReLU networks, a fourth layer\ncreates features that represent reflections of training data about themselves.\nThe Lasso representation sheds insight to globally optimal networks and the\nsolution landscape.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01046v1",
    "authors": [
      "Emi Zeger",
      "Yifei Wang",
      "Aaron Mishkin",
      "Tolga Ergen",
      "Emmanuel Candès",
      "Mert Pilanci"
    ]
  },
  {
    "id": "2403.01131",
    "title": "LLaMoCo: Instruction Tuning of Large Language Models for Optimization\n  Code Generation",
    "abstract": "  Recent research explores optimization using large language models (LLMs) by\neither iteratively seeking next-step solutions from LLMs or directly prompting\nLLMs for an optimizer. However, these approaches exhibit inherent limitations,\nincluding low operational efficiency, high sensitivity to prompt design, and a\nlack of domain-specific knowledge. We introduce LLaMoCo, the first\ninstruction-tuning framework designed to adapt LLMs for solving optimization\nproblems in a code-to-code manner. Specifically, we establish a comprehensive\ninstruction set containing well-described problem prompts and effective\noptimization codes. We then develop a novel two-phase learning strategy that\nincorporates a contrastive learning-based warm-up procedure before the\ninstruction-tuning phase to enhance the convergence behavior during model\nfine-tuning. The experiment results demonstrate that a CodeGen (350M) model\nfine-tuned by our LLaMoCo achieves superior optimization performance compared\nto GPT-4 Turbo and the other competitors across both synthetic and realistic\nproblem sets. The fine-tuned model and the usage instructions are available at\nhttps://anonymous.4open.science/r/LLaMoCo-722A.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01131v2",
    "authors": [
      "Zeyuan Ma",
      "Hongshu Guo",
      "Jiacheng Chen",
      "Guojun Peng",
      "Zhiguang Cao",
      "Yining Ma",
      "Yue-Jiao Gong"
    ]
  },
  {
    "id": "2403.01286",
    "title": "Summary Paper: Use Case on Building Collaborative Safe Autonomous\n  Systems-A Robotdog for Guiding Visually Impaired People",
    "abstract": "  This is a summary paper of a use case of a Robotdog dedicated to guide\nvisually impaired people in complex environment like a smart intersection. In\nsuch scenarios, the Robotdog has to autonomously decide whether it is safe to\ncross the intersection or not in order to further guide the human. We leverage\ndata sharing and collaboration between the Robotdog and other autonomous\nsystems operating in the same environment. We propose a system architecture for\nautonomous systems through a separation of a collaborative decision layer, to\nenable collective decision making processes, where data about the environment,\nrelevant to the Robotdog decision, together with evidences for trustworthiness\nabout other systems and the environment are shared.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.01286v1",
    "authors": ["Aman Malhotra", "Selma Saidi"]
  }
]
