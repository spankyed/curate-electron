[
  {
    "id": "2402.13782",
    "title": "Semirings for Probabilistic and Neuro-Symbolic Logic Programming",
    "abstract": "  The field of probabilistic logic programming (PLP) focuses on integrating\nprobabilistic models into programming languages based on logic. Over the past\n30 years, numerous languages and frameworks have been developed for modeling,\ninference and learning in probabilistic logic programs. While originally PLP\nfocused on discrete probability, more recent approaches have incorporated\ncontinuous distributions as well as neural networks, effectively yielding\nneural-symbolic methods. We provide a unified algebraic perspective on PLP,\nshowing that many if not most of the extensions of PLP can be cast within a\ncommon algebraic logic programming framework, in which facts are labeled with\nelements of a semiring and disjunction and conjunction are replaced by addition\nand multiplication. This does not only hold for the PLP variations itself but\nalso for the underlying execution mechanism that is based on (algebraic) model\ncounting.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13782v1",
    "authors": [
      "Vincent Derkinderen",
      "Robin Manhaeve",
      "Pedro Zuidberg Dos Martires",
      "Luc De Raedt"
    ]
  },
  {
    "id": "2402.13785",
    "title": "Synthesis of Hierarchical Controllers Based on Deep Reinforcement\n  Learning Policies",
    "abstract": "  We propose a novel approach to the problem of controller design for\nenvironments modeled as Markov decision processes (MDPs). Specifically, we\nconsider a hierarchical MDP a graph with each vertex populated by an MDP called\na \"room\". We first apply deep reinforcement learning (DRL) to obtain low-level\npolicies for each room, scaling to large rooms of unknown structure. We then\napply reactive synthesis to obtain a high-level planner that chooses which\nlow-level policy to execute in each room. The central challenge in synthesizing\nthe planner is the need for modeling rooms. We address this challenge by\ndeveloping a DRL procedure to train concise \"latent\" policies together with PAC\nguarantees on their performance. Unlike previous approaches, ours circumvents a\nmodel distillation step. Our approach combats sparse rewards in DRL and enables\nreusability of low-level policies. We demonstrate feasibility in a case study\ninvolving agent navigation amid moving obstacles.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13785v1",
    "authors": [
      "Florent Delgrange",
      "Guy Avni",
      "Anna Lukina",
      "Christian Schilling",
      "Ann Nowé",
      "Guillermo A. Pérez"
    ]
  },
  {
    "id": "2402.13927",
    "title": "The Delusional Hedge Algorithm as a Model of Human Learning from Diverse\n  Opinions",
    "abstract": "  Whereas cognitive models of learning often assume direct experience with both\nthe features of an event and with a true label or outcome, much of everyday\nlearning arises from hearing the opinions of others, without direct access to\neither the experience or the ground truth outcome. We consider how people can\nlearn which opinions to trust in such scenarios by extending the hedge\nalgorithm: a classic solution for learning from diverse information sources. We\nfirst introduce a semi-supervised variant we call the delusional hedge capable\nof learning from both supervised and unsupervised experiences. In two\nexperiments, we examine the alignment between human judgments and predictions\nfrom the standard hedge, the delusional hedge, and a heuristic baseline model.\nResults indicate that humans effectively incorporate both labeled and unlabeled\ninformation in a manner consistent with the delusional hedge algorithm --\nsuggesting that human learners not only gauge the accuracy of information\nsources but also their consistency with other reliable sources. The findings\nadvance our understanding of human learning from diverse opinions, with\nimplications for the development of algorithms that better capture how people\nlearn to weigh conflicting information sources.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13927v1",
    "authors": ["Yun-Shiuan Chuang", "Jerry Zhu", "Timothy T. Rogers"]
  },
  {
    "id": "2402.14083",
    "title": "Beyond A*: Better Planning with Transformers via Search Dynamics\n  Bootstrapping",
    "abstract": "  While Transformers have enabled tremendous progress in various application\nsettings, such architectures still lag behind traditional symbolic planners for\nsolving complex decision making tasks. In this work, we demonstrate how to\ntrain Transformers to solve complex planning tasks and present Searchformer, a\nTransformer model that optimally solves previously unseen Sokoban puzzles 93.7%\nof the time, while using up to 26.8% fewer search steps than standard $A^*$\nsearch. Searchformer is an encoder-decoder Transformer model trained to predict\nthe search dynamics of $A^*$. This model is then fine-tuned via expert\niterations to perform fewer search steps than $A^*$ search while still\ngenerating an optimal plan. In our training method, $A^*$'s search dynamics are\nexpressed as a token sequence outlining when task states are added and removed\ninto the search tree during symbolic planning. In our ablation studies on maze\nnavigation, we find that Searchformer significantly outperforms baselines that\npredict the optimal plan directly with a 5-10$\\times$ smaller model size and a\n10$\\times$ smaller training dataset. We also demonstrate how Searchformer\nscales to larger and more complex decision making tasks like Sokoban with\nimproved percentage of solved tasks and shortened search dynamics.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14083v1",
    "authors": [
      "Lucas Lehnert",
      "Sainbayar Sukhbaatar",
      "Paul Mcvay",
      "Michael Rabbat",
      "Yuandong Tian"
    ]
  },
  {
    "id": "2402.13440",
    "title": "A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and\n  Probabilistic Decision Making",
    "abstract": "  Multi-agent reinforcement learning (MARL) is well-suited for runtime\ndecision-making in optimizing the performance of systems where multiple agents\ncoexist and compete for shared resources. However, applying common deep\nlearning-based MARL solutions to real-world problems suffers from issues of\ninterpretability, sample efficiency, partial observability, etc. To address\nthese challenges, we present an event-driven formulation, where decision-making\nis handled by distributed co-operative MARL agents using neuro-symbolic\nmethods. The recently introduced neuro-symbolic Logical Neural Networks (LNN)\nframework serves as a function approximator for the RL, to train a rules-based\npolicy that is both logical and interpretable by construction. To enable\ndecision-making under uncertainty and partial observability, we developed a\nnovel probabilistic neuro-symbolic framework, Probabilistic Logical Neural\nNetworks (PLNN), which combines the capabilities of logical reasoning with\nprobabilistic graphical models. In PLNN, the upward/downward inference\nstrategy, inherited from LNN, is coupled with belief bounds by setting the\nactivation function for the logical operator associated with each neural\nnetwork node to a probability-respecting generalization of the Fr\\'echet\ninequalities. These PLNN nodes form the unifying element that combines\nprobabilistic logic and Bayes Nets, permitting inference for variables with\nunobserved states. We demonstrate our contributions by addressing key MARL\nchallenges for power sharing in a system-on-chip application.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13440v1",
    "authors": [
      "Chitra Subramanian",
      "Miao Liu",
      "Naweed Khan",
      "Jonathan Lenchner",
      "Aporva Amarnath",
      "Sarathkrishna Swaminathan",
      "Ryan Riegel",
      "Alexander Gray"
    ]
  },
  {
    "id": "2402.13457",
    "title": "LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study",
    "abstract": "  Large Language Models (LLMS) have increasingly become central to generating\ncontent with potential societal impacts. Notably, these models have\ndemonstrated capabilities for generating content that could be deemed harmful.\nTo mitigate these risks, researchers have adopted safety training techniques to\nalign model outputs with societal values to curb the generation of malicious\ncontent. However, the phenomenon of \"jailbreaking\", where carefully crafted\nprompts elicit harmful responses from models, persists as a significant\nchallenge. This research conducts a comprehensive analysis of existing studies\non jailbreaking LLMs and their defense techniques. We meticulously investigate\nnine attack techniques and seven defense techniques applied across three\ndistinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate\nthe effectiveness of these attack and defense techniques. Our findings reveal\nthat existing white-box attacks underperform compared to universal techniques\nand that including special tokens in the input significantly affects the\nlikelihood of successful attacks. This research highlights the need to\nconcentrate on the security facets of LLMs. Additionally, we contribute to the\nfield by releasing our datasets and testing framework, aiming to foster further\nresearch into LLM security. We believe these contributions will facilitate the\nexploration of security measures within this domain.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13457v1",
    "authors": ["Zihao Xu", "Yi Liu", "Gelei Deng", "Yuekang Li", "Stjepan Picek"]
  },
  {
    "id": "2402.13462",
    "title": "Potential and Challenges of Model Editing for Social Debiasing",
    "abstract": "  Large language models (LLMs) trained on vast corpora suffer from inevitable\nstereotype biases. Mitigating these biases with fine-tuning could be both\ncostly and data-hungry. Model editing methods, which focus on modifying LLMs in\na post-hoc manner, are of great potential to address debiasing. However, it\nlacks a comprehensive study that facilitates both internal and external model\nediting methods, supports various bias types, as well as understands the pros\nand cons of applying editing methods to stereotypical debiasing. To mitigate\nthis gap, we carefully formulate social debiasing into an editing problem and\nbenchmark seven existing model editing algorithms on stereotypical debiasing,\ni.e., debias editing. Our findings in three scenarios reveal both the potential\nand challenges of debias editing: (1) Existing model editing methods can\neffectively preserve knowledge and mitigate biases, while the generalization of\ndebias effect from edited sentences to semantically equivalent sentences is\nlimited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et\nal. 2022b), while internal editing methods degenerate with the number of edits.\n(3) Model editing algorithms achieve generalization towards unseen biases both\nwithin the same type and from different types. In light of these findings, we\nfurther propose two simple but effective methods to improve debias editing, and\nexperimentally show the effectiveness of the proposed methods.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13462v1",
    "authors": ["Jianhao Yan", "Futing Wang", "Yafu Li", "Yue Zhang"]
  },
  {
    "id": "2402.13463",
    "title": "RefuteBench: Evaluating Refuting Instruction-Following for Large\n  Language Models",
    "abstract": "  The application scope of large language models (LLMs) is increasingly\nexpanding. In practical use, users might provide feedback based on the model's\noutput, hoping for a responsive model that can complete responses according to\ntheir feedback. Whether the model can appropriately respond to users' refuting\nfeedback and consistently follow through with execution has not been thoroughly\nanalyzed. In light of this, this paper proposes a comprehensive benchmark,\nRefuteBench, covering tasks such as question answering, machine translation,\nand email writing. The evaluation aims to assess whether models can positively\naccept feedback in form of refuting instructions and whether they can\nconsistently adhere to user demands throughout the conversation. We conduct\nevaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit\ninclination to their internal knowledge, often failing to comply with user\nfeedback. Additionally, as the length of the conversation increases, models\ngradually forget the user's stated feedback and roll back to their own\nresponses. We further propose a recall-and-repeat prompts as a simple and\neffective way to enhance the model's responsiveness to feedback.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13463v2",
    "authors": ["Jianhao Yan", "Yun Luo", "Yue Zhang"]
  },
  {
    "id": "2402.13475",
    "title": "Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal\n  Learning for Glaucoma Forecasting from Irregular Time Series Images",
    "abstract": "  Glaucoma is one of the major eye diseases that leads to progressive optic\nnerve fiber damage and irreversible blindness, afflicting millions of\nindividuals. Glaucoma forecast is a good solution to early screening and\nintervention of potential patients, which is helpful to prevent further\ndeterioration of the disease. It leverages a series of historical fundus images\nof an eye and forecasts the likelihood of glaucoma occurrence in the future.\nHowever, the irregular sampling nature and the imbalanced class distribution\nare two challenges in the development of disease forecasting approaches. To\nthis end, we introduce the Multi-scale Spatio-temporal Transformer Network\n(MST-former) based on the transformer architecture tailored for sequential\nimage inputs, which can effectively learn representative semantic information\nfrom sequential images on both temporal and spatial dimensions. Specifically,\nwe employ a multi-scale structure to extract features at various resolutions,\nwhich can largely exploit rich spatial information encoded in each image.\nBesides, we design a time distance matrix to scale time attention in a\nnon-linear manner, which could effectively deal with the irregularly sampled\ndata. Furthermore, we introduce a temperature-controlled Balanced Softmax\nCross-entropy loss to address the class imbalance issue. Extensive experiments\non the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset\ndemonstrate the superiority of the proposed MST-former method, achieving an AUC\nof 98.6% for glaucoma forecasting. Besides, our method shows excellent\ngeneralization capability on the Alzheimer's Disease Neuroimaging Initiative\n(ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and\nAlzheimer's disease prediction, outperforming the compared method by a large\nmargin.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13475v1",
    "authors": ["Xikai Yang", "Jian Wu", "Xi Wang", "Yuchen Yuan", "Ning Li Wang", "Pheng-Ann Heng"]
  },
  {
    "id": "2402.13481",
    "title": "Learning to Model Diverse Driving Behaviors in Highly Interactive\n  Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning",
    "abstract": "  Autonomous vehicles trained through Multi-Agent Reinforcement Learning (MARL)\nhave shown impressive results in many driving scenarios. However, the\nperformance of these trained policies can be impacted when faced with diverse\ndriving styles and personalities, particularly in highly interactive\nsituations. This is because conventional MARL algorithms usually operate under\nthe assumption of fully cooperative behavior among all agents and focus on\nmaximizing team rewards during training. To address this issue, we introduce\nthe Personality Modeling Network (PeMN), which includes a cooperation value\nfunction and personality parameters to model the varied interactions in\nhigh-interactive scenarios. The PeMN also enables the training of a background\ntraffic flow with diverse behaviors, thereby improving the performance and\ngeneralization of the ego vehicle. Our extensive experimental studies, which\nincorporate different personality parameters in high-interactive driving\nscenarios, demonstrate that the personality parameters effectively model\ndiverse driving styles and that policies trained with PeMN demonstrate better\ngeneralization compared to traditional MARL methods.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13481v1",
    "authors": ["Liu Weiwei", "Hu Wenxuan", "Jing Wei", "Lei Lanxin", "Gao Lingping", "Liu Yong"]
  },
  {
    "id": "2402.13514",
    "title": "Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer\n  for Compositional Unknown Questions",
    "abstract": "  Retrieve-then-read and generate-then-read are two typical solutions to handle\nunknown and known questions in open-domain question-answering, while the former\nretrieves necessary external knowledge and the later prompt the large language\nmodels to generate internal known knowledge encoded in the parameters. However,\nfew of previous works consider the compositional unknown questions, which\nconsist of several known or unknown sub-questions. Thus, simple binary\nclassification (known or unknown) becomes sub-optimal and inefficient since it\nwill call external retrieval excessively for each compositional unknown\nquestion. To this end, we propose the first Compositional unknown\nQuestion-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer\n(Self-DC) framework to empower LLMs to adaptively call different methods\non-demand, resulting in better performance and efficiency. Experimental results\non two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve\ncomparable or even better performance with much more less retrieval times\ncompared with several strong baselines.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13514v1",
    "authors": [
      "Hongru Wang",
      "Boyang Xue",
      "Baohang Zhou",
      "Tianhua Zhang",
      "Cunxiang Wang",
      "Guanhua Chen",
      "Huimin Wang",
      "Kam-fai Wong"
    ]
  },
  {
    "id": "2402.13517",
    "title": "Round Trip Translation Defence against Large Language Model Jailbreaking\n  Attacks",
    "abstract": "  Large language models (LLMs) are susceptible to social-engineered attacks\nthat are human-interpretable but require a high level of comprehension for LLMs\nto counteract. Existing defensive measures can only mitigate less than half of\nthese attacks at most. To address this issue, we propose the Round Trip\nTranslation (RTT) method, the first algorithm specifically designed to defend\nagainst social-engineered attacks on LLMs. RTT paraphrases the adversarial\nprompt and generalizes the idea conveyed, making it easier for LLMs to detect\ninduced harmful behavior. This method is versatile, lightweight, and\ntransferrable to different LLMs. Our defense successfully mitigated over 70% of\nPrompt Automatic Iterative Refinement (PAIR) attacks, which is currently the\nmost effective defense to the best of our knowledge. We are also the first to\nattempt mitigating the MathsAttack and reduced its attack success rate by\nalmost 40%. Our code is publicly available at\nhttps://github.com/Cancanxxx/Round_Trip_Translation_Defence\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13517v1",
    "authors": ["Canaan Yung", "Hadi Mohaghegh Dolatabadi", "Sarah Erfani", "Christopher Leckie"]
  },
  {
    "id": "2402.13521",
    "title": "Test-Driven Development for Code Generation",
    "abstract": "  Large language models (LLMs) like GPT4, have shown proficiency in generating\ncode snippets from problem statements. Traditionally software development by\nhumans followed a similar methodology of writing code from problem statements\nor requirements. However, in the past, there have been several studies that\nhave shown the value of test-driven development (TDD) where humans write tests\nbased on problem statements before the code for the functionality is written.\nIn the context of LLM-based code generation, one obvious benefit of TDD is that\nthe developer then knows for sure if the generated code has passed all the\ngiven tests or not. Therefore, in this paper, we want to empirically evaluate\nthe hypothesis: giving the problem statements and tests as input to GPT4 is\nbetter than just giving the problem statement as input. To test our hypothesis,\nwe build a framework TGen. In our experiments on the MBPP, HumanEval and\nCodeChef datasets, we consistently find that including tests solves more\nprogramming problems than not including them. Thus we show that TDD is a better\ndevelopment model than just using a problem statement when using GPT4 for code\ngeneration tasks.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13521v1",
    "authors": ["Noble Saji Mathews", "Meiyappan Nagappan"]
  },
  {
    "id": "2402.13534",
    "title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning\n  for Sequence Labeling",
    "abstract": "  Sequence labeling models often benefit from incorporating external knowledge.\nHowever, this practice introduces data heterogeneity and complicates the model\nwith additional modules, leading to increased expenses for training a\nhigh-performing model. To address this challenge, we propose a two-stage\ncurriculum learning (TCL) framework specifically designed for sequence labeling\ntasks. The TCL framework enhances training by gradually introducing data\ninstances from easy to hard, aiming to improve both performance and training\nspeed. Furthermore, we explore different metrics for assessing the difficulty\nlevels of sequence labeling tasks. Through extensive experimentation on six\nChinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we\ndemonstrate the effectiveness of our model in enhancing the performance of\nsequence labeling models. Additionally, our analysis indicates that TCL\naccelerates training and alleviates the slow training problem associated with\ncomplex models.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13534v1",
    "authors": ["Xuemei Tang", "Qi Su"]
  },
  {
    "id": "2402.13536",
    "title": "Exploring the Limits of Semantic Image Compression at Micro-bits per\n  Pixel",
    "abstract": "  Traditional methods, such as JPEG, perform image compression by operating on\nstructural information, such as pixel values or frequency content. These\nmethods are effective to bitrates around one bit per pixel (bpp) and higher at\nstandard image sizes. In contrast, text-based semantic compression directly\nstores concepts and their relationships using natural language, which has\nevolved with humans to efficiently represent these salient concepts. These\nmethods can operate at extremely low bitrates by disregarding structural\ninformation like location, size, and orientation. In this work, we use GPT-4V\nand DALL-E3 from OpenAI to explore the quality-compression frontier for image\ncompression and identify the limitations of current technology. We push\nsemantic compression as low as 100 $\\mu$bpp (up to $10,000\\times$ smaller than\nJPEG) by introducing an iterative reflection process to improve the decoded\nimage. We further hypothesize this 100 $\\mu$bpp level represents a soft limit\non semantic compression at standard image resolutions.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13536v1",
    "authors": ["Jordan Dotzel", "Bahaa Kotb", "James Dotzel", "Mohamed Abdelfattah", "Zhiru Zhang"]
  },
  {
    "id": "2402.13550",
    "title": "Are LLMs Effective Negotiators? Systematic Evaluation of the\n  Multifaceted Capabilities of LLMs in Negotiation Dialogues",
    "abstract": "  A successful negotiation demands a deep comprehension of the conversation\ncontext, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as\nstrategic reasoning and effective communication, making it challenging for\nautomated systems. Given the remarkable performance of LLMs across a variety of\nNLP tasks, in this work, we aim to understand how LLMs can advance different\naspects of negotiation research, ranging from designing dialogue systems to\nproviding pedagogical feedback and scaling up data collection practices. To\nthis end, we devise a methodology to analyze the multifaceted capabilities of\nLLMs across diverse dialogue scenarios covering all the time stages of a\ntypical negotiation interaction. Our analysis adds to the increasing evidence\nfor the superiority of GPT-4 across various tasks while also providing insights\ninto specific tasks that remain difficult for LLMs. For instance, the models\ncorrelate poorly with human players when making subjective assessments about\nthe negotiation dialogues and often struggle to generate responses that are\ncontextually appropriate as well as strategically advantageous.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13550v1",
    "authors": [
      "Deuksin Kwon",
      "Emily Weiss",
      "Tara Kulshrestha",
      "Kushal Chawla",
      "Gale M. Lucas",
      "Jonathan Gratch"
    ]
  },
  {
    "id": "2402.13556",
    "title": "Inductive Graph Alignment Prompt: Bridging the Gap between Graph\n  Pre-training and Inductive Fine-tuning From Spectral Perspective",
    "abstract": "  The \"Graph pre-training and fine-tuning\" paradigm has significantly improved\nGraph Neural Networks(GNNs) by capturing general knowledge without manual\nannotations for downstream tasks. However, due to the immense gap of data and\ntasks between the pre-training and fine-tuning stages, the model performance is\nstill limited. Inspired by prompt fine-tuning in Natural Language\nProcessing(NLP), many endeavors have been made to bridge the gap in graph\ndomain. But existing methods simply reformulate the form of fine-tuning tasks\nto the pre-training ones. With the premise that the pre-training graphs are\ncompatible with the fine-tuning ones, these methods typically operate in\ntransductive setting. In order to generalize graph pre-training to inductive\nscenario where the fine-tuning graphs might significantly differ from\npre-training ones, we propose a novel graph prompt based method called\nInductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph\npre-training frameworks and analyze the essence of graph pre-training from\ngraph spectral theory. Then we identify the two sources of the data gap in\ninductive setting: (i) graph signal gap and (ii) graph structure gap. Based on\nthe insight of graph pre-training, we propose to bridge the graph signal gap\nand the graph structure gap with learnable prompts in the spectral space. A\ntheoretical analysis ensures the effectiveness of our method. At last, we\nconduct extensive experiments among nodes classification and graph\nclassification tasks under the transductive, semi-inductive and inductive\nsettings. The results demonstrate that our proposed method can successfully\nbridge the data gap under different settings.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13556v1",
    "authors": ["Yuchen Yan", "Peiyan Zhang", "Zheng Fang", "Qingqing Long"]
  },
  {
    "id": "2402.13571",
    "title": "Multilingual Coreference Resolution in Low-resource South Asian\n  Languages",
    "abstract": "  Coreference resolution involves the task of identifying text spans within a\ndiscourse that pertain to the same real-world entity. While this task has been\nextensively explored in the English language, there has been a notable scarcity\nof publicly accessible resources and models for coreference resolution in South\nAsian languages. We introduce a Translated dataset for Multilingual Coreference\nResolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools\nfor translation and word-alignment. Nearly all of the predicted translations\nsuccessfully pass a sanity check, and 75% of English references align with\ntheir predicted translations. Using multilingual encoders, two off-the-shelf\ncoreference resolution models were trained on a concatenation of TransMuCoRes\nand a Hindi coreference resolution dataset with manual annotations. The best\nperforming model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,\nrespectively, on our test-split of Hindi golden set. This study is the first to\nevaluate an end-to-end coreference resolution model on a Hindi golden set.\nFurthermore, this work underscores the limitations of current coreference\nevaluation metrics when applied to datasets with split antecedents, advocating\nfor the development of more suitable evaluation metrics.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13571v1",
    "authors": ["Ritwik Mishra", "Pooja Desur", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"]
  },
  {
    "id": "2402.13575",
    "title": "Flexible Physical Camouflage Generation Based on a Differential Approach",
    "abstract": "  This study introduces a novel approach to neural rendering, specifically\ntailored for adversarial camouflage, within an extensive 3D rendering\nframework. Our method, named FPA, goes beyond traditional techniques by\nfaithfully simulating lighting conditions and material variations, ensuring a\nnuanced and realistic representation of textures on a 3D target. To achieve\nthis, we employ a generative approach that learns adversarial patterns from a\ndiffusion model. This involves incorporating a specially designed adversarial\nloss and covert constraint loss to guarantee the adversarial and covert nature\nof the camouflage in the physical world. Furthermore, we showcase the\neffectiveness of the proposed camouflage in sticker mode, demonstrating its\nability to cover the target without compromising adversarial information.\nThrough empirical and physical experiments, FPA exhibits strong performance in\nterms of attack success rate and transferability. Additionally, the designed\nsticker-mode camouflage, coupled with a concealment constraint, adapts to the\nenvironment, yielding diverse styles of texture. Our findings highlight the\nversatility and efficacy of the FPA approach in adversarial camouflage\napplications.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13575v1",
    "authors": [
      "Yang Li",
      "Wenyi Tan",
      "Chenxing Zhao",
      "Shuangju Zhou",
      "Xinkai Liang",
      "Quan Pan"
    ]
  },
  {
    "id": "2402.13582",
    "title": "Mastering the Game of Guandan with Deep Reinforcement Learning and\n  Behavior Regulating",
    "abstract": "  Games are a simplified model of reality and often serve as a favored platform\nfor Artificial Intelligence (AI) research. Much of the research is concerned\nwith game-playing agents and their decision making processes. The game of\nGuandan (literally, \"throwing eggs\") is a challenging game where even\nprofessional human players struggle to make the right decision at times. In\nthis paper we propose a framework named GuanZero for AI agents to master this\ngame using Monte-Carlo methods and deep neural networks. The main contribution\nof this paper is about regulating agents' behavior through a carefully designed\nneural network encoding scheme. We then demonstrate the effectiveness of the\nproposed framework by comparing it with state-of-the-art approaches.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13582v1",
    "authors": ["Yifan Yanggong", "Hao Pan", "Lei Wang"]
  },
  {
    "id": "2402.13602",
    "title": "Hybrid Reasoning Based on Large Language Models for Autonomous Car\n  Driving",
    "abstract": "  Large Language Models (LLMs) have garnered significant attention for their\nability to understand text and images, generate human-like text, and perform\ncomplex reasoning tasks. However, their ability to generalize this advanced\nreasoning with a combination of natural language text for decision-making in\ndynamic situations requires further exploration. In this study, we investigate\nhow well LLMs can adapt and apply a combination of arithmetic and common-sense\nreasoning, particularly in autonomous driving scenarios. We hypothesize that\nLLMs hybrid reasoning abilities can improve autonomous driving by enabling them\nto analyze detected object and sensor data, understand driving regulations and\nphysical laws, and offer additional context. This addresses complex scenarios,\nlike decisions in low visibility (due to weather conditions), where traditional\nmethods might fall short. We evaluated Large Language Models (LLMs) based on\naccuracy by comparing their answers with human-generated ground truth inside\nCARLA. The results showed that when a combination of images (detected objects)\nand sensor data is fed into the LLM, it can offer precise information for brake\nand throttle control in autonomous vehicles across various weather conditions.\nThis formulation and answers can assist in decision-making for auto-pilot\nsystems.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13602v1",
    "authors": [
      "Mehdi Azarafza",
      "Mojtaba Nayyeri",
      "Charles Steinmetz",
      "Steffen Staab",
      "Achim Rettberg"
    ]
  },
  {
    "id": "2402.13635",
    "title": "The METRIC-framework for assessing data quality for trustworthy AI in\n  medicine: a systematic review",
    "abstract": "  The adoption of machine learning (ML) and, more specifically, deep learning\n(DL) applications into all major areas of our lives is underway. The\ndevelopment of trustworthy AI is especially important in medicine due to the\nlarge implications for patients' lives. While trustworthiness concerns various\naspects including ethical, technical and privacy requirements, we focus on the\nimportance of data quality (training/test) in DL. Since data quality dictates\nthe behaviour of ML products, evaluating data quality will play a key part in\nthe regulatory approval of medical AI products. We perform a systematic review\nfollowing PRISMA guidelines using the databases PubMed and ACM Digital Library.\nWe identify 2362 studies, out of which 62 records fulfil our eligibility\ncriteria. From this literature, we synthesise the existing knowledge on data\nquality frameworks and combine it with the perspective of ML applications in\nmedicine. As a result, we propose the METRIC-framework, a specialised data\nquality framework for medical training data comprising 15 awareness dimensions,\nalong which developers of medical ML applications should investigate a dataset.\nThis knowledge helps to reduce biases as a major source of unfairness, increase\nrobustness, facilitate interpretability and thus lays the foundation for\ntrustworthy AI in medicine. Incorporating such systematic assessment of medical\ndatasets into regulatory approval processes has the potential to accelerate the\napproval of ML products and builds the basis for new standards.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13635v1",
    "authors": [
      "Daniel Schwabe",
      "Katinka Becker",
      "Martin Seyferth",
      "Andreas Klaß",
      "Tobias Schäffter"
    ]
  },
  {
    "id": "2402.13647",
    "title": "Unsupervised Text Style Transfer via LLMs and Attention Masking with\n  Multi-way Interactions",
    "abstract": "  Unsupervised Text Style Transfer (UTST) has emerged as a critical task within\nthe domain of Natural Language Processing (NLP), aiming to transfer one\nstylistic aspect of a sentence into another style without changing its\nsemantics, syntax, or other attributes. This task is especially challenging\ngiven the intrinsic lack of parallel text pairings. Among existing methods for\nUTST tasks, attention masking approach and Large Language Models (LLMs) are\ndeemed as two pioneering methods. However, they have shortcomings in generating\nunsmooth sentences and changing the original contents, respectively. In this\npaper, we investigate if we can combine these two methods effectively. We\npropose four ways of interactions, that are pipeline framework with tuned\norders; knowledge distillation from LLMs to attention masking model; in-context\nlearning with constructed parallel examples. We empirically show these\nmulti-way interactions can improve the baselines in certain perspective of\nstyle strength, content preservation and text fluency. Experiments also\ndemonstrate that simply conducting prompting followed by attention\nmasking-based revision can consistently surpass the other systems, including\nsupervised text style transfer systems. On Yelp-clean and Amazon-clean\ndatasets, it improves the previously best mean metric by 0.5 and 3.0 absolute\npercentages respectively, and achieves new SOTA results.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13647v1",
    "authors": ["Lei Pan", "Yunshi Lan", "Yang Li", "Weining Qian"]
  },
  {
    "id": "2402.13671",
    "title": "KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual\n  Machine-Generated Text Detection",
    "abstract": "  SemEval-2024 Task 8 is focused on multigenerator, multidomain, and\nmultilingual black-box machine-generated text detection. Such a detection is\nimportant for preventing a potential misuse of large language models (LLMs),\nthe newest of which are very capable in generating multilingual human-like\ntexts. We have coped with this task in multiple ways, utilizing language\nidentification and parameter-efficient fine-tuning of smaller LLMs for text\nclassification. We have further used the per-language classification-threshold\ncalibration to uniquely combine fine-tuned models predictions with statistical\ndetection metrics to improve generalization of the system detection\nperformance. Our submitted method achieved competitive results, ranking at the\nfourth place, just under 1 percentage point behind the winner.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13671v1",
    "authors": ["Michal Spiegel", "Dominik Macko"]
  },
  {
    "id": "2402.13709",
    "title": "SaGE: Evaluating Moral Consistency in Large Language Models",
    "abstract": "  Despite recent advancements showcasing the impressive capabilities of Large\nLanguage Models (LLMs) in conversational systems, we show that even\nstate-of-the-art LLMs are morally inconsistent in their generations,\nquestioning their reliability (and trustworthiness in general). Prior works in\nLLM evaluation focus on developing ground-truth data to measure accuracy on\nspecific tasks. However, for moral scenarios that often lack universally\nagreed-upon answers, consistency in model responses becomes crucial for their\nreliability. To address this issue, we propose an information-theoretic measure\ncalled Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of\nThumb\" (RoTs) to measure a model's moral consistency. RoTs are abstract\nprinciples learned by a model and can help explain their decision-making\nstrategies effectively. To this extent, we construct the Moral Consistency\nCorpus (MCC), containing 50K moral questions, responses to them by LLMs, and\nthe RoTs that these models followed. Furthermore, to illustrate the\ngeneralizability of SaGE, we use it to investigate LLM consistency on two\npopular datasets -- TruthfulQA and HellaSwag. Our results reveal that\ntask-accuracy and consistency are independent problems, and there is a dire\nneed to investigate these issues further.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13709v1",
    "authors": [
      "Vamshi Krishna Bonagiri",
      "Sreeram Vennam",
      "Priyanshul Govil",
      "Ponnurangam Kumaraguru",
      "Manas Gaur"
    ]
  },
  {
    "id": "2402.13711",
    "title": "DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based\n  Graph Continual Learning",
    "abstract": "  We investigate the replay buffer in rehearsal-based approaches for graph\ncontinual learning (GCL) methods. Existing rehearsal-based GCL methods select\nthe most representative nodes for each class and store them in a replay buffer\nfor later use in training subsequent tasks. However, we discovered that\nconsidering only the class representativeness of each replayed node makes the\nreplayed nodes to be concentrated around the center of each class, incurring a\npotential risk of overfitting to nodes residing in those regions, which\naggravates catastrophic forgetting. Moreover, as the rehearsal-based approach\nheavily relies on a few replayed nodes to retain knowledge obtained from\nprevious tasks, involving the replayed nodes that have irrelevant neighbors in\nthe model training may have a significant detrimental impact on model\nperformance. In this paper, we propose a GCL model named DSLR, specifically, we\ndevise a coverage-based diversity (CD) approach to consider both the class\nrepresentativeness and the diversity within each class of the replayed nodes.\nMoreover, we adopt graph structure learning (GSL) to ensure that the replayed\nnodes are connected to truly informative neighbors. Extensive experimental\nresults demonstrate the effectiveness and efficiency of DSLR. Our source code\nis available at https://github.com/seungyoon-Choi/DSLR_official.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13711v3",
    "authors": [
      "Seungyoon Choi",
      "Wonjoong Kim",
      "Sungwon Kim",
      "Yeonjun In",
      "Sein Kim",
      "Chanyoung Park"
    ]
  },
  {
    "id": "2402.13731",
    "title": "The Da Vinci Code of Large Pre-trained Language Models: Deciphering\n  Degenerate Knowledge Neurons",
    "abstract": "  This study explores the mechanism of factual knowledge storage in pre-trained\nlanguage models (PLMs). Previous research suggests that factual knowledge is\nstored within multi-layer perceptron weights, and some storage units exhibit\ndegeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper\nprovides a comprehensive definition of DKNs that covers both structural and\nfunctional aspects, pioneering the study of structures in PLMs' factual\nknowledge storage units. Based on this, we introduce the Neurological Topology\nClustering method, which allows the formation of DKNs in any numbers and\nstructures, leading to a more accurate DKN acquisition. Furthermore, we\nintroduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely\nintegrates model robustness, evolvability, and complexity for a holistic\nassessment of PLMs. Within this framework, our execution of 34 experiments\nacross 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs.\nThe code will be available soon.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13731v1",
    "authors": [
      "Yuheng Chen",
      "Pengfei Cao",
      "Yubo Chen",
      "Yining Wang",
      "Shengping Liu",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  {
    "id": "2402.13741",
    "title": "Unlocking Instructive In-Context Learning with Tabular Prompting for\n  Relational Triple Extraction",
    "abstract": "  The in-context learning (ICL) for relational triple extraction (RTE) has\nachieved promising performance, but still encounters two key challenges: (1)\nhow to design effective prompts and (2) how to select proper demonstrations.\nExisting methods, however, fail to address these challenges appropriately. On\nthe one hand, they usually recast RTE task to text-to-text prompting formats,\nwhich is unnatural and results in a mismatch between the output format at the\npre-training time and the inference time for large language models (LLMs). On\nthe other hand, they only utilize surface natural language features and lack\nconsideration of triple semantics in sample selection. These issues are\nblocking improved performance in ICL for RTE, thus we aim to tackle prompt\ndesigning and sample selection challenges simultaneously. To this end, we\ndevise a tabular prompting for RTE (\\textsc{TableIE}) which frames RTE task\ninto a table generation task to incorporate explicit structured information\ninto ICL, facilitating conversion of outputs to RTE structures. Then we propose\ninstructive in-context learning (I$^2$CL) which only selects and annotates a\nfew samples considering internal triple semantics in massive unlabeled samples.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13741v1",
    "authors": [
      "Guozheng Li",
      "Wenjun Ke",
      "Peng Wang",
      "Zijie Xu",
      "Ke Ji",
      "Jiajun Liu",
      "Ziyu Shang",
      "Qiqing Luo"
    ]
  },
  {
    "id": "2402.13752",
    "title": "AI-Powered Predictions for Electricity Load in Prosumer Communities",
    "abstract": "  The flexibility in electricity consumption and production in communities of\nresidential buildings, including those with renewable energy sources and energy\nstorage (a.k.a., prosumers), can effectively be utilized through the\nadvancement of short-term demand response mechanisms. It is known that\nflexibility can further be increased if demand response is performed at the\nlevel of communities of prosumers, since aggregated groups can better\ncoordinate electricity consumption. However, the effectiveness of such\nshort-term optimization is highly dependent on the accuracy of electricity load\nforecasts both for each building as well as for the whole community. Structural\nvariations in the electricity load profile can be associated with different\nexogenous factors, such as weather conditions, calendar information and day of\nthe week, as well as user behavior. In this paper, we review a wide range of\nelectricity load forecasting techniques, that can provide significant\nassistance in optimizing load consumption in prosumer communities. We present\nand test artificial intelligence (AI) powered short-term load forecasting\nmethodologies that operate with black-box time series models, such as\nFacebook's Prophet and Long Short-term Memory (LSTM) models; season-based\nSARIMA and smoothing Holt-Winters models; and empirical regression-based models\nthat utilize domain knowledge. The integration of weather forecasts into\ndata-driven time series forecasts is also tested. Results show that the\ncombination of persistent and regression terms (adapted to the load forecasting\ntask) achieves the best forecast accuracy.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13752v1",
    "authors": ["Aleksei Kychkin", "Georgios C. Chasparis"]
  },
  {
    "id": "2402.13764",
    "title": "CriticBench: Evaluating Large Language Models as Critic",
    "abstract": "  Critique ability are crucial in the scalable oversight and self-improvement\nof Large Language Models (LLMs). While many recent studies explore the critique\nability of LLMs to judge and refine flaws in generations, how to\ncomprehensively and reliably measure the critique abilities of LLMs is\nunder-explored. This paper introduces CriticBench, a novel benchmark designed\nto comprehensively and reliably evaluate four key critique ability dimensions\nof LLMs: feedback, comparison, refinement and meta-feedback. CriticBench\nencompasses nine diverse tasks, each assessing the LLMs' ability to critique\nresponses at varying levels of quality granularity. Our extensive evaluations\nof open-source and closed-source LLMs reveal intriguing relationships between\nthe critique ability and tasks, response qualities, and model scales. Datasets,\nresources and evaluation toolkit for CriticBench will be publicly released at\nhttps://github.com/open-compass/CriticBench.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13764v3",
    "authors": [
      "Tian Lan",
      "Wenwei Zhang",
      "Chen Xu",
      "Heyan Huang",
      "Dahua Lin",
      "Kai Chen",
      "Xian-ling Mao"
    ]
  },
  {
    "id": "2402.13777",
    "title": "Deep Generative Models for Offline Policy Learning: Tutorial, Survey,\n  and Perspectives on Future Directions",
    "abstract": "  Deep generative models (DGMs) have demonstrated great success across various\ndomains, particularly in generating texts, images, and videos using models\ntrained from offline data. Similarly, data-driven decision-making and robotic\ncontrol also necessitate learning a generator function from the offline data to\nserve as the strategy or policy. In this case, applying deep generative models\nin offline policy learning exhibits great potential, and numerous studies have\nexplored in this direction. However, this field still lacks a comprehensive\nreview and so developments of different branches are relatively independent.\nThus, we provide the first systematic review on the applications of deep\ngenerative models for offline policy learning. In particular, we cover five\nmainstream deep generative models, including Variational Auto-Encoders,\nGenerative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion\nModels, and their applications in both offline reinforcement learning (offline\nRL) and imitation learning (IL). Offline RL and IL are two main branches of\noffline policy learning and are widely-adopted techniques for sequential\ndecision-making. Specifically, for each type of DGM-based offline policy\nlearning, we distill its fundamental scheme, categorize related works based on\nthe usage of the DGM, and sort out the development process of algorithms in\nthat field. Subsequent to the main content, we provide in-depth discussions on\ndeep generative models and offline policy learning as a summary, based on which\nwe present our perspectives on future research directions. This work offers a\nhands-on reference for the research progress in deep generative models for\noffline policy learning, and aims to inspire improved DGM-based offline RL or\nIL algorithms. For convenience, we maintain a paper list on\nhttps://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13777v4",
    "authors": [
      "Jiayu Chen",
      "Bhargav Ganguly",
      "Yang Xu",
      "Yongsheng Mei",
      "Tian Lan",
      "Vaneet Aggarwal"
    ]
  },
  {
    "id": "2402.13840",
    "title": "LLM4SBR: A Lightweight and Effective Framework for Integrating Large\n  Language Models in Session-based Recommendation",
    "abstract": "  Traditional session-based recommendation (SBR) utilizes session behavior\nsequences from anonymous users for recommendation. Although this strategy is\nhighly efficient, it sacrifices the inherent semantic information of the items,\nmaking it difficult for the model to understand the true intent of the session\nand resulting in a lack of interpretability in the recommended results.\nRecently, large language models (LLMs) have flourished across various domains,\noffering a glimpse of hope in addressing the aforementioned challenges.\nInspired by the impact of LLMs, research exploring the integration of LLMs with\nthe Recommender system (RS) has surged like mushrooms after rain. However,\nconstrained by high time and space costs, as well as the brief and anonymous\nnature of session data, the first LLM recommendation framework suitable for\nindustrial deployment has yet to emerge in the field of SBR. To address the\naforementioned challenges, we have proposed the LLM Integration Framework for\nSBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR\nadopts a two-step strategy. Firstly, we transform session data into a bimodal\nform of text and behavior. In the first step, leveraging the inferential\ncapabilities of LLMs, we conduct inference on session text data from different\nperspectives and design the component for auxiliary enhancement. In the second\nstep, the SBR model is trained on behavior data, aligning and averaging two\nmodal session representations from different perspectives. Finally, we fuse\nsession representations from different perspectives and modalities as the\nultimate session representation for recommendation. We conducted experiments on\ntwo real-world datasets, and the results demonstrate that LLM4SBR significantly\nimproves the performance of traditional SBR models and is highly lightweight\nand efficient, making it suitable for industrial deployment.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13840v1",
    "authors": [
      "Shutong Qiao",
      "Chen Gao",
      "Junhao Wen",
      "Wei Zhou",
      "Qun Luo",
      "Peixuan Chen",
      "Yong Li"
    ]
  },
  {
    "id": "2402.13866",
    "title": "Kuaiji: the First Chinese Accounting Large Language Model",
    "abstract": "  Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated\nimpressive proficiency in comprehending and generating natural language.\nHowever, they encounter difficulties when tasked with adapting to specialized\ndomains such as accounting. To address this challenge, we introduce Kuaiji, a\ntailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned\nusing the Baichuan framework, which encompasses continuous pre-training and\nsupervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing\nlarge genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy\nand response speed. Our contributions encompass the creation of the first\nChinese accounting dataset, the establishment of Kuaiji as a leading\nopen-source Chinese accounting LLM, and the validation of its efficacy through\nreal-world accounting scenarios.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13866v2",
    "authors": [
      "Jiayuan Luo",
      "Songhua Yang",
      "Xiaoling Qiu",
      "Panyu Chen",
      "Yufei Nai",
      "Wenxuan Zeng",
      "Wentao Zhang",
      "Xinke Jiang"
    ]
  },
  {
    "id": "2402.13917",
    "title": "What Linguistic Features and Languages are Important in LLM Translation?",
    "abstract": "  Large Language Models (LLMs) demonstrate strong capability across multiple\ntasks, including machine translation. Our study focuses on evaluating Llama2's\nmachine translation capabilities and exploring how translation depends on\nlanguages in its training data. Our experiments show that the 7B Llama2 model\nyields above 10 BLEU score for all languages it has seen, but not always for\nlanguages it has not seen. Most gains for those unseen languages are observed\nthe most with the model scale compared to using chat versions or adding shot\ncount. Furthermore, our linguistic distance analysis reveals that syntactic\nsimilarity is not always the primary linguistic factor in determining\ntranslation quality. Interestingly, we discovered that under specific\ncircumstances, some languages, despite having significantly less training data\nthan English, exhibit strong correlations comparable to English. Our\ndiscoveries here give new perspectives for the current landscape of LLMs,\nraising the possibility that LLMs centered around languages other than English\nmay offer a more effective foundation for a multilingual model.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13917v1",
    "authors": [
      "Ryandito Diandaru",
      "Lucky Susanto",
      "Zilu Tang",
      "Ayu Purwarianti",
      "Derry Wijaya"
    ]
  },
  {
    "id": "2402.13919",
    "title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in\n  Clinical Summarization",
    "abstract": "  Large Language Models (LLMs) such as GPT and Llama have demonstrated\nsignificant achievements in summarization tasks but struggle with factual\ninaccuracies, a critical issue in clinical NLP applications where errors could\nlead to serious consequences. To counter the high costs and limited\navailability of expert-annotated data for factual alignment, this study\nintroduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate\nhigh-quality feedback aimed at enhancing factual consistency in clinical note\nsummarization. Our research primarily focuses on edit feedback, mirroring the\npractical scenario in which medical professionals refine AI system outputs\nwithout the need for additional annotations. Despite GPT's proven expertise in\nvarious clinical NLP tasks, such as the Medical Licensing Examination, there is\nscant research on its capacity to deliver expert-level edit feedback for\nimproving weaker LMs or LLMs generation quality. This work leverages GPT's\nadvanced capabilities in clinical NLP to offer expert-level edit feedback.\nThrough the use of two distinct alignment algorithms (DPO and SALT) based on\nGPT edit feedback, our goal is to reduce hallucinations and align closely with\nmedical facts, endeavoring to narrow the divide between AI-generated content\nand factual accuracy. This highlights the substantial potential of GPT edits in\nenhancing the alignment of clinical factuality.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13919v1",
    "authors": [
      "Prakamya Mishra",
      "Zonghai Yao",
      "Parth Vashisht",
      "Feiyun Ouyang",
      "Beining Wang",
      "Vidhi Dhaval Mody",
      "Hong Yu"
    ]
  },
  {
    "id": "2402.13926",
    "title": "Large Language Models are Vulnerable to Bait-and-Switch Attacks for\n  Generating Harmful Content",
    "abstract": "  The risks derived from large language models (LLMs) generating deceptive and\ndamaging content have been the subject of considerable research, but even safe\ngenerations can lead to problematic downstream impacts. In our study, we shift\nthe focus to how even safe text coming from LLMs can be easily turned into\npotentially dangerous content through Bait-and-Switch attacks. In such attacks,\nthe user first prompts LLMs with safe questions and then employs a simple\nfind-and-replace post-hoc technique to manipulate the outputs into harmful\nnarratives. The alarming efficacy of this approach in generating toxic content\nhighlights a significant challenge in developing reliable safety guardrails for\nLLMs. In particular, we stress that focusing on the safety of the verbatim LLM\noutputs is insufficient and that we also need to consider post-hoc\ntransformations.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13926v1",
    "authors": ["Federico Bianchi", "James Zou"]
  },
  {
    "id": "2402.13979",
    "title": "The Importance of Architecture Choice in Deep Learning for Climate\n  Applications",
    "abstract": "  Machine Learning has become a pervasive tool in climate science applications.\nHowever, current models fail to address nonstationarity induced by\nanthropogenic alterations in greenhouse emissions and do not routinely quantify\nthe uncertainty of proposed projections. In this paper, we model the Atlantic\nMeridional Overturning Circulation (AMOC) which is of major importance to\nclimate in Europe and the US East Coast by transporting warm water to these\nregions, and has the potential for abrupt collapse. We can generate arbitrarily\nextreme climate scenarios through arbitrary time scales which we then predict\nusing neural networks. Our analysis shows that the AMOC is predictable using\nneural networks under a diverse set of climate scenarios. Further experiments\nreveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead\nof imitating its progression through autocorrelation. With quantified\nuncertainty, an intriguing pattern of \"spikes\" before critical points of\ncollapse in the AMOC casts doubt on previous analyses that predicted an AMOC\ncollapse within this century. Our results show that Bayesian Neural Networks\nperform poorly compared to more dense architectures and care should be taken\nwhen applying neural networks to nonstationary scenarios such as climate\nprojections. Further, our results highlight that big NN models might have\ndifficulty in modeling global Earth System dynamics accurately and be\nsuccessfully applied in nonstationary climate scenarios due to the physics\nbeing challenging for neural networks to capture.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13979v1",
    "authors": ["Simon Dräger", "Maike Sonnewald"]
  },
  {
    "id": "2402.14007",
    "title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of\n  Text Watermark for Large Language Models",
    "abstract": "  Text watermarking technology aims to tag and identify content produced by\nlarge language models (LLMs) to prevent misuse. In this study, we introduce the\nconcept of ''cross-lingual consistency'' in text watermarking, which assesses\nthe ability of text watermarks to maintain their effectiveness after being\ntranslated into other languages. Preliminary empirical results from two LLMs\nand three watermarking methods reveal that current text watermarking\ntechnologies lack consistency when texts are translated into various languages.\nBased on this observation, we propose a Cross-lingual Watermark Removal Attack\n(CWRA) to bypass watermarking by first obtaining a response from an LLM in a\npivot language, which is then translated into the target language. CWRA can\neffectively remove watermarks by reducing the Area Under the Curve (AUC) from\n0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors\nthat contribute to the cross-lingual consistency in text watermarking and\npropose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14007v1",
    "authors": [
      "Zhiwei He",
      "Binglin Zhou",
      "Hongkun Hao",
      "Aiwei Liu",
      "Xing Wang",
      "Zhaopeng Tu",
      "Zhuosheng Zhang",
      "Rui Wang"
    ]
  },
  {
    "id": "2402.14033",
    "title": "VN Network: Embedding Newly Emerging Entities with Virtual Neighbors",
    "abstract": "  Embedding entities and relations into continuous vector spaces has attracted\na surge of interest in recent years. Most embedding methods assume that all\ntest entities are available during training, which makes it time-consuming to\nretrain embeddings for newly emerging entities. To address this issue, recent\nworks apply the graph neural network on the existing neighbors of the unseen\nentities. In this paper, we propose a novel framework, namely Virtual Neighbor\n(VN) network, to address three key challenges. Firstly, to reduce the neighbor\nsparsity problem, we introduce the concept of the virtual neighbors inferred by\nrules. And we assign soft labels to these neighbors by solving a\nrule-constrained problem, rather than simply regarding them as unquestionably\ntrue. Secondly, many existing methods only use one-hop or two-hop neighbors for\naggregation and ignore the distant information that may be helpful. Instead, we\nidentify both logic and symmetric path rules to capture complex patterns.\nFinally, instead of one-time injection of rules, we employ an iterative\nlearning scheme between the embedding method and virtual neighbor prediction to\ncapture the interactions within. Experimental results on two knowledge graph\ncompletion tasks demonstrate that our VN network significantly outperforms\nstate-of-the-art baselines. Furthermore, results on Subject/Object-R show that\nour proposed VN network is highly robust to the neighbor sparsity problem.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14033v1",
    "authors": ["Yongquan He", "Zihan Wang", "Peng Zhang", "Zhaopeng Tu", "Zhaochun Ren"]
  },
  {
    "id": "2402.14034",
    "title": "AgentScope: A Flexible yet Robust Multi-Agent Platform",
    "abstract": "  With the rapid advancement of Large Language Models (LLMs), significant\nprogress has been made in multi-agent applications. However, the complexities\nin coordinating agents' cooperation and LLMs' erratic performance pose notable\nchallenges in developing robust and efficient multi-agent applications. To\ntackle these challenges, we propose AgentScope, a developer-centric multi-agent\nplatform with message exchange as its core communication mechanism. Together\nwith abundant syntactic tools, built-in resources, and user-friendly\ninteractions, our communication mechanism significantly reduces the barriers to\nboth development and understanding. Towards robust and flexible multi-agent\napplication, AgentScope provides both built-in and customizable fault tolerance\nmechanisms while it is also armed with system-level supports for multi-modal\ndata generation, storage and transmission. Additionally, we design an\nactor-based distribution framework, enabling easy conversion between local and\ndistributed deployments and automatic parallel optimization without extra\neffort. With these features, AgentScope empowers developers to build\napplications that fully realize the potential of intelligent agents. We have\nreleased AgentScope at https://github.com/modelscope/agentscope, and hope\nAgentScope invites wider participation and innovation in this fast-moving\nfield.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14034v1",
    "authors": [
      "Dawei Gao",
      "Zitao Li",
      "Weirui Kuang",
      "Xuchen Pan",
      "Daoyuan Chen",
      "Zhijian Ma",
      "Bingchen Qian",
      "Liuyi Yao",
      "Lin Zhu",
      "Chen Cheng",
      "Hongzhu Shi",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ]
  },
  {
    "id": "2402.14035",
    "title": "Wisdom of Committee: Distilling from Foundation Model to Specialized\n  Application Model",
    "abstract": "  Recent advancements in foundation models have yielded impressive performance\nacross a wide range of tasks. Meanwhile, for specific applications,\npractitioners have been developing specialized application models. To enjoy the\nbenefits of both kinds of models, one natural path is to transfer the knowledge\nin foundation models into specialized application models, which are generally\nmore efficient for serving. Techniques from knowledge distillation may be\napplied here, where the application model learns to mimic the foundation model.\nHowever, specialized application models and foundation models have substantial\ngaps in capacity, employing distinct architectures, using different input\nfeatures from different modalities, and being optimized on different\ndistributions. These differences in model characteristics lead to significant\nchallenges for distillation methods. In this work, we propose creating a\nteaching committee comprising both foundation model teachers and complementary\nteachers. Complementary teachers possess model characteristics akin to the\nstudent's, aiming to bridge the gap between the foundation model and\nspecialized application models for a smoother knowledge transfer. Further, to\naccommodate the dissimilarity among the teachers in the committee, we introduce\nDiverseDistill, which allows the student to understand the expertise of each\nteacher and extract task knowledge. Our evaluations demonstrate that adding\ncomplementary teachers enhances student performance. Finally, DiverseDistill\nconsistently outperforms baseline distillation methods, regardless of the\nteacher choices, resulting in significantly improved student performance.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14035v2",
    "authors": [
      "Zichang Liu",
      "Qingyun Liu",
      "Yuening Li",
      "Liang Liu",
      "Anshumali Shrivastava",
      "Shuchao Bi",
      "Lichan Hong",
      "Ed H. Chi",
      "Zhe Zhao"
    ]
  },
  {
    "id": "2402.14037",
    "title": "An Effective Networks Intrusion Detection Approach Based on Hybrid\n  Harris Hawks and Multi-Layer Perceptron",
    "abstract": "  This paper proposes an Intrusion Detection System (IDS) employing the Harris\nHawks Optimization algorithm (HHO) to optimize Multilayer Perceptron learning\nby optimizing bias and weight parameters. HHO-MLP aims to select optimal\nparameters in its learning process to minimize intrusion detection errors in\nnetworks. HHO-MLP has been implemented using EvoloPy NN framework, an\nopen-source Python tool specialized for training MLPs using evolutionary\nalgorithms. For purposes of comparing the HHO model against other evolutionary\nmethodologies currently available, specificity and sensitivity measures,\naccuracy measures, and mse and rmse measures have been calculated using KDD\ndatasets. Experiments have demonstrated the HHO MLP method is effective at\nidentifying malicious patterns. HHO-MLP has been tested against evolutionary\nalgorithms like Butterfly Optimization Algorithm (BOA), Grasshopper\nOptimization Algorithms (GOA), and Black Widow Optimizations (BOW), with\nvalidation by Random Forest (RF), XG-Boost. HHO-MLP showed superior performance\nby attaining top scores with accuracy rate of 93.17%, sensitivity level of\n89.25%, and specificity percentage of 95.41%.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14037v1",
    "authors": [
      "Moutaz Alazab",
      "Ruba Abu Khurma",
      "Pedro A. Castillo",
      "Bilal Abu-Salih",
      "Alejandro Martin",
      "David Camacho"
    ]
  },
  {
    "id": "2402.14039",
    "title": "Specialty detection in the context of telemedicine in a highly\n  imbalanced multi-class distribution",
    "abstract": "  The Covid-19 pandemic has led to an increase in the awareness of and demand\nfor telemedicine services, resulting in a need for automating the process and\nrelying on machine learning (ML) to reduce the operational load. This research\nproposes a specialty detection classifier based on a machine learning model to\nautomate the process of detecting the correct specialty for each question and\nrouting it to the correct doctor. The study focuses on handling multiclass and\nhighly imbalanced datasets for Arabic medical questions, comparing some\noversampling techniques, developing a Deep Neural Network (DNN) model for\nspecialty detection, and exploring the hidden business areas that rely on\nspecialty detection such as customizing and personalizing the consultation flow\nfor different specialties. The proposed module is deployed in both synchronous\nand asynchronous medical consultations to provide more real-time\nclassification, minimize the doctor effort in addressing the correct specialty,\nand give the system more flexibility in customizing the medical consultation\nflow. The evaluation and assessment are based on accuracy, precision, recall,\nand F1-score. The experimental results suggest that combining multiple\ntechniques, such as SMOTE and reweighing with keyword identification, is\nnecessary to achieve improved performance in detecting rare classes in\nimbalanced multiclass datasets. By using these techniques, specialty detection\nmodels can more accurately detect rare classes in real-world scenarios where\nimbalanced data is common.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14039v1",
    "authors": ["Alaa Alomari", "Hossam Faris", "Pedro A. Castillo"]
  },
  {
    "id": "2402.14044",
    "title": "A new approach for solving global optimization and engineering problems\n  based on modified Sea Horse Optimizer",
    "abstract": "  Sea Horse Optimizer (SHO) is a noteworthy metaheuristic algorithm that\nemulates various intelligent behaviors exhibited by sea horses, encompassing\nfeeding patterns, male reproductive strategies, and intricate movement\npatterns. To mimic the nuanced locomotion of sea horses, SHO integrates the\nlogarithmic helical equation and Levy flight, effectively incorporating both\nrandom movements with substantial step sizes and refined local exploitation.\nAdditionally, the utilization of Brownian motion facilitates a more\ncomprehensive exploration of the search space. This study introduces a robust\nand high-performance variant of the SHO algorithm named mSHO. The enhancement\nprimarily focuses on bolstering SHO's exploitation capabilities by replacing\nits original method with an innovative local search strategy encompassing three\ndistinct steps: a neighborhood-based local search, a global non-neighbor-based\nsearch, and a method involving circumnavigation of the existing search region.\nThese techniques improve mSHO algorithm's search capabilities, allowing it to\nnavigate the search space and converge toward optimal solutions efficiently.\nThe comprehensive results distinctly establish the supremacy and efficiency of\nthe mSHO method as an exemplary tool for tackling an array of optimization\nquandaries. The results show that the proposed mSHO algorithm has a total rank\nof 1 for CEC'2020 test functions. In contrast, the mSHO achieved the best value\nfor the engineering problems, recording a value of 0.012665, 2993.634, 0.01266,\n1.724967, 263.8915, 0.032255, 58507.14, 1.339956, and 0.23524 for the pressure\nvessel design, speed reducer design, tension/compression spring, welded beam\ndesign, three-bar truss engineering design, industrial refrigeration system,\nmulti-Product batch plant, cantilever beam problem, multiple disc clutch brake\nproblems, respectively.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14044v1",
    "authors": [
      "Fatma A. Hashim",
      "Reham R. Mostafa",
      "Ruba Abu Khurma",
      "Raneem Qaddoura",
      "P. A. Castillo"
    ]
  },
  {
    "id": "2402.14047",
    "title": "Simple and Effective Transfer Learning for Neuro-Symbolic Integration",
    "abstract": "  Deep Learning (DL) techniques have achieved remarkable successes in recent\nyears. However, their ability to generalize and execute reasoning tasks remains\na challenge. A potential solution to this issue is Neuro-Symbolic Integration\n(NeSy), where neural approaches are combined with symbolic reasoning. Most of\nthese methods exploit a neural network to map perceptions to symbols and a\nlogical reasoner to predict the output of the downstream task. These methods\nexhibit superior generalization capacity compared to fully neural\narchitectures. However, they suffer from several issues, including slow\nconvergence, learning difficulties with complex perception tasks, and\nconvergence to local minima. This paper proposes a simple yet effective method\nto ameliorate these problems. The key idea involves pretraining a neural model\non the downstream task. Then, a NeSy model is trained on the same task via\ntransfer learning, where the weights of the perceptual part are injected from\nthe pretrained network. The key observation of our work is that the neural\nnetwork fails to generalize only at the level of the symbolic part while being\nperfectly capable of learning the mapping from perceptions to symbols. We have\ntested our training strategy on various SOTA NeSy methods and datasets,\ndemonstrating consistent improvements in the aforementioned problems.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14047v1",
    "authors": ["Alessandro Daniele", "Tommaso Campari", "Sagar Malhotra", "Luciano Serafini"]
  },
  {
    "id": "2402.14048",
    "title": "PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial\n  Optimization",
    "abstract": "  Reinforcement learning-based methods for constructing solutions to\ncombinatorial optimization problems are rapidly approaching the performance of\nhuman-designed algorithms. To further narrow the gap, learning-based approaches\nmust efficiently explore the solution space during the search process. Recent\napproaches artificially increase exploration by enforcing diverse solution\ngeneration through handcrafted rules, however, these rules can impair solution\nquality and are difficult to design for more complex problems. In this paper,\nwe introduce PolyNet, an approach for improving exploration of the solution\nspace by learning complementary solution strategies. In contrast to other\nworks, PolyNet uses only a single-decoder and a training schema that does not\nenforce diverse solution generation through handcrafted rules. We evaluate\nPolyNet on four combinatorial optimization problems and observe that the\nimplicit diversity mechanism allows PolyNet to find better solutions than\napproaches the explicitly enforce diverse solution generation.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14048v1",
    "authors": ["André Hottung", "Mridul Mahajan", "Kevin Tierney"]
  },
  {
    "id": "2402.14116",
    "title": "FanOutQA: Multi-Hop, Multi-Document Question Answering for Large\n  Language Models",
    "abstract": "  One type of question that is commonly found in day-to-day scenarios is\n``fan-out'' questions, complex multi-hop, multi-document reasoning questions\nthat require finding information about a large number of entities. However,\nthere exist few resources to evaluate this type of question-answering\ncapability among large language models. To evaluate complex reasoning in LLMs\nmore fully, we present FanOutQA, a high-quality dataset of fan-out\nquestion-answer pairs and human-annotated decompositions with English Wikipedia\nas the knowledge base. We formulate three benchmark settings across our dataset\nand benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B,\nfinding that contemporary models still have room to improve reasoning over\ninter-document dependencies in a long context. We provide our dataset and\nopen-source tools to run models to encourage evaluation at https://fanoutqa.com\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14116v1",
    "authors": ["Andrew Zhu", "Alyssa Hwang", "Liam Dugan", "Chris Callison-Burch"]
  },
  {
    "id": "2402.14118",
    "title": "Masked Matrix Multiplication for Emergent Sparsity",
    "abstract": "  Artificial intelligence workloads, especially transformer models, exhibit\nemergent sparsity in which computations perform selective sparse access to\ndense data. The workloads are inefficient on hardware designed for dense\ncomputations and do not map well onto sparse data representations. We build a\nvectorized and parallel matrix-multiplication system A X B = C that eliminates\nunnecessary computations and avoids branches based on a runtime evaluation of\nsparsity. We use a combination of dynamic code lookup to adapt to the specific\nsparsity encoded in the B matrix and preprocessing of sparsity maps of the A\nand B matrices to compute conditional branches once for the whole computation.\nFor a wide range of sparsity, from 60% to 95% zeros, our implementation\nperforms fewer instructions and increases performance when compared with Intel\nMKL's dense or sparse matrix multiply routines. Benefits can be as large as 2\ntimes speedup and 4 times fewer instructions.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14118v1",
    "authors": ["Brian Wheatman", "Meghana Madhyastha", "Randal Burns"]
  },
  {
    "id": "2402.14143",
    "title": "SecurePose: Automated Face Blurring and Human Movement Kinematics\n  Extraction from Videos Recorded in Clinical Settings",
    "abstract": "  Movement disorders are typically diagnosed by consensus-based expert\nevaluation of clinically acquired patient videos. However, such broad sharing\nof patient videos poses risks to patient privacy. Face blurring can be used to\nde-identify videos, but this process is often manual and time-consuming.\nAvailable automated face blurring techniques are subject to either excessive,\ninconsistent, or insufficient facial blurring - all of which can be disastrous\nfor video assessment and patient privacy. Furthermore, assessing movement\ndisorders in these videos is often subjective. The extraction of quantifiable\nkinematic features can help inform movement disorder assessment in these\nvideos, but existing methods to do this are prone to errors if using\npre-blurred videos. We have developed an open-source software called SecurePose\nthat can both achieve reliable face blurring and automated kinematic extraction\nin patient videos recorded in a clinic setting using an iPad. SecurePose,\nextracts kinematics using a pose estimation method (OpenPose), tracks and\nuniquely identifies all individuals in the video, identifies the patient, and\nperforms face blurring. The software was validated on gait videos recorded in\noutpatient clinic visits of 116 children with cerebral palsy. The validation\ninvolved assessing intermediate steps of kinematics extraction and face\nblurring with manual blurring (ground truth). Moreover, when SecurePose was\ncompared with six selected existing methods, it outperformed other methods in\nautomated face detection and achieved ceiling accuracy in 91.08% less time than\na robust manual face blurring method. Furthermore, ten experienced researchers\nfound SecurePose easy to learn and use, as evidenced by the System Usability\nScale. The results of this work validated the performance and usability of\nSecurePose on clinically recorded gait videos for face blurring and kinematics\nextraction.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14143v1",
    "authors": ["Rishabh Bajpai", "Bhooma Aravamuthan"]
  },
  {
    "id": "2402.14147",
    "title": "Wikibench: Community-Driven Data Curation for AI Evaluation on Wikipedia",
    "abstract": "  AI tools are increasingly deployed in community contexts. However, datasets\nused to evaluate AI are typically created by developers and annotators outside\na given community, which can yield misleading conclusions about AI performance.\nHow might we empower communities to drive the intentional design and curation\nof evaluation datasets for AI that impacts them? We investigate this question\non Wikipedia, an online community with multiple AI-based content moderation\ntools deployed. We introduce Wikibench, a system that enables communities to\ncollaboratively curate AI evaluation datasets, while navigating ambiguities and\ndifferences in perspective through discussion. A field study on Wikipedia shows\nthat datasets curated using Wikibench can effectively capture community\nconsensus, disagreement, and uncertainty. Furthermore, study participants used\nWikibench to shape the overall data curation process, including refining label\ndefinitions, determining data inclusion criteria, and authoring data\nstatements. Based on our findings, we propose future directions for systems\nthat support community-driven data curation.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14147v1",
    "authors": [
      "Tzu-Sheng Kuo",
      "Aaron Halfaker",
      "Zirui Cheng",
      "Jiwoo Kim",
      "Meng-Hsin Wu",
      "Tongshuang Wu",
      "Kenneth Holstein",
      "Haiyi Zhu"
    ]
  },
  {
    "id": "2402.14155",
    "title": "Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for\n  Intent Recognition?",
    "abstract": "  Task-oriented dialogue systems are expected to handle a constantly expanding\nset of intents and domains even after they have been deployed to support more\nand more functionalities. To live up to this expectation, it becomes critical\nto mitigate the catastrophic forgetting problem (CF) that occurs in continual\nlearning (CL) settings for a task such as intent recognition. While existing\ndialogue systems research has explored replay-based and regularization-based\nmethods to this end, the effect of domain ordering on the CL performance of\nintent recognition models remains unexplored. If understood well, domain\nordering has the potential to be an orthogonal technique that can be leveraged\nalongside existing techniques such as experience replay. Our work fills this\ngap by comparing the impact of three domain-ordering strategies (min-sum path,\nmax-sum path, random) on the CL performance of a generative intent recognition\nmodel. Our findings reveal that the min-sum path strategy outperforms the\nothers in reducing catastrophic forgetting when training on the 220M T5-Base\nmodel. However, this advantage diminishes with the larger 770M T5-Large model.\nThese results underscores the potential of domain ordering as a complementary\nstrategy for mitigating catastrophic forgetting in continually learning intent\nrecognition models, particularly in resource-constrained scenarios.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14155v1",
    "authors": ["Amogh Mannekote", "Xiaoyi Tian", "Kristy Elizabeth Boyer", "Bonnie J. Dorr"]
  },
  {
    "id": "2402.14160",
    "title": "Recursive Speculative Decoding: Accelerating LLM Inference via Sampling\n  Without Replacement",
    "abstract": "  Speculative decoding is an inference-acceleration method for large language\nmodels (LLMs) where a small language model generates a draft-token sequence\nwhich is further verified by the target LLM in parallel. Recent works have\nadvanced this method by establishing a draft-token tree, achieving superior\nperformance over a single-sequence speculative decoding. However, those works\nindependently generate tokens at each level of the tree, not leveraging the\ntree's entire diversifiability. Besides, their empirical superiority has been\nshown for fixed length of sequences, implicitly granting more computational\nresource to LLM for the tree-based methods. None of the existing works has\nconducted empirical studies with fixed target computational budgets despite its\nimportance to resource-bounded devices. We present Recursive Speculative\nDecoding (RSD), a novel tree-based method that samples draft tokens without\nreplacement and maximizes the diversity of the tree. During RSD's drafting, the\ntree is built by either Gumbel-Top-$k$ trick that draws tokens without\nreplacement in parallel or Stochastic Beam Search that samples sequences\nwithout replacement while early-truncating unlikely draft sequences and\nreducing the computational cost of LLM. We empirically evaluate RSD with Llama\n2 and OPT models, showing that RSD outperforms the baseline methods,\nconsistently for fixed draft sequence length and in most cases for fixed\ncomputational budgets at LLM.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14160v1",
    "authors": [
      "Wonseok Jeon",
      "Mukul Gagrani",
      "Raghavv Goel",
      "Junyoung Park",
      "Mingu Lee",
      "Christopher Lott"
    ]
  },
  {
    "id": "2402.14162",
    "title": "On Large Visual Language Models for Medical Imaging Analysis: An\n  Empirical Study",
    "abstract": "  Recently, large language models (LLMs) have taken the spotlight in natural\nlanguage processing. Further, integrating LLMs with vision enables the users to\nexplore emergent abilities with multimodal data. Visual language models (VLMs),\nsuch as LLaVA, Flamingo, or CLIP, have demonstrated impressive performance on\nvarious visio-linguistic tasks. Consequently, there are enormous applications\nof large models that could be potentially used in the biomedical imaging field.\nAlong that direction, there is a lack of related work to show the ability of\nlarge models to diagnose the diseases. In this work, we study the zero-shot and\nfew-shot robustness of VLMs on the medical imaging analysis tasks. Our\ncomprehensive experiments demonstrate the effectiveness of VLMs in analyzing\nbiomedical images such as brain MRIs, microscopic images of blood cells, and\nchest X-rays.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14162v1",
    "authors": ["Minh-Hao Van", "Prateek Verma", "Xintao Wu"]
  },
  {
    "id": "2402.14179",
    "title": "Bangla AI: A Framework for Machine Translation Utilizing Large Language\n  Models for Ethnic Media",
    "abstract": "  Ethnic media, which caters to diaspora communities in host nations, serves as\na vital platform for these communities to both produce content and access\ninformation. Rather than utilizing the language of the host nation, ethnic\nmedia delivers news in the language of the immigrant community. For instance,\nin the USA, Bangla ethnic media presents news in Bangla rather than English.\nThis research delves into the prospective integration of large language models\n(LLM) and multi-lingual machine translations (MMT) within the ethnic media\nindustry. It centers on the transformative potential of using LLM in MMT in\nvarious facets of news translation, searching, and categorization. The paper\noutlines a theoretical framework elucidating the integration of LLM and MMT\ninto the news searching and translation processes for ethnic media.\nAdditionally, it briefly addresses the potential ethical challenges associated\nwith the incorporation of LLM and MMT in news translation procedures.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14179v1",
    "authors": ["MD Ashraful Goni", "Fahad Mostafa", "Kerk F. Kee"]
  },
  {
    "id": "2402.14873",
    "title": "Technical Report on the Checkfor.ai AI-Generated Text Classifier",
    "abstract": "  We present the CheckforAI text classifier, a transformer-based neural network\ntrained to distinguish text written by large language models from text written\nby humans. CheckforAI outperforms zero-shot methods such as DetectGPT as well\nas leading commercial AI detection tools with over 9 times lower error rates on\na comprehensive benchmark comprised of ten text domains (student writing,\ncreative writing, scientific writing, books, encyclopedias, news, email,\nscientific papers, short-form Q&A) and 8 open- and closed-source large language\nmodels. We propose a training algorithm, hard negative mining with synthetic\nmirrors, that enables our classifier to achieve orders of magnitude lower false\npositive rates on high-data domains such as reviews. Finally, we show that\nCheckforAI is not biased against nonnative English speakers and generalizes to\ndomains and models unseen during training.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14873v2",
    "authors": ["Bradley Emi", "Max Spero"]
  },
  {
    "id": "2402.14879",
    "title": "Driving Generative Agents With Their Personality",
    "abstract": "  This research explores the potential of Large Language Models (LLMs) to\nutilize psychometric values, specifically personality information, within the\ncontext of video game character development. Affective Computing (AC) systems\nquantify a Non-Player character's (NPC) psyche, and an LLM can take advantage\nof the system's information by using the values for prompt generation. The\nresearch shows an LLM can consistently represent a given personality profile,\nthereby enhancing the human-like characteristics of game characters.\nRepurposing a human examination, the International Personality Item Pool (IPIP)\nquestionnaire, to evaluate an LLM shows that the model can accurately generate\ncontent concerning the personality provided. Results show that the improvement\nof LLM, such as the latest GPT-4 model, can consistently utilize and interpret\na personality to represent behavior.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14879v1",
    "authors": ["Lawrence J. Klinkert", "Stephanie Buongiorno", "Corey Clark"]
  },
  {
    "id": "2402.16733",
    "title": "DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing",
    "abstract": "  Automated essay scoring (AES) is a useful tool in English as a Foreign\nLanguage (EFL) writing education, offering real-time essay scores for students\nand instructors. However, previous AES models were trained on essays and scores\nirrelevant to the practical scenarios of EFL writing education and usually\nprovided a single holistic score due to the lack of appropriate datasets. In\nthis paper, we release DREsS, a large-scale, standard dataset for rubric-based\nautomated essay scoring. DREsS comprises three sub-datasets: DREsS_New,\nDREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with\n1.7K essays authored by EFL undergraduate students and scored by English\neducation experts. We also standardize existing rubric-based essay scoring\ndatasets as DREsS_Std. We suggest CASE, a corruption-based augmentation\nstrategy for essays, which generates 20K synthetic samples of DREsS_CASE and\nimproves the baseline results by 45.44%. DREsS will enable further research to\nprovide a more accurate and practical AES system for EFL writing education.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.16733v1",
    "authors": ["Haneul Yoo", "Jieun Han", "So-Yeon Ahn", "Alice Oh"]
  },
  {
    "id": "2402.19237",
    "title": "Context-based Interpretable Spatio-Temporal Graph Convolutional Network\n  for Human Motion Forecasting",
    "abstract": "  Human motion prediction is still an open problem extremely important for\nautonomous driving and safety applications. Due to the complex spatiotemporal\nrelation of motion sequences, this remains a challenging problem not only for\nmovement prediction but also to perform a preliminary interpretation of the\njoint connections. In this work, we present a Context-based Interpretable\nSpatio-Temporal Graph Convolutional Network (CIST-GCN), as an efficient 3D\nhuman pose forecasting model based on GCNs that encompasses specific layers,\naiding model interpretability and providing information that might be useful\nwhen analyzing motion distribution and body behavior. Our architecture extracts\nmeaningful information from pose sequences, aggregates displacements and\naccelerations into the input model, and finally predicts the output\ndisplacements. Extensive experiments on Human 3.6M, AMASS, 3DPW, and ExPI\ndatasets demonstrate that CIST-GCN outperforms previous methods in human motion\nprediction and robustness. Since the idea of enhancing interpretability for\nmotion prediction has its merits, we showcase experiments towards it and\nprovide preliminary evaluations of such insights here. available code:\nhttps://github.com/QualityMinds/cistgcn\n",
    "pdfLink": "http://arxiv.org/pdf/2402.19237v1",
    "authors": ["Edgar Medina", "Leyong Loh", "Namrata Gurung", "Kyung Hun Oh", "Niels Heller"]
  },
  {
    "id": "2402.13448",
    "title": "ED-Copilot: Reduce Emergency Department Wait Time with Language Model\n  Diagnostic Assistance",
    "abstract": "  In the emergency department (ED), patients undergo triage and multiple\nlaboratory tests before diagnosis. This process is time-consuming, and causes\nED crowding which significantly impacts patient mortality, medical errors,\nstaff burnout, etc. This work proposes (time) cost-effective diagnostic\nassistance that explores the potential of artificial intelligence (AI) systems\nin assisting ED clinicians to make time-efficient and accurate diagnoses. Using\npublicly available patient data, we collaborate with ED clinicians to curate\nMIMIC-ED-Assist, a benchmark that measures the ability of AI systems in\nsuggesting laboratory tests that minimize ED wait times, while correctly\npredicting critical outcomes such as death. We develop ED-Copilot which\nsequentially suggests patient-specific laboratory tests and makes diagnostic\npredictions. ED-Copilot uses a pre-trained bio-medical language model to encode\npatient information and reinforcement learning to minimize ED wait time and\nmaximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist,\nED-Copilot improves prediction accuracy over baselines while halving average\nwait time from four hours to two hours. Ablation studies demonstrate the\nimportance of model scale and use of a bio-medical language model. Further\nanalyses reveal the necessity of personalized laboratory test suggestions for\ndiagnosing patients with severe cases, as well as the potential of ED-Copilot\nin providing ED clinicians with informative laboratory test recommendations.\nOur code is available at https://github.com/cxcscmu/ED-Copilot.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13448v1",
    "authors": ["Liwen Sun", "Abhineet Agarwal", "Aaron Kornblith", "Bin Yu", "Chenyan Xiong"]
  },
  {
    "id": "2402.13482",
    "title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks",
    "abstract": "  Despite large successes of recent language models on diverse tasks, they\nsuffer from severe performance degeneration in low-resource settings with\nlimited training data available. Many existing works tackle this problem by\ngenerating synthetic data from the training data and then training models on\nthem, recently using Large Language Models (LLMs). However, in low-resource\nsettings, the amount of seed data samples to use for data augmentation is very\nsmall, which makes generated samples suboptimal and less diverse. To tackle\nthis challenge, we propose a novel method that augments training data by\nincorporating a wealth of examples from other datasets, along with the given\ntraining data. Specifically, we first retrieve the relevant instances from\nother datasets, such as their input-output pairs or contexts, based on their\nsimilarities with the given seed data, and then prompt LLMs to generate new\nsamples with the contextual information within and across the original and\nretrieved samples. This approach can ensure that the generated data is not only\nrelevant but also more diverse than what could be achieved using the limited\nseed data alone. We validate our proposed Retrieval-Augmented Data Augmentation\n(RADA) framework on multiple datasets under low-resource settings of training\nand test-time data augmentation scenarios, on which it outperforms existing\nLLM-powered data augmentation baselines.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13482v1",
    "authors": ["Minju Seo", "Jinheon Baek", "James Thorne", "Sung Ju Hwang"]
  },
  {
    "id": "2402.13512",
    "title": "From Self-Attention to Markov Models: Unveiling the Dynamics of\n  Generative Transformers",
    "abstract": "  Modern language models rely on the transformer architecture and attention\nmechanism to perform language understanding and text generation. In this work,\nwe study learning a 1-layer self-attention model from a set of prompts and\nassociated output data sampled from the model. We first establish a precise\nmapping between the self-attention mechanism and Markov models: Inputting a\nprompt to the model samples the output token according to a context-conditioned\nMarkov chain (CCMC) which weights the transition matrix of a base Markov chain.\nAdditionally, incorporating positional encoding results in position-dependent\nscaling of the transition probabilities. Building on this formalism, we develop\nidentifiability/coverage conditions for the prompt distribution that guarantee\nconsistent estimation and establish sample complexity guarantees under IID\nsamples. Finally, we study the problem of learning from a single output\ntrajectory generated from an initial prompt. We characterize an intriguing\nwinner-takes-all phenomenon where the generative process implemented by\nself-attention collapses into sampling a limited subset of tokens due to its\nnon-mixing nature. This provides a mathematical explanation to the tendency of\nmodern LLMs to generate repetitive text. In summary, the equivalence to CCMC\nprovides a simple but powerful framework to study self-attention and its\nproperties.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13512v1",
    "authors": [
      "M. Emrullah Ildiz",
      "Yixiao Huang",
      "Yingcong Li",
      "Ankit Singh Rawat",
      "Samet Oymak"
    ]
  },
  {
    "id": "2402.13516",
    "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models",
    "abstract": "  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, it has been proven a\npromising paradigm to boost model inference efficiency. Nevertheless, most\nlarge language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces an effective sparsification method named \"ProSparse\" to push\nLLMs for higher activation sparsity without decreasing model performance.\nSpecifically, after substituting the activation function of LLMs with ReLU,\nProSparse adopts progressive sparsity regularization with a factor smoothly\nincreasing along sine curves in multiple stages. This can enhance activation\nsparsity and alleviate performance degradation by avoiding radical shifts in\nactivation distribution. With ProSparse, we obtain high sparsity of 89.32% and\n88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable\nperformance to their original Swish-activated versions. Our inference\nacceleration experiments further demonstrate the practical acceleration brought\nby higher activation sparsity.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13516v2",
    "authors": [
      "Chenyang Song",
      "Xu Han",
      "Zhengyan Zhang",
      "Shengding Hu",
      "Xiyu Shi",
      "Kuai Li",
      "Chen Chen",
      "Zhiyuan Liu",
      "Guangli Li",
      "Tao Yang",
      "Maosong Sun"
    ]
  },
  {
    "id": "2402.13533",
    "title": "FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models\n  for Financial Applications with High-Performance Computing",
    "abstract": "  Large language models (LLMs) are computationally intensive. The computation\nworkload and the memory footprint grow quadratically with the dimension (layer\nwidth). Most of LLMs' parameters come from the linear layers of the transformer\nstructure and are highly redundant. These linear layers contribute more than\n80% of the computation workload and 99% of the model size. To pretrain and\nfinetune LLMs efficiently, there are three major challenges to address: 1)\nreducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3)\nimproving GPU utilization when using distributed training. Prior methods, such\nas LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the\nnumber of trainable parameters and model size, respectively. However, the\nresulting model still consumes a large amount of GPU memory. In this paper, we\npresent high-performance GPU-based methods that exploit low-rank structures to\npretrain and finetune LLMs for financial applications. We replace one\nconventional linear layer of the transformer structure with two narrower linear\nlayers, which allows us to reduce the number of parameters by several orders of\nmagnitude. By quantizing the parameters into low precision (8-bit and 4-bit),\nthe memory consumption of the resulting model is further reduced. Compared with\nexisting LLMs, our methods achieve a speedup of 1.3X and a model compression\nratio of 2.64X for pretaining without accuracy drop. For finetuning, our\nmethods achieve an average accuracy increase of 6.3% and 24.0% in general tasks\nand financial tasks, respectively, and GPU memory consumption ratio of 6.3X.\nThe sizes of our models are smaller than 0.59 GB, allowing inference on a\nsmartphone.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13533v1",
    "authors": ["Xiao-Yang Liu", "Jie Zhang", "Guoxuan Wang", "Weiqing Tong", "Anwar Walid"]
  },
  {
    "id": "2402.13542",
    "title": "ARL2: Aligning Retrievers for Black-box Large Language Models via\n  Self-guided Adaptive Relevance Labeling",
    "abstract": "  Retrieval-augmented generation enhances large language models (LLMs) by\nincorporating relevant information from external knowledge sources. This\nenables LLMs to adapt to specific domains and mitigate hallucinations in\nknowledge-intensive tasks. However, existing retrievers are often misaligned\nwith LLMs due to their separate training processes and the black-box nature of\nLLMs. To address this challenge, we propose ARL2, a retriever learning\ntechnique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and\nscore relevant evidence, enabling learning the retriever from robust LLM\nsupervision. Furthermore, ARL2 uses an adaptive self-training strategy for\ncurating high-quality and diverse relevance data, which can effectively reduce\nthe annotation cost. Extensive experiments demonstrate the effectiveness of\nARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared\nto the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer\nlearning capabilities and strong zero-shot generalization abilities. Our code\nwill be published at \\url{https://github.com/zhanglingxi-cs/ARL2}.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13542v1",
    "authors": ["Lingxi Zhang", "Yue Yu", "Kuan Wang", "Chao Zhang"]
  },
  {
    "id": "2402.13567",
    "title": "Spot Check Equivalence: an Interpretable Metric for Information\n  Elicitation Mechanisms",
    "abstract": "  Because high-quality data is like oxygen for AI systems, effectively\neliciting information from crowdsourcing workers has become a first-order\nproblem for developing high-performance machine learning algorithms. Two\nprevalent paradigms, spot-checking and peer prediction, enable the design of\nmechanisms to evaluate and incentivize high-quality data from human labelers.\nSo far, at least three metrics have been proposed to compare the performances\nof these techniques [33, 8, 3]. However, different metrics lead to divergent\nand even contradictory results in various contexts. In this paper, we harmonize\nthese divergent stories, showing that two of these metrics are actually the\nsame within certain contexts and explain the divergence of the third. Moreover,\nwe unify these different contexts by introducing \\textit{Spot Check\nEquivalence}, which offers an interpretable metric for the effectiveness of a\npeer prediction mechanism. Finally, we present two approaches to compute spot\ncheck equivalence in various contexts, where simulation results verify the\neffectiveness of our proposed metric.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13567v1",
    "authors": ["Shengwei Xu", "Yichi Zhang", "Paul Resnick", "Grant Schoenebeck"]
  },
  {
    "id": "2402.13572",
    "title": "On the Expressive Power of a Variant of the Looped Transformer",
    "abstract": "  Besides natural language processing, transformers exhibit extraordinary\nperformance in solving broader applications, including scientific computing and\ncomputer vision. Previous works try to explain this from the expressive power\nand capability perspectives that standard transformers are capable of\nperforming some algorithms. To empower transformers with algorithmic\ncapabilities and motivated by the recently proposed looped transformer (Yang et\nal., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed\nAlgorithm Transformer (abbreviated as AlgoFormer). Compared with the standard\ntransformer and vanilla looped transformer, the proposed AlgoFormer can achieve\nsignificantly higher expressiveness in algorithm representation when using the\nsame number of parameters. In particular, inspired by the structure of\nhuman-designed learning algorithms, our transformer block consists of a\npre-transformer that is responsible for task pre-processing, a looped\ntransformer for iterative optimization algorithms, and a post-transformer for\nproducing the desired results after post-processing. We provide theoretical\nevidence of the expressive power of the AlgoFormer in solving some challenging\nproblems, mirroring human-designed algorithms. Furthermore, some theoretical\nand empirical results are presented to show that the designed transformer has\nthe potential to be smarter than human-designed algorithms. Experimental\nresults demonstrate the empirical superiority of the proposed transformer in\nthat it outperforms the standard transformer and vanilla looped transformer in\nsome challenging tasks.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13572v1",
    "authors": [
      "Yihang Gao",
      "Chuanyang Zheng",
      "Enze Xie",
      "Han Shi",
      "Tianyang Hu",
      "Yu Li",
      "Michael K. Ng",
      "Zhenguo Li",
      "Zhaoqiang Liu"
    ]
  },
  {
    "id": "2402.13573",
    "title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images",
    "abstract": "  Attention mechanism has been crucial for image diffusion models, however,\ntheir quadratic computational complexity limits the sizes of images we can\nprocess within reasonable time and memory constraints. This paper investigates\nthe importance of dense attention in generative image models, which often\ncontain redundant features, making them suitable for sparser attention\nmechanisms. We propose a novel training-free method ToDo that relies on token\ndownsampling of key and value tokens to accelerate Stable Diffusion inference\nby up to 2x for common sizes and up to 4.5x or more for high resolutions like\n2048x2048. We demonstrate that our approach outperforms previous methods in\nbalancing efficient throughput and fidelity.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13573v2",
    "authors": ["Ethan Smith", "Nayan Saxena", "Aninda Saha"]
  },
  {
    "id": "2402.13598",
    "title": "User-LLM: Efficient LLM Contextualization with User Embeddings",
    "abstract": "  Large language models (LLMs) have revolutionized natural language processing.\nHowever, effectively incorporating complex and potentially noisy user\ninteraction data remains a challenge. To address this, we propose User-LLM, a\nnovel framework that leverages user embeddings to contextualize LLMs. These\nembeddings, distilled from diverse user interactions using self-supervised\npretraining, capture latent user preferences and their evolution over time. We\nintegrate these user embeddings with LLMs through cross-attention and\nsoft-prompting, enabling LLMs to dynamically adapt to user context. Our\ncomprehensive experiments on MovieLens, Amazon Review, and Google Local Review\ndatasets demonstrate significant performance gains across various tasks.\nNotably, our approach outperforms text-prompt-based contextualization on long\nsequence tasks and tasks that require deep user understanding while being\ncomputationally efficient. We further incorporate Perceiver layers to\nstreamline the integration between user encoders and LLMs, reducing\ncomputational demands.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13598v1",
    "authors": [
      "Lin Ning",
      "Luyang Liu",
      "Jiaxing Wu",
      "Neo Wu",
      "Devora Berlowitz",
      "Sushant Prakash",
      "Bradley Green",
      "Shawn O'Banion",
      "Jun Xie"
    ]
  },
  {
    "id": "2402.13610",
    "title": "Data-driven Discovery with Large Generative Models",
    "abstract": "  With the accumulation of data at an unprecedented rate, its potential to fuel\nscientific discovery is growing exponentially. This position paper urges the\nMachine Learning (ML) community to exploit the capabilities of large generative\nmodels (LGMs) to develop automated systems for end-to-end data-driven discovery\n-- a paradigm encompassing the search and verification of hypotheses purely\nfrom a set of provided datasets, without the need for additional data\ncollection or physical experiments. We first outline several desiderata for an\nideal data-driven discovery system. Then, through DATAVOYAGER, a\nproof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of\nthese desiderata -- a feat previously unattainable -- while also highlighting\nimportant limitations in the current system that open up opportunities for\nnovel ML research. We contend that achieving accurate, reliable, and robust\nend-to-end discovery systems solely through the current capabilities of LGMs is\nchallenging. We instead advocate for fail-proof tool integration, along with\nactive user moderation through feedback mechanisms, to foster data-driven\nscientific discoveries with efficiency and reproducibility.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13610v1",
    "authors": [
      "Bodhisattwa Prasad Majumder",
      "Harshit Surana",
      "Dhruv Agarwal",
      "Sanchaita Hazra",
      "Ashish Sabharwal",
      "Peter Clark"
    ]
  },
  {
    "id": "2402.13615",
    "title": "Analyizing the Conjunction Fallacy as a Fact",
    "abstract": "  Since the seminal paper by Tversky and Kahneman, the conjunction fallacy has\nbeen the subject of multiple debates and become a fundamental challenge for\ncognitive theories in decision-making. In this article, we take a rather\nuncommon perspective on this phenomenon. Instead of trying to explain the\nnature or causes of the conjunction fallacy (intensional definition), we\nanalyze its range of factual possibilities (extensional definition). We show\nthat the majority of research on the conjunction fallacy, according to our\nsample of experiments reviewed which covers literature between 1983 and 2016,\nhas focused on a narrow part of the a priori factual possibilities, implying\nthat explanations of the conjunction fallacy are fundamentally biased by the\nshort scope of possibilities explored. The latter is a rather curious aspect of\nthe research evolution in the conjunction fallacy considering that the very\nnature of it is motivated by extensional considerations.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13615v1",
    "authors": ["Tomas Veloz", "Olha Sobetska"]
  },
  {
    "id": "2402.13714",
    "title": "An Evaluation of Large Language Models in Bioinformatics Research",
    "abstract": "  Large language models (LLMs) such as ChatGPT have gained considerable\ninterest across diverse research communities. Their notable ability for text\ncompletion and generation has inaugurated a novel paradigm for\nlanguage-interfaced problem solving. However, the potential and efficacy of\nthese models in bioinformatics remain incompletely explored. In this work, we\nstudy the performance LLMs on a wide spectrum of crucial bioinformatics tasks.\nThese tasks include the identification of potential coding regions, extraction\nof named entities for genes and proteins, detection of antimicrobial and\nanti-cancer peptides, molecular optimization, and resolution of educational\nbioinformatics problems. Our findings indicate that, given appropriate prompts,\nLLMs like GPT variants can successfully handle most of these tasks. In\naddition, we provide a thorough analysis of their limitations in the context of\ncomplicated bioinformatics tasks. In conclusion, we believe that this work can\nprovide new perspectives and motivate future research in the field of LLMs\napplications, AI for Science and bioinformatics.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13714v1",
    "authors": [
      "Hengchuang Yin",
      "Zhonghui Gu",
      "Fanhao Wang",
      "Yiparemu Abuduhaibaier",
      "Yanqiao Zhu",
      "Xinming Tu",
      "Xian-Sheng Hua",
      "Xiao Luo",
      "Yizhou Sun"
    ]
  },
  {
    "id": "2402.13750",
    "title": "Breaking the Barrier: Utilizing Large Language Models for Industrial\n  Recommendation Systems through an Inferential Knowledge Graph",
    "abstract": "  Recommendation systems are widely used in e-commerce websites and online\nplatforms to address information overload. However, existing systems primarily\nrely on historical data and user feedback, making it difficult to capture user\nintent transitions. Recently, Knowledge Base (KB)-based models are proposed to\nincorporate expert knowledge, but it struggle to adapt to new items and the\nevolving e-commerce environment. To address these challenges, we propose a\nnovel Large Language Model based Complementary Knowledge Enhanced\nRecommendation System (LLM-KERec). It introduces an entity extractor that\nextracts unified concept terms from item and user information. To provide\ncost-effective and reliable prior knowledge, entity pairs are generated based\non entity popularity and specific strategies. The large language model\ndetermines complementary relationships in each entity pair, constructing a\ncomplementary knowledge graph. Furthermore, a new complementary recall module\nand an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of\nthe ranking model using real complementary exposure-click samples. Extensive\nexperiments conducted on three industry datasets demonstrate the significant\nperformance improvement of our model compared to existing approaches.\nAdditionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm\nfor consumption by recommending complementary items. In summary, LLM-KERec\naddresses the limitations of traditional recommendation systems by\nincorporating complementary knowledge and utilizing a large language model to\ncapture user intent transitions, adapt to new items, and enhance recommendation\nefficiency in the evolving e-commerce landscape.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13750v1",
    "authors": ["Qian Zhao", "Hao Qian", "Ziqi Liu", "Gong-Duo Zhang", "Lihong Gu"]
  },
  {
    "id": "2402.13754",
    "title": "Reinforcement learning-assisted quantum architecture search for\n  variational quantum algorithms",
    "abstract": "  A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is\nidentifying functional quantum circuits. These circuits must also adhere to the\nconstraints imposed by current quantum hardware limitations. Variational\nquantum algorithms (VQAs), a class of quantum-classical optimization\nalgorithms, were developed to address these challenges in the currently\navailable quantum devices. However, the overall performance of VQAs depends on\nthe initialization strategy of the variational circuit, the structure of the\ncircuit (also known as ansatz), and the configuration of the cost function.\nFocusing on the structure of the circuit, in this thesis, we improve the\nperformance of VQAs by automating the search for an optimal structure for the\nvariational circuits using reinforcement learning (RL). Within the thesis, the\noptimality of a circuit is determined by evaluating its depth, the overall\ncount of gates and parameters, and its accuracy in solving the given problem.\nThe task of automating the search for optimal quantum circuits is known as\nquantum architecture search (QAS). The majority of research in QAS is primarily\nfocused on a noiseless scenario. Yet, the impact of noise on the QAS remains\ninadequately explored. In this thesis, we tackle the issue by introducing a\ntensor-based quantum circuit encoding, restrictions on environment dynamics to\nexplore the search space of possible circuits efficiently, an episode halting\nscheme to steer the agent to find shorter circuits, a double deep Q-network\n(DDQN) with an $\\epsilon$-greedy policy for better stability. The numerical\nexperiments on noiseless and noisy quantum hardware show that in dealing with\nvarious VQAs, our RL-based QAS outperforms existing QAS. Meanwhile, the methods\nwe propose in the thesis can be readily adapted to address a wide range of\nother VQAs.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13754v2",
    "authors": ["Akash Kundu"]
  },
  {
    "id": "2402.13771",
    "title": "Mask-up: Investigating Biases in Face Re-identification for Masked Faces",
    "abstract": "  AI based Face Recognition Systems (FRSs) are now widely distributed and\ndeployed as MLaaS solutions all over the world, moreso since the COVID-19\npandemic for tasks ranging from validating individuals' faces while buying SIM\ncards to surveillance of citizens. Extensive biases have been reported against\nmarginalized groups in these systems and have led to highly discriminatory\noutcomes. The post-pandemic world has normalized wearing face masks but FRSs\nhave not kept up with the changing times. As a result, these systems are\nsusceptible to mask based face occlusion. In this study, we audit four\ncommercial and nine open-source FRSs for the task of face re-identification\nbetween different varieties of masked and unmasked images across five benchmark\ndatasets (total 14,722 images). These simulate a realistic\nvalidation/surveillance task as deployed in all major countries around the\nworld. Three of the commercial and five of the open-source FRSs are highly\ninaccurate; they further perpetuate biases against non-White individuals, with\nthe lowest accuracy being 0%. A survey for the same task with 85 human\nparticipants also results in a low accuracy of 40%. Thus a human-in-the-loop\nmoderation in the pipeline does not alleviate the concerns, as has been\nfrequently hypothesized in literature. Our large-scale study shows that\ndevelopers, lawmakers and users of such services need to rethink the design\nprinciples behind FRSs, especially for the task of face re-identification,\ntaking cognizance of observed biases.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13771v1",
    "authors": ["Siddharth D Jaiswal", "Ankit Kr. Verma", "Animesh Mukherjee"]
  },
  {
    "id": "2402.13779",
    "title": "Contextual Molecule Representation Learning from Chemical Reaction\n  Knowledge",
    "abstract": "  In recent years, self-supervised learning has emerged as a powerful tool to\nharness abundant unlabelled data for representation learning and has been\nbroadly adopted in diverse areas. However, when applied to molecular\nrepresentation learning (MRL), prevailing techniques such as masked sub-unit\nreconstruction often fall short, due to the high degree of freedom in the\npossible combinations of atoms within molecules, which brings insurmountable\ncomplexity to the masking-reconstruction paradigm. To tackle this challenge, we\nintroduce REMO, a self-supervised learning framework that takes advantage of\nwell-defined atom-combination rules in common chemistry. Specifically, REMO\npre-trains graph/Transformer encoders on 1.7 million known chemical reactions\nin the literature. We propose two pre-training objectives: Masked Reaction\nCentre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO\noffers a novel solution to MRL by exploiting the underlying shared patterns in\nchemical reactions as \\textit{context} for pre-training, which effectively\ninfers meaningful representations of common chemistry knowledge. Such\ncontextual representations can then be utilized to support diverse downstream\nmolecular tasks with minimum finetuning, such as affinity prediction and\ndrug-drug interaction prediction. Extensive experimental results on\nMoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type\nclassification show that across all tested downstream tasks, REMO outperforms\nthe standard baseline of single-molecule masked modeling used in current MRL.\nRemarkably, REMO is the pioneering deep learning model surpassing\nfingerprint-based methods in activity cliff benchmarks.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13779v1",
    "authors": [
      "Han Tang",
      "Shikun Feng",
      "Bicheng Lin",
      "Yuyan Ni",
      "JIngjing Liu",
      "Wei-Ying Ma",
      "Yanyan Lan"
    ]
  },
  {
    "id": "2402.13809",
    "title": "NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual\n  Feature Guided Diffusion",
    "abstract": "  Reconstructing visual stimuli from functional Magnetic Resonance Imaging\n(fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval\nof the brain. A challenge persists in reconstructing a cohesive alignment of\ndetails (such as structure, background, texture, color, etc.). Moreover, LDMs\nwould generate different image results even under the same conditions. For\nthese, we first uncover the neuroscientific perspective of LDM-based methods\nthat is top-down creation based on pre-trained knowledge from massive images\nbut lack of detail-driven bottom-up perception resulting in unfaithful details.\nWe propose NeuralDiffuser which introduces primary visual feature guidance to\nprovide detail cues in the form of gradients, extending the bottom-up process\nfor LDM-based methods to achieve faithful semantics and details. We also\ndeveloped a novel guidance strategy to ensure the consistency of repeated\nreconstructions rather than a variety of results. We obtain the\nstate-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset\n(NSD), which offers more faithful details and consistent results.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13809v1",
    "authors": ["Haoyu Li", "Hao Wu", "Badong Chen"]
  },
  {
    "id": "2402.13846",
    "title": "Large Language Models are Advanced Anonymizers",
    "abstract": "  Recent work in privacy research on large language models has shown that they\nachieve near human-level performance at inferring personal data from real-world\nonline texts. With consistently increasing model capabilities, existing text\nanonymization methods are currently lacking behind regulatory requirements and\nadversarial threats. This raises the question of how individuals can\neffectively protect their personal data in sharing online texts. In this work,\nwe take two steps to answer this question: We first present a new setting for\nevaluating anonymizations in the face of adversarial LLMs inferences, allowing\nfor a natural measurement of anonymization performance while remedying some of\nthe shortcomings of previous metrics. We then present our LLM-based adversarial\nanonymization framework leveraging the strong inferential capabilities of LLMs\nto inform our anonymization procedure. In our experimental evaluation, we show\non real-world and synthetic online texts how adversarial anonymization\noutperforms current industry-grade anonymizers both in terms of the resulting\nutility and privacy.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13846v1",
    "authors": ["Robin Staab", "Mark Vero", "Mislav Balunović", "Martin Vechev"]
  },
  {
    "id": "2402.13853",
    "title": "RealDex: Towards Human-like Grasping for Robotic Dexterous Hand",
    "abstract": "  In this paper, we introduce RealDex, a pioneering dataset capturing authentic\ndexterous hand grasping motions infused with human behavioral patterns,\nenriched by multi-view and multimodal visual data. Utilizing a teleoperation\nsystem, we seamlessly synchronize human-robot hand poses in real time. This\ncollection of human-like motions is crucial for training dexterous hands to\nmimic human movements more naturally and precisely. RealDex holds immense\npromise in advancing humanoid robot for automated perception, cognition, and\nmanipulation in real-world scenarios. Moreover, we introduce a cutting-edge\ndexterous grasping motion generation framework, which aligns with human\nexperience and enhances real-world applicability through effectively utilizing\nMultimodal Large Language Models. Extensive experiments have demonstrated the\nsuperior performance of our method on RealDex and other open datasets. The\ncomplete dataset and code will be made available upon the publication of this\nwork.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13853v1",
    "authors": [
      "Yumeng Liu",
      "Yaxun Yang",
      "Youzhuo Wang",
      "Xiaofei Wu",
      "Jiamin Wang",
      "Yichen Yao",
      "Sören Schwertfeger",
      "Sibei Yang",
      "Wenping Wang",
      "Jingyi Yu",
      "Xuming He",
      "Yuexin Ma"
    ]
  },
  {
    "id": "2402.13871",
    "title": "An Explainable Transformer-based Model for Phishing Email Detection: A\n  Large Language Model Approach",
    "abstract": "  Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13871v1",
    "authors": ["Mohammad Amaz Uddin", "Iqbal H. Sarker"]
  },
  {
    "id": "2402.13897",
    "title": "Science Checker Reloaded: A Bidirectional Paradigm for Transparency and\n  Logical Reasoning",
    "abstract": "  Information retrieval is a rapidly evolving field. However it still faces\nsignificant limitations in the scientific and industrial vast amounts of\ninformation, such as semantic divergence and vocabulary gaps in sparse\nretrieval, low precision and lack of interpretability in semantic search, or\nhallucination and outdated information in generative models. In this paper, we\nintroduce a two-block approach to tackle these hurdles for long documents. The\nfirst block enhances language understanding in sparse retrieval by query\nexpansion to retrieve relevant documents. The second block deepens the result\nby providing comprehensive and informative answers to the complex question\nusing only the information spread in the long document, enabling bidirectional\nengagement. At various stages of the pipeline, intermediate results are\npresented to users to facilitate understanding of the system's reasoning. We\nbelieve this bidirectional approach brings significant advancements in terms of\ntransparency, logical thinking, and comprehensive understanding in the field of\nscientific information retrieval.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13897v1",
    "authors": ["Loïc Rakotoson", "Sylvain Massip", "Fréjus A. A. Laleye"]
  },
  {
    "id": "2402.13914",
    "title": "Explain to Question not to Justify",
    "abstract": "  Explainable Artificial Intelligence (XAI) is a young but very promising field\nof research. Unfortunately, the progress in this field is currently slowed down\nby divergent and incompatible goals. In this paper, we separate various threads\ntangled within the area of XAI into two complementary cultures of\nhuman/value-oriented explanations (BLUE XAI) and model/validation-oriented\nexplanations (RED XAI). We also argue that the area of RED XAI is currently\nunder-explored and hides great opportunities and potential for important\nresearch necessary to ensure the safety of AI systems. We conclude this paper\nby presenting promising challenges in this area.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13914v1",
    "authors": ["Przemyslaw Biecek", "Wojciech Samek"]
  },
  {
    "id": "2402.13929",
    "title": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
    "abstract": "  We propose a diffusion distillation method that achieves new state-of-the-art\nin one-step/few-step 1024px text-to-image generation based on SDXL. Our method\ncombines progressive and adversarial distillation to achieve a balance between\nquality and mode coverage. In this paper, we discuss the theoretical analysis,\ndiscriminator design, model formulation, and training techniques. We\nopen-source our distilled SDXL-Lightning models both as LoRA and full UNet\nweights.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13929v2",
    "authors": ["Shanchuan Lin", "Anran Wang", "Xiao Yang"]
  },
  {
    "id": "2402.13934",
    "title": "Do Efficient Transformers Really Save Computation?",
    "abstract": "  As transformer-based language models are trained on increasingly large\ndatasets and with vast numbers of parameters, finding more efficient\nalternatives to the standard Transformer has become very valuable. While many\nefficient Transformers and Transformer alternatives have been proposed, none\nprovide theoretical guarantees that they are a suitable replacement for the\nstandard Transformer. This makes it challenging to identify when to use a\nspecific model and what directions to prioritize for further investigation. In\nthis paper, we aim to understand the capabilities and limitations of efficient\nTransformers, specifically the Sparse Transformer and the Linear Transformer.\nWe focus on their reasoning capability as exhibited by Chain-of-Thought (CoT)\nprompts and follow previous works to model them as Dynamic Programming (DP)\nproblems. Our results show that while these models are expressive enough to\nsolve general DP tasks, contrary to expectations, they require a model size\nthat scales with the problem size. Nonetheless, we identify a class of DP\nproblems for which these models can be more efficient than the standard\nTransformer. We confirm our theoretical results through experiments on\nrepresentative DP tasks, adding to the understanding of efficient Transformers'\npractical strengths and weaknesses.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13934v1",
    "authors": [
      "Kai Yang",
      "Jan Ackermann",
      "Zhenyu He",
      "Guhao Feng",
      "Bohang Zhang",
      "Yunzhen Feng",
      "Qiwei Ye",
      "Di He",
      "Liwei Wang"
    ]
  },
  {
    "id": "2402.13945",
    "title": "Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty\n  in Scientific Machine Learning",
    "abstract": "  This paper investigates the use of probabilistic neural networks (PNNs) to\nmodel aleatoric uncertainty, which refers to the inherent variability in the\ninput-output relationships of a system, often characterized by unequal variance\nor heteroscedasticity. Unlike traditional neural networks that produce\ndeterministic outputs, PNNs generate probability distributions for the target\nvariable, allowing the determination of both predicted means and intervals in\nregression scenarios. Contributions of this paper include the development of a\nprobabilistic distance metric to optimize PNN architecture, and the deployment\nof PNNs in controlled data sets as well as a practical material science case\ninvolving fiber-reinforced composites. The findings confirm that PNNs\neffectively model aleatoric uncertainty, proving to be more appropriate than\nthe commonly employed Gaussian process regression for this purpose.\nSpecifically, in a real-world scientific machine learning context, PNNs yield\nremarkably accurate output mean estimates with R-squared scores approaching\n0.97, and their predicted intervals exhibit a high correlation coefficient of\nnearly 0.80, closely matching observed data intervals. Hence, this research\ncontributes to the ongoing exploration of leveraging the sophisticated\nrepresentational capacity of neural networks to delineate complex input-output\nrelationships in scientific problems.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13945v1",
    "authors": ["Farhad Pourkamali-Anaraki", "Jamal F. Husseini", "Scott E. Stapleton"]
  },
  {
    "id": "2402.14015",
    "title": "Corrective Machine Unlearning",
    "abstract": "  Machine Learning models increasingly face data integrity challenges due to\nthe use of large-scale training datasets drawn from the internet. We study what\nmodel developers can do if they detect that some data was manipulated or\nincorrect. Such manipulated data can cause adverse effects like vulnerability\nto backdoored samples, systematic biases, and in general, reduced accuracy on\ncertain input domains. Often, all manipulated training samples are not known,\nand only a small, representative subset of the affected data is flagged.\n  We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the\nimpact of data affected by unknown manipulations on a trained model, possibly\nknowing only a subset of impacted samples. We demonstrate that the problem of\ncorrective unlearning has significantly different requirements from traditional\nprivacy-oriented unlearning. We find most existing unlearning methods,\nincluding the gold-standard retraining-from-scratch, require most of the\nmanipulated data to be identified for effective corrective unlearning. However,\none approach, SSD, achieves limited success in unlearning adverse effects with\njust a small portion of the manipulated samples, showing the tractability of\nthis setting. We hope our work spurs research towards developing better methods\nfor corrective unlearning and offers practitioners a new strategy to handle\ndata integrity challenges arising from web-scale training.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14015v1",
    "authors": [
      "Shashwat Goel",
      "Ameya Prabhu",
      "Philip Torr",
      "Ponnurangam Kumaraguru",
      "Amartya Sanyal"
    ]
  },
  {
    "id": "2402.14041",
    "title": "E2USD: Efficient-yet-effective Unsupervised State Detection for\n  Multivariate Time Series",
    "abstract": "  We propose E2USD that enables efficient-yet-accurate unsupervised MTS state\ndetection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor\n(FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together\nencode input MTSs at low computational overhead. Additionally, we propose a\nFalse Negative Cancellation Contrastive Learning method (FNCCLearning) to\ncounteract the effects of false negatives and to achieve more cluster-friendly\nembedding spaces. To reduce computational overhead further in streaming\nsettings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive\nexperiments with six baselines and six datasets offer evidence that E2USD is\ncapable of SOTA accuracy at significantly reduced computational overhead. Our\ncode is available at https://github.com/AI4CTS/E2Usd.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14041v3",
    "authors": [
      "Zhichen Lai",
      "Huan Li",
      "Dalin Zhang",
      "Yan Zhao",
      "Weizhu Qian",
      "Christian S. Jensen"
    ]
  },
  {
    "id": "2402.14042",
    "title": "Protect and Extend -- Using GANs for Synthetic Data Generation of\n  Time-Series Medical Records",
    "abstract": "  Preservation of private user data is of paramount importance for high Quality\nof Experience (QoE) and acceptability, particularly with services treating\nsensitive data, such as IT-based health services. Whereas anonymization\ntechniques were shown to be prone to data re-identification, synthetic data\ngeneration has gradually replaced anonymization since it is relatively less\ntime and resource-consuming and more robust to data leakage. Generative\nAdversarial Networks (GANs) have been used for generating synthetic datasets,\nespecially GAN frameworks adhering to the differential privacy phenomena. This\nresearch compares state-of-the-art GAN-based models for synthetic data\ngeneration to generate time-series synthetic medical records of dementia\npatients which can be distributed without privacy concerns. Predictive\nmodeling, autocorrelation, and distribution analysis are used to assess the\nQuality of Generating (QoG) of the generated data. The privacy preservation of\nthe respective models is assessed by applying membership inference attacks to\ndetermine potential data leakage risks. Our experiments indicate the\nsuperiority of the privacy-preserving GAN (PPGAN) model over other models\nregarding privacy preservation while maintaining an acceptable level of QoG.\nThe presented results can support better data protection for medical use cases\nin the future.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14042v1",
    "authors": [
      "Navid Ashrafi",
      "Vera Schmitt",
      "Robert P. Spang",
      "Sebastian Möller",
      "Jan-Niklas Voigt-Antons"
    ]
  },
  {
    "id": "2402.14049",
    "title": "Generative Adversarial Models for Extreme Downscaling of Climate\n  Datasets",
    "abstract": "  Addressing the challenges of climate change requires accurate and\nhigh-resolution mapping of climate and weather variables. However, many\nexisting climate datasets, such as the gridded outputs of the state-of-the-art\nnumerical climate models (e.g., general circulation models), are only available\nat very coarse spatial resolutions due to the model complexity and extremely\nhigh computational demand. Deep-learning-based methods, particularly generative\nadversarial networks (GANs) and their variants, have proved effective for\nrefining natural images, and have shown great promise in improving scientific\ndatasets. In this paper, we describe a conditional GAN-based geospatial\ndownscaling method for extreme downscaling of gridded climate datasets.\nCompared to most existing methods, the method can generate high-resolution\naccurate climate datasets from very low-resolution inputs. More importantly,\nthe method explicitly considers the uncertainty inherent to the downscaling\nprocess that tends to be ignored in existing methods. Given an input, the\nmethod can produce a multitude of plausible high-resolution samples instead of\none single deterministic result. These samples allow for an empirical\nexploration and inferences of model uncertainty and robustness. With a case\nstudy of gridded climate datasets (wind velocity and solar irradiance), we\ndemonstrate the performances of the framework in downscaling tasks with very\nhigh scaling factors (up to $64\\times$) and highlight the advantages of the\nframework with a comprehensive comparison with commonly used downscaling\nmethods, including area-to-point (ATP) kriging, deep image prior (DIP),\nenhanced deep super-resolution network (EDSR), enhanced super-resolution\ngenerative adversarial networks (ESRGAN), and physics-informed\nresolution-enhancing GAN (PhIRE GAN).\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14049v1",
    "authors": ["Guiye Li", "Guofeng Cao"]
  },
  {
    "id": "2402.14080",
    "title": "Efficient Normalized Conformal Prediction and Uncertainty Quantification\n  for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests",
    "abstract": "  Deep learning models are being adopted and applied on various critical\ndecision-making tasks, yet they are trained to provide point predictions\nwithout providing degrees of confidence. The trustworthiness of deep learning\nmodels can be increased if paired with uncertainty estimations. Conformal\nPrediction has emerged as a promising method to pair machine learning models\nwith prediction intervals, allowing for a view of the model's uncertainty.\nHowever, popular uncertainty estimation methods for conformal prediction fail\nto provide heteroskedastic intervals that are equally accurate for all samples.\nIn this paper, we propose a method to estimate the uncertainty of each sample\nby calculating the variance obtained from a Deep Regression Forest. We show\nthat the deep regression forest variance improves the efficiency and coverage\nof normalized inductive conformal prediction on a drug response prediction\ntask.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14080v1",
    "authors": ["Daniel Nolte", "Souparno Ghosh", "Ranadip Pal"]
  },
  {
    "id": "2402.14081",
    "title": "Robust Learning of Noisy Time Series Collections Using Stochastic\n  Process Models with Motion Codes",
    "abstract": "  While time series classification and forecasting problems have been\nextensively studied, the cases of noisy time series data with arbitrary time\nsequence lengths have remained challenging. Each time series instance can be\nthought of as a sample realization of a noisy dynamical model, which is\ncharacterized by a continuous stochastic process. For many applications, the\ndata are mixed and consist of several types of noisy time series sequences\nmodeled by multiple stochastic processes, making the forecasting and\nclassification tasks even more challenging. Instead of regressing data naively\nand individually to each time series type, we take a latent variable model\napproach using a mixtured Gaussian processes with learned spectral kernels.\nMore specifically, we auto-assign each type of noisy time series data a\nsignature vector called its motion code. Then, conditioned on each assigned\nmotion code, we infer a sparse approximation of the corresponding time series\nusing the concept of the most informative timestamps. Our unmixing\nclassification approach involves maximizing the likelihood across all the mixed\nnoisy time series sequences of varying lengths. This stochastic approach allows\nus to learn not only within a single type of noisy time series data but also\nacross many underlying stochastic processes, giving us a way to learn multiple\ndynamical models in an integrated and robust manner. The different learned\nlatent stochastic models allow us to generate specific sub-type forecasting. We\nprovide several quantitative comparisons demonstrating the performance of our\napproach.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14081v1",
    "authors": ["Chandrajit Bajaj", "Minh Nguyen"]
  },
  {
    "id": "2402.14086",
    "title": "LexC-Gen: Generating Data for Extremely Low-Resource Languages with\n  Large Language Models and Bilingual Lexicons",
    "abstract": "  Data scarcity in low-resource languages can be addressed with word-to-word\ntranslations from labeled task data in high-resource languages using bilingual\nlexicons. However, bilingual lexicons often have limited lexical overlap with\ntask data, which results in poor translation coverage and lexicon utilization.\nWe propose lexicon-conditioned data generation (LexC-Gen), a method that\ngenerates low-resource-language classification task data at scale.\nSpecifically, LexC-Gen first uses high-resource-language words from bilingual\nlexicons to generate lexicon-compatible task data, and then it translates them\ninto low-resource languages with bilingual lexicons via word translation.\nAcross 17 extremely low-resource languages, LexC-Gen generated data is\ncompetitive with expert-translated gold data, and yields on average 5.6 and 8.9\npoints improvement over existing lexicon-based word translation methods on\nsentiment analysis and topic classification tasks respectively. We show that\nconditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen\nis also practical -- it only needs a single GPU to generate data at scale. It\nworks well with open-access LLMs, and its cost is one-fifth of the cost of\nGPT4-based multilingual data generation.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14086v1",
    "authors": ["Zheng-Xin Yong", "Cristina Menghini", "Stephen H. Bach"]
  },
  {
    "id": "2402.14090",
    "title": "Social Environment Design",
    "abstract": "  Artificial Intelligence (AI) holds promise as a technology that can be used\nto improve government and economic policy-making. This paper proposes a new\nresearch agenda towards this end by introducing Social Environment Design, a\ngeneral framework for the use of AI for automated policy-making that connects\nwith the Reinforcement Learning, EconCS, and Computational Social Choice\ncommunities. The framework seeks to capture general economic environments,\nincludes voting on policy objectives, and gives a direction for the systematic\nanalysis of government and economic policy through AI simulation. We highlight\nkey open problems for future research in AI-based policy-making. By solving\nthese challenges, we hope to achieve various social welfare objectives, thereby\npromoting more ethical and responsible decision making.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14090v1",
    "authors": [
      "Edwin Zhang",
      "Sadie Zhao",
      "Tonghan Wang",
      "Safwan Hossain",
      "Henry Gasztowtt",
      "Stephan Zheng",
      "David C. Parkes",
      "Milind Tambe",
      "Yiling Chen"
    ]
  },
  {
    "id": "2402.14095",
    "title": "Zero-shot generalization across architectures for visual classification",
    "abstract": "  Generalization to unseen data is a key desideratum for deep networks, but its\nrelation to classification accuracy is unclear. Using a minimalist vision\ndataset and a measure of generalizability, we show that popular networks, from\ndeep convolutional networks (CNNs) to transformers, vary in their power to\nextrapolate to unseen classes both across layers and across architectures.\nAccuracy is not a good predictor of generalizability, and generalization varies\nnon-monotonically with layer depth. Code is available at\nhttps://github.com/dyballa/zero-shot-generalization.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14095v2",
    "authors": ["Evan Gerritz", "Luciano Dyballa", "Steven W. Zucker"]
  },
  {
    "id": "2402.14096",
    "title": "EyeTrans: Merging Human and Machine Attention for Neural Code\n  Summarization",
    "abstract": "  Neural code summarization leverages deep learning models to automatically\ngenerate brief natural language summaries of code snippets. The development of\nTransformer models has led to extensive use of attention during model design.\nWhile existing work has primarily and almost exclusively focused on static\nproperties of source code and related structural representations like the\nAbstract Syntax Tree (AST), few studies have considered human attention, that\nis, where programmers focus while examining and comprehending code. In this\npaper, we develop a method for incorporating human attention into machine\nattention to enhance neural code summarization. To facilitate this\nincorporation and vindicate this hypothesis, we introduce EyeTrans, which\nconsists of three steps: (1) we conduct an extensive eye-tracking human study\nto collect and pre-analyze data for model training, (2) we devise a\ndata-centric approach to integrate human attention with machine attention in\nthe Transformer architecture, and (3) we conduct comprehensive experiments on\ntwo code summarization tasks to demonstrate the effectiveness of incorporating\nhuman attention into Transformers. Integrating human attention leads to an\nimprovement of up to 29.91% in Functional Summarization and up to 6.39% in\nGeneral Code Summarization performance, demonstrating the substantial benefits\nof this combination. We further explore performance in terms of robustness and\nefficiency by creating challenging summarization scenarios in which EyeTrans\nexhibits interesting properties. We also visualize the attention map to depict\nthe simplifying effect of machine attention in the Transformer by incorporating\nhuman attention. This work has the potential to propel AI research in software\nengineering by introducing more human-centered approaches and data.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14096v3",
    "authors": [
      "Yifan Zhang",
      "Jiliang Li",
      "Zachary Karas",
      "Aakash Bansal",
      "Toby Jia-Jun Li",
      "Collin McMillan",
      "Kevin Leach",
      "Yu Huang"
    ]
  },
  {
    "id": "2402.14123",
    "title": "DeiSAM: Segment Anything with Deictic Prompting",
    "abstract": "  Large-scale, pre-trained neural networks have demonstrated strong\ncapabilities in various tasks, including zero-shot image segmentation. To\nidentify concrete objects in complex scenes, humans instinctively rely on\ndeictic descriptions in natural language, i.e., referring to something\ndepending on the context such as \"The object that is on the desk and behind the\ncup.\". However, deep learning approaches cannot reliably interpret such deictic\nrepresentations due to their lack of reasoning capabilities in complex\nscenarios. To remedy this issue, we propose DeiSAM -- a combination of large\npre-trained neural networks with differentiable logic reasoners -- for deictic\npromptable segmentation. Given a complex, textual segmentation description,\nDeiSAM leverages Large Language Models (LLMs) to generate first-order logic\nrules and performs differentiable forward reasoning on generated scene graphs.\nSubsequently, DeiSAM segments objects by matching them to the logically\ninferred image regions. As part of our evaluation, we propose the Deictic\nVisual Genome (DeiVG) dataset, containing paired visual input and complex,\ndeictic textual prompts. Our empirical results demonstrate that DeiSAM is a\nsubstantial improvement over purely data-driven baselines for deictic\npromptable segmentation.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14123v1",
    "authors": [
      "Hikaru Shindo",
      "Manuel Brack",
      "Gopika Sudhakaran",
      "Devendra Singh Dhami",
      "Patrick Schramowski",
      "Kristian Kersting"
    ]
  },
  {
    "id": "2402.14151",
    "title": "BIRCO: A Benchmark of Information Retrieval Tasks with Complex\n  Objectives",
    "abstract": "  We present the Benchmark of Information Retrieval (IR) tasks with Complex\nObjectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve\ndocuments given multi-faceted user objectives. The benchmark's complexity and\ncompact size make it suitable for evaluating large language model (LLM)-based\ninformation retrieval systems. We present a modular framework for investigating\nfactors that may influence LLM performance on retrieval tasks, and identify a\nsimple baseline model which matches or outperforms existing approaches and more\ncomplex alternatives. No approach achieves satisfactory performance on all\nbenchmark tasks, suggesting that stronger models and new retrieval protocols\nare necessary to address complex user needs.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14151v1",
    "authors": [
      "Xiaoyue Wang",
      "Jianyou Wang",
      "Weili Cao",
      "Kaicheng Wang",
      "Ramamohan Paturi",
      "Leon Bergen"
    ]
  },
  {
    "id": "2402.14860",
    "title": "Ranking Large Language Models without Ground Truth",
    "abstract": "  Evaluation and ranking of large language models (LLMs) has become an\nimportant problem with the proliferation of these models and their impact.\nEvaluation methods either require human responses which are expensive to\nacquire or use pairs of LLMs to evaluate each other which can be unreliable. In\nthis paper, we provide a novel perspective where, given a dataset of prompts\n(viz. questions, instructions, etc.) and a set of LLMs, we rank them without\naccess to any ground truth or reference responses. Inspired by real life where\nboth an expert and a knowledgeable person can identify a novice our main idea\nis to consider triplets of models, where each one of them evaluates the other\ntwo, correctly identifying the worst model in the triplet with high\nprobability. We also analyze our idea and provide sufficient conditions for it\nto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.\nIn experiments on different generative tasks (summarization, multiple-choice,\nand dialog), our methods reliably recover close to true rankings without\nreference data. This points to a viable low-resource mechanism for practical\nuse.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14860v1",
    "authors": [
      "Amit Dhurandhar",
      "Rahul Nair",
      "Moninder Singh",
      "Elizabeth Daly",
      "Karthikeyan Natesan Ramamurthy"
    ]
  },
  {
    "id": "2402.14861",
    "title": "CloudNine: Analyzing Meteorological Observation Impact on Weather\n  Prediction Using Explainable Graph Neural Networks",
    "abstract": "  The impact of meteorological observations on weather forecasting varies with\nsensor type, location, time, and other environmental factors. Thus,\nquantitative analysis of observation impacts is crucial for effective and\nefficient development of weather forecasting systems. However, the existing\nimpact analysis methods are difficult to be widely applied due to their high\ndependencies on specific forecasting systems. Also, they cannot provide\nobservation impacts at multiple spatio-temporal scales, only global impacts of\nobservation types. To address these issues, we present a novel system called\n``CloudNine,'' which allows analysis of individual observations' impacts on\nspecific predictions based on explainable graph neural networks (XGNNs).\nCombining an XGNN-based atmospheric state estimation model with a numerical\nweather prediction model, we provide a web application to search for\nobservations in the 3D space of the Earth system and to visualize the impact of\nindividual observations on predictions in specific spatial regions and time\nperiods.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14861v1",
    "authors": ["Hyeon-Ju Jeon", "Jeon-Ho Kang", "In-Hyuk Kwon", "O-Joun Lee"]
  },
  {
    "id": "2402.14865",
    "title": "DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing\n  Agents",
    "abstract": "  Evaluation of large language models (LLMs) has raised great concerns in the\ncommunity due to the issue of data contamination. Existing work designed\nevaluation protocols using well-defined algorithms for specific tasks, which\ncannot be easily extended to diverse scenarios. Moreover, current evaluation\nbenchmarks can only provide the overall benchmark results and cannot support a\nfine-grained and multifaceted analysis of LLMs' abilities. In this paper, we\npropose meta probing agents (MPA), a general dynamic evaluation protocol\ninspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal\n2, which naturally extends the previous DyVal~\\citep{zhu2023dyval}. MPA designs\nthe probing and judging agents to automatically transform an original\nevaluation problem into a new one following psychometric theory on three basic\ncognitive abilities: language understanding, problem solving, and domain\nknowledge. These basic abilities are also dynamically configurable, allowing\nmultifaceted analysis. We conducted extensive evaluations using MPA and found\nthat most LLMs achieve poorer performance, indicating room for improvement. Our\nmultifaceted analysis demonstrated the strong correlation between the basic\nabilities and an implicit Matthew effect on model size, i.e., larger models\npossess stronger correlations of the abilities. MPA can also be used as a data\naugmentation approach to enhance LLMs.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14865v1",
    "authors": ["Kaijie Zhu", "Jindong Wang", "Qinlin Zhao", "Ruochen Xu", "Xing Xie"]
  },
  {
    "id": "2402.14866",
    "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for\n  Large Language Models",
    "abstract": "  Large Language Models (LLMs) have greatly advanced the natural language\nprocessing paradigm. However, the high computational load and huge model sizes\npose a grand challenge for deployment on edge devices. To this end, we propose\nAPTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs,\nwhich considers not only the second-order information of each layer's weights,\nbut also, for the first time, the nonlinear effect of attention outputs on the\nentire model. We leverage the Hessian trace as a sensitivity metric for\nmixed-precision quantization, ensuring an informed precision reduction that\nretains model performance. Experiments show APTQ surpasses previous\nquantization methods, achieving an average of 4 bit width a 5.22 perplexity\nnearly equivalent to full precision in the C4 dataset. In addition, APTQ\nattains state-of-the-art zero-shot accuracy of 68.24\\% and 70.48\\% at an\naverage bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating\nits effectiveness to produce high-quality quantized LLMs.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14866v1",
    "authors": ["Ziyi Guan", "Hantao Huang", "Yupeng Su", "Hong Huang", "Ngai Wong", "Hao Yu"]
  },
  {
    "id": "2402.14867",
    "title": "Effects of term weighting approach with and without stop words removing\n  on Arabic text classification",
    "abstract": "  Classifying text is a method for categorizing documents into pre-established\ngroups. Text documents must be prepared and represented in a way that is\nappropriate for the algorithms used for data mining prior to classification. As\na result, a number of term weighting strategies have been created in the\nliterature to enhance text categorization algorithms' functionality. This study\ncompares the effects of Binary and Term frequency weighting feature\nmethodologies on the text's classification method when stop words are\neliminated once and when they are not. In recognition of assessing the effects\nof prior weighting of features approaches on classification results in terms of\naccuracy, recall, precision, and F-measure values, we used an Arabic data set\nmade up of 322 documents divided into six main topics (agriculture, economy,\nhealth, politics, science, and sport), each of which contains 50 documents,\nwith the exception of the health category, which contains 61 documents. The\nresults demonstrate that for all metrics, the term frequency feature weighting\napproach with stop word removal outperforms the binary approach, while for\naccuracy, recall, and F-Measure, the binary approach outperforms the TF\napproach without stop word removal. However, for precision, the two approaches\nproduce results that are very similar. Additionally, it is clear from the data\nthat, using the same phrase weighting approach, stop word removing increases\nclassification accuracy.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14867v1",
    "authors": ["Esra'a Alhenawi", "Ruba Abu Khurma", "Pedro A. Castillo", "Maribel G. Arenas"]
  },
  {
    "id": "2402.14871",
    "title": "LLM Based Multi-Agent Generation of Semi-structured Documents from\n  Semantic Templates in the Public Administration Domain",
    "abstract": "  In the last years' digitalization process, the creation and management of\ndocuments in various domains, particularly in Public Administration (PA), have\nbecome increasingly complex and diverse. This complexity arises from the need\nto handle a wide range of document types, often characterized by\nsemi-structured forms. Semi-structured documents present a fixed set of data\nwithout a fixed format. As a consequence, a template-based solution cannot be\nused, as understanding a document requires the extraction of the data\nstructure. The recent introduction of Large Language Models (LLMs) has enabled\nthe creation of customized text output satisfying user requests. In this work,\nwe propose a novel approach that combines the LLMs with prompt engineering and\nmulti-agent systems for generating new documents compliant with a desired\nstructure. The main contribution of this work concerns replacing the commonly\nused manual prompting with a task description generated by semantic retrieval\nfrom an LLM. The potential of this approach is demonstrated through a series of\nexperiments and case studies, showcasing its effectiveness in real-world PA\nscenarios.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14871v1",
    "authors": [
      "Emanuele Musumeci",
      "Michele Brienza",
      "Vincenzo Suriani",
      "Daniele Nardi",
      "Domenico Daniele Bloisi"
    ]
  },
  {
    "id": "2402.14872",
    "title": "Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts\n  Against Open-source LLMs",
    "abstract": "  Large Language Models (LLMs), used in creative writing, code generation, and\ntranslation, generate text based on input sequences but are vulnerable to\njailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak\nprompt methods use a combination of jailbreak templates followed by questions\nto ask to create jailbreak prompts. However, existing jailbreak prompt designs\ngenerally suffer from excessive semantic differences, resulting in an inability\nto resist defenses that use simple semantic metrics as thresholds. Jailbreak\nprompts are semantically more varied than the original questions used for\nqueries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach\nthat bypasses LLMs by generating jailbreak prompts that are semantically\nsimilar to the original question. We model the search for jailbreak prompts\nthat satisfy both semantic similarity and jailbreak validity as a\nmulti-objective optimization problem and employ a standardized set of genetic\nalgorithms for generating eligible prompts. Compared to the baseline\nAutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%\nhigher without ONION defense and 85.2% higher with ONION defense. SMJ's better\nperformance in all three semantic meaningfulness metrics of Jailbreak Prompt,\nSimilarity, and Outlier, also means that SMJ is resistant to defenses that use\nthose metrics as thresholds.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14872v2",
    "authors": [
      "Xiaoxia Li",
      "Siyuan Liang",
      "Jiyi Zhang",
      "Han Fang",
      "Aishan Liu",
      "Ee-Chien Chang"
    ]
  },
  {
    "id": "2402.14874",
    "title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with\n  Contrastive Decoding and Distillation",
    "abstract": "  We propose a straightforward approach called Distillation Contrastive\nDecoding (DCD) to enhance the reasoning capabilities of Large Language Models\n(LLMs) during inference. In contrast to previous approaches that relied on\nsmaller amateur models or analysis of hidden state differences, DCD employs\nContrastive Chain-of-thought Prompting and advanced distillation techniques,\nincluding Dropout and Quantization. This approach effectively addresses the\nlimitations of Contrastive Decoding (CD), which typically requires both an\nexpert and an amateur model, thus increasing computational resource demands. By\nintegrating contrastive prompts with distillation, DCD obviates the need for an\namateur model and reduces memory usage. Our evaluations demonstrate that DCD\nsignificantly enhances LLM performance across a range of reasoning benchmarks,\nsurpassing both CD and existing methods in the GSM8K and StrategyQA datasets.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14874v1",
    "authors": ["Phuc Phan", "Hieu Tran", "Long Phan"]
  },
  {
    "id": "2402.14875",
    "title": "What's in a Name? Auditing Large Language Models for Race and Gender\n  Bias",
    "abstract": "  We employ an audit design to investigate biases in state-of-the-art large\nlanguage models, including GPT-4. In our study, we prompt the models for advice\ninvolving a named individual across a variety of scenarios, such as during car\npurchase negotiations or election outcome predictions. We find that the advice\nsystematically disadvantages names that are commonly associated with racial\nminorities and women. Names associated with Black women receive the least\nadvantageous outcomes. The biases are consistent across 42 prompt templates and\nseveral models, indicating a systemic issue rather than isolated incidents.\nWhile providing numerical, decision-relevant anchors in the prompt can\nsuccessfully counteract the biases, qualitative details have inconsistent\neffects and may even increase disparities. Our findings underscore the\nimportance of conducting audits at the point of LLM deployment and\nimplementation to mitigate their potential for harm against marginalized\ncommunities.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14875v2",
    "authors": ["Amit Haim", "Alejandro Salinas", "Julian Nyarko"]
  },
  {
    "id": "2402.14878",
    "title": "Energy-efficiency Limits on Training AI Systems using Learning-in-Memory",
    "abstract": "  Learning-in-memory (LIM) is a recently proposed paradigm to overcome\nfundamental memory bottlenecks in training machine learning systems. While\ncompute-in-memory (CIM) approaches can address the so-called memory-wall (i.e.\nenergy dissipated due to repeated memory read access) they are agnostic to the\nenergy dissipated due to repeated memory writes at the precision required for\ntraining (the update-wall), and they don't account for the energy dissipated\nwhen transferring information between short-term and long-term memories (the\nconsolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can\nbe overcome if the energy barrier of physical memories is adaptively modulated\nsuch that the dynamics of memory updates and consolidation match the Lyapunov\ndynamics of gradient-descent training of an AI model. In this paper, we derive\nnew theoretical lower bounds on energy dissipation when training AI systems\nusing different LIM approaches. The analysis presented here is model-agnostic\nand highlights the trade-off between energy efficiency and the speed of\ntraining. The resulting non-equilibrium energy-efficiency bounds have a similar\nflavor as that of Landauer's energy-dissipation bounds. We also extend these\nlimits by taking into account the number of floating-point operations (FLOPs)\nused for training, the size of the AI model, and the precision of the training\nparameters. Our projections suggest that the energy-dissipation lower-bound to\ntrain a brain scale AI system (comprising of $10^{15}$ parameters) using LIM is\n$10^8 \\sim 10^9$ Joules, which is on the same magnitude the Landauer's\nadiabatic lower-bound and $6$ to $7$ orders of magnitude lower than the\nprojections obtained using state-of-the-art AI accelerator hardware\nlower-bounds.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14878v1",
    "authors": ["Zihao Chen", "Johannes Leugering", "Gert Cauwenberghs", "Shantanu Chakrabartty"]
  },
  {
    "id": "2402.14880",
    "title": "Automatic Histograms: Leveraging Language Models for Text Dataset\n  Exploration",
    "abstract": "  Making sense of unstructured text datasets is perennially difficult, yet\nincreasingly relevant with Large Language Models. Data workers often rely on\ndataset summaries, especially distributions of various derived features. Some\nfeatures, like toxicity or topics, are relevant to many datasets, but many\ninteresting features are domain specific: instruments and genres for a music\ndataset, or diseases and symptoms for a medical dataset. Accordingly, data\nworkers often run custom analyses for each dataset, which is cumbersome and\ndifficult. We present AutoHistograms, a visualization tool leveragingLLMs.\nAutoHistograms automatically identifies relevant features, visualizes them with\nhistograms, and allows the user to interactively query the dataset for\ncategories of entities and create new histograms. In a user study with 10 data\nworkers (n=10), we observe that participants can quickly identify insights and\nexplore the data using AutoHistograms, and conceptualize a broad range of\napplicable use cases. Together, this tool and user study contributeto the\ngrowing field of LLM-assisted sensemaking tools.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14880v1",
    "authors": ["Emily Reif", "Crystal Qian", "James Wexler", "Minsuk Kahng"]
  },
  {
    "id": "2402.14881",
    "title": "A Study on the Vulnerability of Test Questions against ChatGPT-based\n  Cheating",
    "abstract": "  ChatGPT is a chatbot that can answer text prompts fairly accurately, even\nperforming very well on postgraduate-level questions. Many educators have found\nthat their take-home or remote tests and exams are vulnerable to ChatGPT-based\ncheating because students may directly use answers provided by tools like\nChatGPT. In this paper, we try to provide an answer to an important question:\nhow well ChatGPT can answer test questions and how we can detect whether the\nquestions of a test can be answered correctly by ChatGPT. We generated\nChatGPT's responses to the MedMCQA dataset, which contains over 10,000 medical\nschool entrance exam questions. We analyzed the responses and uncovered certain\ntypes of questions ChatGPT answers more inaccurately than others. In addition,\nwe have created a basic natural language processing model to single out the\nmost vulnerable questions to ChatGPT in a collection of questions or a sample\nexam. Our tool can be used by test-makers to avoid ChatGPT-vulnerable test\nquestions.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14881v1",
    "authors": ["Shanker Ram", "Chen Qian"]
  },
  {
    "id": "2402.15527",
    "title": "PCA-Bench: Evaluating Multimodal Large Language Models in\n  Perception-Cognition-Action Chain",
    "abstract": "  We present PCA-Bench, a multimodal decision-making benchmark for evaluating\nthe integrated capabilities of Multimodal Large Language Models (MLLMs).\nDeparting from previous benchmarks focusing on simplistic tasks and individual\nmodel capability, PCA-Bench introduces three complex scenarios: autonomous\ndriving, domestic robotics, and open-world games. Given task instructions and\ndiverse contexts, the model is required to seamlessly integrate multiple\ncapabilities of Perception, Cognition, and Action in a reasoning chain to make\naccurate decisions. Moreover, PCA-Bench features error localization\ncapabilities, scrutinizing model inaccuracies in areas such as perception,\nknowledge, or reasoning. This enhances the reliability of deploying MLLMs. To\nbalance accuracy and efficiency in evaluation, we propose PCA-Eval, an\nautomatic evaluation protocol, and assess 10 prevalent MLLMs. The results\nreveal significant performance disparities between open-source models and\npowerful proprietary models like GPT-4 Vision. To address this, we introduce\nEmbodied-Instruction-Evolution (EIE), an automatic framework for synthesizing\ninstruction tuning examples in multimodal embodied environments. EIE generates\n7,510 training examples in PCA-Bench and enhances the performance of\nopen-source MLLMs, occasionally surpassing GPT-4 Vision (+3\\% in decision\naccuracy), thereby validating the effectiveness of EIE. Our findings suggest\nthat robust MLLMs like GPT4-Vision show promise for decision-making in embodied\nagents, opening new avenues for MLLM research.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.15527v1",
    "authors": [
      "Liang Chen",
      "Yichi Zhang",
      "Shuhuai Ren",
      "Haozhe Zhao",
      "Zefan Cai",
      "Yuchi Wang",
      "Peiyi Wang",
      "Xiangdi Meng",
      "Tianyu Liu",
      "Baobao Chang"
    ]
  },
  {
    "id": "2402.16611",
    "title": "Understanding the Dataset Practitioners Behind Large Language Model\n  Development",
    "abstract": "  As large language models (LLMs) become more advanced and impactful, it is\nincreasingly important to scrutinize the data that they rely upon and produce.\nWhat is it to be a dataset practitioner doing this work? We approach this in\ntwo parts: first, we define the role of \"dataset practitioner\" by performing a\nretrospective analysis on the responsibilities of teams contributing to LLM\ndevelopment at Google. Then, we conduct semi-structured interviews with a\ncross-section of these practitioners (N=10). We find that data quality is the\ntop priority. To evaluate data quality, practitioners either rely on their own\nintuition or write custom evaluation logic. There is a lack of consensus across\npractitioners on what quality is and how to evaluate it. We discuss potential\nreasons for this phenomenon and opportunities for alignment.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.16611v1",
    "authors": ["Crystal Qian", "Emily Reif", "Minsuk Kahng"]
  },
  {
    "id": "2402.14174",
    "title": "Blending Data-Driven Priors in Dynamic Games",
    "abstract": "  As intelligent robots like autonomous vehicles become increasingly deployed\nin the presence of people, the extent to which these systems should leverage\nmodel-based game-theoretic planners versus data-driven policies for safe,\ninteraction-aware motion planning remains an open question. Existing dynamic\ngame formulations assume all agents are task-driven and behave optimally.\nHowever, in reality, humans tend to deviate from the decisions prescribed by\nthese models, and their behavior is better approximated under a noisy-rational\nparadigm. In this work, we investigate a principled methodology to blend a\ndata-driven reference policy with an optimization-based game-theoretic policy.\nWe formulate KLGame, a type of non-cooperative dynamic game with\nKullback-Leibler (KL) regularization with respect to a general, stochastic, and\npossibly multi-modal reference policy. Our method incorporates, for each\ndecision maker, a tunable parameter that permits modulation between task-driven\nand data-driven behaviors. We propose an efficient algorithm for computing\nmultimodal approximate feedback Nash equilibrium strategies of KLGame in real\ntime. Through a series of simulated and real-world autonomous driving\nscenarios, we demonstrate that KLGame policies can more effectively incorporate\nguidance from the reference policy and account for noisily-rational human\nbehaviors versus non-regularized baselines.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.14174v2",
    "authors": [
      "Justin Lidard",
      "Haimin Hu",
      "Asher Hancock",
      "Zixu Zhang",
      "Albert Gimó Contreras",
      "Vikash Modi",
      "Jonathan DeCastro",
      "Deepak Gopinath",
      "Guy Rosman",
      "Naomi Leonard",
      "María Santos",
      "Jaime Fernández Fisac"
    ]
  },
  {
    "id": "2402.13820",
    "title": "FLD: Fourier Latent Dynamics for Structured Motion Representation and\n  Learning",
    "abstract": "  Motion trajectories offer reliable references for physics-based motion\nlearning but suffer from sparsity, particularly in regions that lack sufficient\ndata coverage. To address this challenge, we introduce a self-supervised,\nstructured representation and generation method that extracts spatial-temporal\nrelationships in periodic or quasi-periodic motions. The motion dynamics in a\ncontinuously parameterized latent space enable our method to enhance the\ninterpolation and generalization capabilities of motion learning algorithms.\nThe motion learning controller, informed by the motion parameterization,\noperates online tracking of a wide range of motions, including targets unseen\nduring training. With a fallback mechanism, the controller dynamically adapts\nits tracking strategy and automatically resorts to safe action execution when a\npotentially risky target is proposed. By leveraging the identified\nspatial-temporal structure, our work opens new possibilities for future\nadvancements in general motion representation and learning algorithms.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13820v1",
    "authors": ["Chenhao Li", "Elijah Stanger-Jones", "Steve Heim", "Sangbae Kim"]
  },
  {
    "id": "2402.13852",
    "title": "Neural Control System for Continuous Glucose Monitoring and Maintenance",
    "abstract": "  Precise glucose level management is pivotal for individuals with diabetes,\naverting severe complications. In this work, we introduce a novel neural\ncontrol system for continuous glucose monitoring and maintenance, utilizing\ndifferential predictive control. Our system, guided by a sophisticated neural\npolicy and differentiable modeling, dynamically adjusts insulin delivery in\nreal-time, enhancing glucose optimization. This end-to-end approach maximizes\nefficiency, ensuring personalized care and improved health outcomes, as\naffirmed by empirical findings.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.13852v1",
    "authors": ["Azmine Toushik Wasi"]
  }
]
