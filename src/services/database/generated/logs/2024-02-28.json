[
  {
    "id": "2403.00833",
    "title": "Position Paper: Agent AI Towards a Holistic Intelligence",
    "abstract": "  Recent advancements in large foundation models have remarkably enhanced our\nunderstanding of sensory information in open-world environments. In leveraging\nthe power of foundation models, it is crucial for AI research to pivot away\nfrom excessive reductionism and toward an emphasis on systems that function as\ncohesive wholes. Specifically, we emphasize developing Agent AI -- an embodied\nsystem that integrates large foundation models into agent actions. The emerging\nfield of Agent AI spans a wide range of existing embodied and agent-based\nmultimodal interactions, including robotics, gaming, and healthcare systems,\netc. In this paper, we propose a novel large action model to achieve embodied\nintelligent behavior, the Agent Foundation Model. On top of this idea, we\ndiscuss how agent AI exhibits remarkable capabilities across a variety of\ndomains and tasks, challenging our understanding of learning and cognition.\nFurthermore, we discuss the potential of Agent AI from an interdisciplinary\nperspective, underscoring AI cognition and consciousness within scientific\ndiscourse. We believe that those discussions serve as a basis for future\nresearch directions and encourage broader societal engagement.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00833v1",
    "authors": [
      "Qiuyuan Huang",
      "Naoki Wake",
      "Bidipta Sarkar",
      "Zane Durante",
      "Ran Gong",
      "Rohan Taori",
      "Yusuke Noda",
      "Demetri Terzopoulos",
      "Noboru Kuno",
      "Ade Famoti",
      "Ashley Llorens",
      "John Langford",
      "Hoi Vo",
      "Li Fei-Fei",
      "Katsu Ikeuchi",
      "Jianfeng Gao"
    ]
  },
  {
    "id": "2402.17969",
    "title": "Vision Language Model-based Caption Evaluation Method Leveraging Visual\n  Context Extraction",
    "abstract": "  Given the accelerating progress of vision and language modeling, accurate\nevaluation of machine-generated image captions remains critical. In order to\nevaluate captions more closely to human preferences, metrics need to\ndiscriminate between captions of varying quality and content. However,\nconventional metrics fail short of comparing beyond superficial matches of\nwords or embedding similarities; thus, they still need improvement. This paper\npresents VisCE$^2$, a vision language model-based caption evaluation method.\nOur method focuses on visual context, which refers to the detailed content of\nimages, including objects, attributes, and relationships. By extracting and\norganizing them into a structured format, we replace the human-written\nreferences with visual contexts and help VLMs better understand the image,\nenhancing evaluation performance. Through meta-evaluation on multiple datasets,\nwe validated that VisCE$^2$ outperforms the conventional pre-trained metrics in\ncapturing caption quality and demonstrates superior consistency with human\njudgment.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.17969v1",
    "authors": ["Koki Maeda", "Shuhei Kurita", "Taiki Miyanishi", "Naoaki Okazaki"]
  },
  {
    "id": "2402.17975",
    "title": "Sample-Efficient Preference-based Reinforcement Learning with Dynamics\n  Aware Rewards",
    "abstract": "  Preference-based reinforcement learning (PbRL) aligns a robot behavior with\nhuman preferences via a reward function learned from binary feedback over agent\nbehaviors. We show that dynamics-aware reward functions improve the sample\nefficiency of PbRL by an order of magnitude. In our experiments we iterate\nbetween: (1) learning a dynamics-aware state-action representation (z^{sa}) via\na self-supervised temporal consistency task, and (2) bootstrapping the\npreference-based reward function from (z^{sa}), which results in faster policy\nlearning and better final policy performance. For example, on quadruped-walk,\nwalker-walk, and cheetah-run, with 50 preference labels we achieve the same\nperformance as existing approaches with 500 preference labels, and we recover\n83\\% and 66\\% of ground truth reward policy performance versus only 38\\% and\n21\\%. The performance gains demonstrate the benefits of explicitly learning a\ndynamics-aware reward model. Repo: \\texttt{https://github.com/apple/ml-reed}.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.17975v1",
    "authors": ["Katherine Metcalf", "Miguel Sarabia", "Natalie Mackraz", "Barry-John Theobald"]
  },
  {
    "id": "2402.18005",
    "title": "Exploring Multi-Document Information Consolidation for Scientific\n  Sentiment Summarization",
    "abstract": "  Modern natural language generation systems with LLMs exhibit the capability\nto generate a plausible summary of multiple documents; however, it is uncertain\nif models truly possess the ability of information consolidation to generate\nsummaries, especially on those source documents with opinionated information.\nTo make scientific sentiment summarization more grounded, we hypothesize that\nin peer review human meta-reviewers follow a three-layer framework of sentiment\nconsolidation to write meta-reviews and it represents the logic of summarizing\nscientific sentiments in meta-review generation. The framework is validated via\nhuman annotation. Based on the framework, we propose evaluation metrics to\nassess the quality of generated meta-reviews, and we find that the hypothesis\nof the sentiment consolidation framework works out empirically when we\nincorporate it as prompts for LLMs to generate meta-reviews in extensive\nexperiments.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18005v1",
    "authors": ["Miao Li", "Jey Han Lau", "Eduard Hovy"]
  },
  {
    "id": "2402.18012",
    "title": "Diffusion Models as Constrained Samplers for Optimization with Unknown\n  Constraints",
    "abstract": "  Addressing real-world optimization problems becomes particularly challenging\nwhen analytic objective functions or constraints are unavailable. While\nnumerous studies have addressed the issue of unknown objectives, limited\nresearch has focused on scenarios where feasibility constraints are not given\nexplicitly. Overlooking these constraints can lead to spurious solutions that\nare unrealistic in practice. To deal with such unknown constraints, we propose\nto perform optimization within the data manifold using diffusion models. To\nconstrain the optimization process to the data manifold, we reformulate the\noriginal optimization problem as a sampling problem from the product of the\nBoltzmann distribution defined by the objective function and the data\ndistribution learned by the diffusion model. To enhance sampling efficiency, we\npropose a two-stage framework that begins with a guided diffusion process for\nwarm-up, followed by a Langevin dynamics stage for further correction.\nTheoretical analysis shows that the initial stage results in a distribution\nfocused on feasible solutions, thereby providing a better initialization for\nthe later stage. Comprehensive experiments on a synthetic dataset, six\nreal-world black-box optimization datasets, and a multi-objective optimization\ndataset show that our method achieves better or comparable performance with\nprevious state-of-the-art baselines.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18012v1",
    "authors": [
      "Lingkai Kong",
      "Yuanqi Du",
      "Wenhao Mu",
      "Kirill Neklyudov",
      "Valentin De Bortol",
      "Haorui Wang",
      "Dongxia Wu",
      "Aaron Ferber",
      "Yi-An Ma",
      "Carla P. Gomes",
      "Chao Zhang"
    ]
  },
  {
    "id": "2402.18013",
    "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
    "abstract": "  This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18013v1",
    "authors": ["Zihao Yi", "Jiarui Ouyang", "Yuwen Liu", "Tianhao Liao", "Zhe Xu", "Ying Shen"]
  },
  {
    "id": "2402.18016",
    "title": "Dynamic Explanation Selection Towards Successful User-Decision Support\n  with Explainable AI",
    "abstract": "  This paper addresses the problem of how to select explanations for XAI\n(Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have\nshown promise in improving user decisions through XAI-generated explanations\nalong with AI predictions. As the development of XAI made various explanations\navailable, we believe that IDSSs can be greatly improved if they can\nstrategically select explanations that guide users to better decisions. This\npaper proposes X-Selector, a method for dynamically selecting explanations.\nX-Selector aims to guide users to better decisions by predicting the impact of\ndifferent combinations of explanations on user decisions. We compared\nX-Selector's performance with two naive strategies (all possible explanations\nand explanations only for the most likely prediction) and two baselines (no\nexplanation and no AI support). The results suggest the potential of X-Selector\nto guide users to recommended decisions and improve the performance when AI\naccuracy is high and a challenge when it is low.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18016v1",
    "authors": ["Yosuke Fukuchi", "Seiji Yamada"]
  },
  {
    "id": "2402.18023",
    "title": "Do Large Language Models Mirror Cognitive Language Processing?",
    "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities in\ntext comprehension and logical reasoning, achiving or even surpassing\nhuman-level performance in numerous cognition tasks. As LLMs are trained from\nmassive textual outputs of human language cognition, it is natural to ask\nwhether LLMs mirror cognitive language processing. Or to what extend LLMs\nresemble cognitive language processing? In this paper, we propose a novel\nmethod that bridge between LLM representations and human cognition signals to\nevaluate how effectively LLMs simulate cognitive language processing. We employ\nRepresentational Similarity Analysis (RSA) to mearsure the alignment between 16\nmainstream LLMs and fMRI signals of the brain. We empirically investigate the\nimpact of a variety of factors (e.g., model scaling, alignment training,\ninstruction appending) on such LLM-brain alignment. Experimental results\nindicate that model scaling is positively correlated with LLM-brain similarity,\nand alignment training can significantly improve LLM-brain similarity.\nAdditionally, the performance of a wide range of LLM evaluations (e.g., MMLU,\nChatbot Arena) is highly correlated with the LLM-brain similarity.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18023v1",
    "authors": ["Yuqi Ren", "Renren Jin", "Tongxuan Zhang", "Deyi Xiong"]
  },
  {
    "id": "2402.18039",
    "title": "ResLoRA: Identity Residual Mapping in Low-Rank Adaption",
    "abstract": "  As one of the most popular parameter-efficient fine-tuning (PEFT) methods,\nlow-rank adaptation (LoRA) is commonly applied to fine-tune large language\nmodels (LLMs). However, updating the weights of LoRA blocks effectively and\nexpeditiously is challenging due to the long calculation path in the original\nmodel. To address this, we propose ResLoRA, an improved framework of LoRA. By\nadding residual paths during training and using merging approaches to eliminate\nthese extra paths during inference, our method can achieve better results in\nfewer training steps without any extra trainable parameters or inference cost\ncompared to LoRA. The experiments on NLG, NLU, and text-to-image tasks\ndemonstrate the effectiveness of our method. To the best of our knowledge,\nResLoRA is the first work that combines the residual path with LoRA. The code\nof our method is available at\nhttps://github.com/microsoft/LMOps/tree/main/reslora .\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18039v1",
    "authors": [
      "Shuhua Shi",
      "Shaohan Huang",
      "Minghui Song",
      "Zhoujun Li",
      "Zihan Zhang",
      "Haizhen Huang",
      "Furu Wei",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang"
    ]
  },
  {
    "id": "2402.18040",
    "title": "Automated Discovery of Integral with Deep Learning",
    "abstract": "  Recent advancements in the realm of deep learning, particularly in the\ndevelopment of large language models (LLMs), have demonstrated AI's ability to\ntackle complex mathematical problems or solving programming challenges.\nHowever, the capability to solve well-defined problems based on extensive\ntraining data differs significantly from the nuanced process of making\nscientific discoveries. Trained on almost all human knowledge available,\ntoday's sophisticated LLMs basically learn to predict sequences of tokens. They\ngenerate mathematical derivations and write code in a similar way as writing an\nessay, and do not have the ability to pioneer scientific discoveries in the\nmanner a human scientist would do.\n  In this study we delve into the potential of using deep learning to\nrediscover a fundamental mathematical concept: integrals. By defining integrals\nas area under the curve, we illustrate how AI can deduce the integral of a\ngiven function, exemplified by inferring $\\int_{0}^{x} t^2 dt = \\frac{x^3}{3}$\nand $\\int_{0}^{x} ae^{bt} dt = \\frac{a}{b} e^{bx} - \\frac{a}{b}$. Our\nexperiments show that deep learning models can approach the task of inferring\nintegrals either through a sequence-to-sequence model, akin to language\ntranslation, or by uncovering the rudimentary principles of integration, such\nas $\\int_{0}^{x} t^n dt = \\frac{x^{n+1}}{n+1}$.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18040v1",
    "authors": ["Xiaoxin Yin"]
  },
  {
    "id": "2402.18041",
    "title": "Datasets for Large Language Models: A Comprehensive Survey",
    "abstract": "  This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18041v1",
    "authors": ["Yang Liu", "Jiahuan Cao", "Chongyu Liu", "Kai Ding", "Lianwen Jin"]
  },
  {
    "id": "2402.18061",
    "title": "On the use of Silver Standard Data for Zero-shot Classification Tasks in\n  Information Extraction",
    "abstract": "  The superior performance of supervised classification methods in the\ninformation extraction (IE) area heavily relies on a large amount of gold\nstandard data. Recent zero-shot classification methods converted the task to\nother NLP tasks (e.g., textual entailment) and used off-the-shelf models of\nthese NLP tasks to directly perform inference on the test data without using a\nlarge amount of IE annotation data. A potentially valuable by-product of these\nmethods is the large-scale silver standard data, i.e., pseudo-labeled data by\nthe off-the-shelf models of other NLP tasks. However, there is no further\ninvestigation into the use of these data. In this paper, we propose a new\nframework, Clean-LaVe, which aims to utilize silver standard data to enhance\nthe zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining\nsilver data; (2) Identifying relatively clean data from silver data; (3)\nFinetuning the off-the-shelf model using clean data; (4) Inference on the test\ndata. The experimental results show that Clean-LaVe can outperform the baseline\nby 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation\nclassification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot\ncross-lingual relation classification task, and by 8% on ACE05-E+ in the\nzero-shot event argument classification task. The code is share in\nhttps://github.com/wjw136/Clean_LaVe.git.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18061v1",
    "authors": ["Jianwei Wang", "Tianyin Wang", "Ziqian Zeng"]
  },
  {
    "id": "2402.18062",
    "title": "Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and\n  Opportunities",
    "abstract": "  With recent advances in artificial intelligence (AI) and robotics, unmanned\nvehicle swarms have received great attention from both academia and industry\ndue to their potential to provide services that are difficult and dangerous to\nperform by humans. However, learning and coordinating movements and actions for\na large number of unmanned vehicles in complex and dynamic environments\nintroduce significant challenges to conventional AI methods. Generative AI\n(GAI), with its capabilities in complex data feature extraction,\ntransformation, and enhancement, offers great potential in solving these\nchallenges of unmanned vehicle swarms. For that, this paper aims to provide a\ncomprehensive survey on applications, challenges, and opportunities of GAI in\nunmanned vehicle swarms. Specifically, we first present an overview of unmanned\nvehicles and unmanned vehicle swarms as well as their use cases and existing\nissues. Then, an in-depth background of various GAI techniques together with\ntheir capabilities in enhancing unmanned vehicle swarms are provided. After\nthat, we present a comprehensive review on the applications and challenges of\nGAI in unmanned vehicle swarms with various insights and discussions. Finally,\nwe highlight open issues of GAI in unmanned vehicle swarms and discuss\npotential research directions.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18062v1",
    "authors": [
      "Guangyuan Liu",
      "Nguyen Van Huynh",
      "Hongyang Du",
      "Dinh Thai Hoang",
      "Dusit Niyato",
      "Kun Zhu",
      "Jiawen Kang",
      "Zehui Xiong",
      "Abbas Jamalipour",
      "Dong In Kim"
    ]
  },
  {
    "id": "2402.18096",
    "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware\n  Mixed Precision Quantization",
    "abstract": "  Key-Value (KV) Caching has become an essential technique for accelerating the\ninference speed and throughput of generative Large Language Models~(LLMs).\nHowever, the memory footprint of the KV cache poses a critical bottleneck in\nLLM deployment as the cache size grows with batch size and sequence length,\noften surpassing even the size of the model itself. Although recent methods\nwere proposed to select and evict unimportant KV pairs from the cache to reduce\nmemory consumption, the potential ramifications of eviction on the generative\nprocess are yet to be thoroughly examined. In this paper, we examine the\ndetrimental impact of cache eviction and observe that unforeseen risks arise as\nthe information contained in the KV pairs is exhaustively discarded, resulting\nin safety breaches, hallucinations, and context loss. Surprisingly, we find\nthat preserving even a small amount of information contained in the evicted KV\npairs via reduced precision quantization substantially recovers the incurred\ndegradation. On the other hand, we observe that the important KV pairs must be\nkept at a relatively higher precision to safeguard the generation quality.\nMotivated by these observations, we propose \\textit{Mixed-precision KV\ncache}~(MiKV), a reliable cache compression method that simultaneously\npreserves the context details by retaining the evicted KV pairs in\nlow-precision and ensure generation quality by keeping the important KV pairs\nin high-precision. Experiments on diverse benchmarks and LLM backbones show\nthat our proposed method offers a state-of-the-art trade-off between\ncompression ratio and performance, compared to other baselines.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18096v1",
    "authors": [
      "June Yong Yang",
      "Byeongwook Kim",
      "Jeongin Bae",
      "Beomseok Kwon",
      "Gunho Park",
      "Eunho Yang",
      "Se Jung Kwon",
      "Dongsoo Lee"
    ]
  },
  {
    "id": "2402.18099",
    "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large\n  Language Models",
    "abstract": "  Model editing aims to precisely modify the behaviours of large language\nmodels (LLMs) on specific knowledge while keeping irrelevant knowledge\nunchanged. It has been proven effective in resolving hallucination and\nout-of-date issues in LLMs. As a result, it can boost the application of LLMs\nin many critical domains (e.g., medical domain), where the hallucination is not\ntolerable. In this paper, we propose two model editing studies and validate\nthem in the medical domain: (1) directly editing the factual medical knowledge\nand (2) editing the explanations to facts. Meanwhile, we observed that current\nmodel editing methods struggle with the specialization and complexity of\nmedical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable\nAdapter strategy for medical model editing. It employs causal tracing to\nidentify the precise location of knowledge in neurons and then introduces\nscalable adapters into the dense layers of LLMs. These adapters are assigned\nscaling values based on the corresponding specific knowledge. To evaluate the\nediting impact, we build two benchmark datasets and introduce a series of\nchallenging and comprehensive metrics. Extensive experiments on medical LLMs\ndemonstrate the editing efficiency of MedLaSA, without affecting irrelevant\nknowledge that is not edited.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18099v1",
    "authors": [
      "Derong Xu",
      "Ziheng Zhang",
      "Zhihong Zhu",
      "Zhenxi Lin",
      "Qidong Liu",
      "Xian Wu",
      "Tong Xu",
      "Xiangyu Zhao",
      "Yefeng Zheng",
      "Enhong Chen"
    ]
  },
  {
    "id": "2402.18104",
    "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few\n  Queries via Disguise and Reconstruction",
    "abstract": "  In recent years, large language models (LLMs) have demonstrated notable\nsuccess across various tasks, but the trustworthiness of LLMs is still an open\nproblem. One specific threat is the potential to generate toxic or harmful\nresponses. Attackers can craft adversarial prompts that induce harmful\nresponses from LLMs. In this work, we pioneer a theoretical foundation in LLMs\nsecurity by identifying bias vulnerabilities within the safety fine-tuning and\ndesign a black-box jailbreak method named DRA (Disguise and Reconstruction\nAttack), which conceals harmful instructions through disguise and prompts the\nmodel to reconstruct the original harmful instruction within its completion. We\nevaluate DRA across various open-source and close-source models, showcasing\nstate-of-the-art jailbreak success rates and attack efficiency. Notably, DRA\nboasts a 90\\% attack success rate on LLM chatbots GPT-4.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18104v1",
    "authors": ["Tong Liu", "Yingjie Zhang", "Zhe Zhao", "Yinpeng Dong", "Guozhu Meng", "Kai Chen"]
  },
  {
    "id": "2402.18113",
    "title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation",
    "abstract": "  The emergence of Large Language Models (LLMs) has brought to light promising\nlanguage generation capabilities, particularly in performing tasks like complex\nreasoning and creative writing. Consequently, distillation through imitation of\nteacher responses has emerged as a popular technique to transfer knowledge from\nLLMs to more accessible, Small Language Models (SLMs). While this works well\nfor simpler tasks, there is a substantial performance gap on tasks requiring\nintricate language comprehension and creativity, such as humor generation. We\nhypothesize that this gap may stem from the fact that creative tasks might be\nhard to learn by imitation alone and explore whether an approach, involving\nsupplementary guidance from the teacher, could yield higher performance. To\naddress this, we study the effect of assigning a dual role to the LLM - as a\n\"teacher\" generating data, as well as a \"critic\" evaluating the student's\nperformance. Our experiments on humor generation reveal that the incorporation\nof feedback significantly narrows the performance gap between SLMs and their\nlarger counterparts compared to merely relying on imitation. As a result, our\nresearch highlights the potential of using feedback as an additional dimension\nto data when transferring complex language abilities via distillation.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18113v1",
    "authors": [
      "Sahithya Ravi",
      "Patrick Huber",
      "Akshat Shrivastava",
      "Aditya Sagar",
      "Ahmed Aly",
      "Vered Shwartz",
      "Arash Einolghozati"
    ]
  },
  {
    "id": "2402.18121",
    "title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for\n  Aminoacian",
    "abstract": "  This study assesses four cutting-edge language models in the underexplored\nAminoacian language. Through evaluation, it scrutinizes their adaptability,\neffectiveness, and limitations in text generation, semantic coherence, and\ncontextual understanding. Uncovering insights into these models' performance in\na low-resourced language, this research pioneers pathways to bridge linguistic\ngaps. By offering benchmarks and understanding challenges, it lays groundwork\nfor future advancements in natural language processing, aiming to elevate the\napplicability of language models in similar linguistic landscapes, marking a\nsignificant step toward inclusivity and progress in language technology.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18121v1",
    "authors": ["Yunze Xiao", "Yiyang Pan"]
  },
  {
    "id": "2402.18139",
    "title": "Cause and Effect: Can Large Language Models Truly Understand Causality?",
    "abstract": "  With the rise of Large Language Models(LLMs), it has become crucial to\nunderstand their capabilities and limitations in deciphering and explaining the\ncomplex web of causal relationships that language entails. Current methods use\neither explicit or implicit causal reasoning, yet there is a strong need for a\nunified approach combining both to tackle a wide array of causal relationships\nmore effectively. This research proposes a novel architecture called Context\nAware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to\nenhance causal reasoning and explainability. The proposed framework\nincorporates an explicit causal detection module with ConceptNet and\ncounterfactual statements, as well as implicit causal detection through LLMs.\nOur framework goes one step further with a layer of counterfactual explanations\nto accentuate LLMs understanding of causality. The knowledge from ConceptNet\nenhances the performance of multiple causal reasoning tasks such as causal\ndiscovery, causal identification and counterfactual reasoning. The\ncounterfactual sentences add explicit knowledge of the not caused by scenarios.\nBy combining these powerful modules, our model aims to provide a deeper\nunderstanding of causal relationships, enabling enhanced interpretability.\nEvaluation of benchmark datasets shows improved performance across all metrics,\nsuch as accuracy, precision, recall, and F1 scores. We also introduce\nCausalNet, a new dataset accompanied by our code, to facilitate further\nresearch in this domain.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18139v1",
    "authors": [
      "Swagata Ashwani",
      "Kshiteesh Hegde",
      "Nishith Reddy Mannuru",
      "Mayank Jindal",
      "Dushyant Singh Sengar",
      "Krishna Chaitanya Rao Kathala",
      "Dishant Banga",
      "Vinija Jain",
      "Aman Chadha"
    ]
  },
  {
    "id": "2402.18144",
    "title": "Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a\n  Large Language Model Based on Group-Level Demographic Information",
    "abstract": "  Large language models exhibit societal biases associated with demographic\ninformation, including race, gender, and others. Endowing such language models\nwith personalities based on demographic data can enable generating opinions\nthat align with those of humans. Building on this idea, we propose \"random\nsilicon sampling,\" a method to emulate the opinions of the human population\nsub-group. Our study analyzed 1) a language model that generates the survey\nresponses that correspond with a human group based solely on its demographic\ndistribution and 2) the applicability of our methodology across various\ndemographic subgroups and thematic questions. Through random silicon sampling\nand using only group-level demographic information, we discovered that language\nmodels can generate response distributions that are remarkably similar to the\nactual U.S. public opinion polls. Moreover, we found that the replicability of\nlanguage models varies depending on the demographic group and topic of the\nquestion, and this can be attributed to inherent societal biases in the models.\nOur findings demonstrate the feasibility of mirroring a group's opinion using\nonly demographic distribution and elucidate the effect of social biases in\nlanguage models on such simulations.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18144v1",
    "authors": [
      "Seungjong Sun",
      "Eungu Lee",
      "Dongyan Nan",
      "Xiangying Zhao",
      "Wonbyung Lee",
      "Bernard J. Jansen",
      "Jang Hyun Kim"
    ]
  },
  {
    "id": "2402.18153",
    "title": "Diffusion-based Neural Network Weights Generation",
    "abstract": "  Transfer learning is a topic of significant interest in recent deep learning\nresearch because it enables faster convergence and improved performance on new\ntasks. While the performance of transfer learning depends on the similarity of\nthe source data to the target data, it is costly to train a model on a large\nnumber of datasets. Therefore, pretrained models are generally blindly selected\nwith the hope that they will achieve good performance on the given task. To\ntackle such suboptimality of the pretrained models, we propose an efficient and\nadaptive transfer learning scheme through dataset-conditioned pretrained\nweights sampling. Specifically, we use a latent diffusion model with a\nvariational autoencoder that can reconstruct the neural network weights, to\nlearn the distribution of a set of pretrained weights conditioned on each\ndataset for transfer learning on unseen datasets. By learning the distribution\nof a neural network on a variety pretrained models, our approach enables\nadaptive sampling weights for unseen datasets achieving faster convergence and\nreaching competitive performance.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18153v1",
    "authors": [
      "Bedionita Soro",
      "Bruno Andreis",
      "Hayeon Lee",
      "Song Chong",
      "Frank Hutter",
      "Sung Ju Hwang"
    ]
  },
  {
    "id": "2402.18158",
    "title": "Evaluating Quantized Large Language Models",
    "abstract": "  Post-training quantization (PTQ) has emerged as a promising technique to\nreduce the cost of large language models (LLMs). Specifically, PTQ can\neffectively mitigate memory consumption and reduce computational overhead in\nLLMs. To meet the requirements of both high efficiency and performance across\ndiverse scenarios, a comprehensive evaluation of quantized LLMs is essential to\nguide the selection of quantization methods. This paper presents a thorough\nevaluation of these factors by evaluating the effect of PTQ on Weight,\nActivation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon,\nBloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with\nparameters ranging from 125M to 180B. The evaluation encompasses five types of\ntasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context\ntasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization\nmethods to demonstrate their applicability. Based on the extensive experiments,\nwe systematically summarize the effect of quantization, provide recommendations\nto apply quantization techniques, and point out future directions.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18158v1",
    "authors": [
      "Shiyao Li",
      "Xuefei Ning",
      "Luning Wang",
      "Tengxuan Liu",
      "Xiangsheng Shi",
      "Shengen Yan",
      "Guohao Dai",
      "Huazhong Yang",
      "Yu Wang"
    ]
  },
  {
    "id": "2402.18164",
    "title": "Autoencoder-based General Purpose Representation Learning for Customer\n  Embedding",
    "abstract": "  In recent years, exploiting the domain-specific underlying structure of data\nand its generative factors for representation learning has shown success in\nvarious use-case agnostic applications. However, the diversity and complexity\nof tabular data have made it challenging to represent these structures in a\nlatent space through multi-dimensional vectors. We design an autoencoder-based\nframework for building general purpose embeddings, we assess the performance of\ndifferent autoencoder architectures, and show simpler models outperform complex\nones in embedding highly complex tabular data. We apply our framework to\nproduce plug-and-play, rich, and anonymized embeddings representing AWS\ncustomers for usage in any model, saving up to 45% of development time, and\nobserve significant improvements in downstream models. Moreover, we propose a\nsignificant improvement to the calculation of reconstruction loss for\nmulti-layer contractive autoencoders (CAE) by calculating the Jacobian of the\nentire encoder leading to a 15% improvement in reconstruction quality when\ncompared to a stacked CAE.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18164v1",
    "authors": ["Jan Henrik Bertrand", "Jacopo Pio Gargano", "Laurent Mombaerts", "Jonathan Taws"]
  },
  {
    "id": "2402.18205",
    "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
    "abstract": "  Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18205v2",
    "authors": [
      "Wei Zhang",
      "Hongcheng Guo",
      "Anjie Le",
      "Jian Yang",
      "Jiaheng Liu",
      "Zhoujun Li",
      "Tieqiao Zheng",
      "Shi Xu",
      "Runqiang Zang",
      "Liangfan Zheng",
      "Bo Zhang"
    ]
  },
  {
    "id": "2402.18222",
    "title": "HearHere: Mitigating Echo Chambers in News Consumption through an\n  AI-based Web System",
    "abstract": "  Considerable efforts are currently underway to mitigate the negative impacts\nof echo chambers, such as increased susceptibility to fake news and resistance\ntowards accepting scientific evidence. Prior research has presented the\ndevelopment of computer systems that support the consumption of news\ninformation from diverse political perspectives to mitigate the echo chamber\neffect. However, existing studies still lack the ability to effectively support\nthe key processes of news information consumption and quantitatively identify a\npolitical stance towards the information. In this paper, we present HearHere,\nan AI-based web system designed to help users accommodate information and\nopinions from diverse perspectives. HearHere facilitates the key processes of\nnews information consumption through two visualizations. Visualization 1\nprovides political news with quantitative political stance information, derived\nfrom our graph-based political classification model, and users can experience\ndiverse perspectives (Hear). Visualization 2 allows users to express their\nopinions on specific political issues in a comment form and observe the\nposition of their own opinions relative to pro-liberal and pro-conservative\ncomments presented on a map interface (Here). Through a user study with 94\nparticipants, we demonstrate the feasibility of HearHere in supporting the\nconsumption of information from various perspectives. Our findings highlight\nthe importance of providing political stance information and quantifying users'\npolitical status as a means to mitigate political polarization. In addition, we\npropose design implications for system development, including the consideration\nof demographics such as political interest and providing users with\ninitiatives.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18222v2",
    "authors": [
      "Youngseung Jeon",
      "Jaehoon Kim",
      "Sohyun Park",
      "Yunyong Ko",
      "Seongeun Ryu",
      "Sang-Wook Kim",
      "Kyungsik Han"
    ]
  },
  {
    "id": "2402.18252",
    "title": "Towards Generalist Prompting for Large Language Models by Mental Models",
    "abstract": "  Large language models (LLMs) have demonstrated impressive performance on many\ntasks. However, to achieve optimal performance, specially designed prompting\nmethods are still needed. These methods either rely on task-specific few-shot\nexamples that require a certain level of domain knowledge, or are designed to\nbe simple but only perform well on a few types of tasks. In this work, we\nattempt to introduce the concept of generalist prompting, which operates on the\ndesign principle of achieving optimal or near-optimal performance on a wide\nrange of tasks while eliminating the need for manual selection and\ncustomization of prompts tailored to specific problems. Furthermore, we propose\nMeMo (Mental Models), an innovative prompting method that is simple-designed\nyet effectively fulfills the criteria of generalist prompting. MeMo distills\nthe cores of various prompting methods into individual mental models and allows\nLLMs to autonomously select the most suitable mental models for the problem,\nachieving or being near to the state-of-the-art results on diverse tasks such\nas STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We\nhope that the insights presented herein will stimulate further exploration of\ngeneralist prompting methods for LLMs.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18252v1",
    "authors": [
      "Haoxiang Guan",
      "Jiyan He",
      "Shuxin Zheng",
      "En-Hong Chen",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  {
    "id": "2402.18267",
    "title": "A Survey on Neural Question Generation: Methods, Applications, and\n  Prospects",
    "abstract": "  In this survey, we present a detailed examination of the advancements in\nNeural Question Generation (NQG), a field leveraging neural network techniques\nto generate relevant questions from diverse inputs like knowledge bases, texts,\nand images. The survey begins with an overview of NQG's background,\nencompassing the task's problem formulation, prevalent benchmark datasets,\nestablished evaluation metrics, and notable applications. It then methodically\nclassifies NQG approaches into three predominant categories: structured NQG,\nwhich utilizes organized data sources, unstructured NQG, focusing on more\nloosely structured inputs like texts or visual content, and hybrid NQG, drawing\non diverse input modalities. This classification is followed by an in-depth\nanalysis of the distinct neural network models tailored for each category,\ndiscussing their inherent strengths and potential limitations. The survey\nculminates with a forward-looking perspective on the trajectory of NQG,\nidentifying emergent research trends and prospective developmental paths.\nAccompanying this survey is a curated collection of related research papers,\ndatasets and codes, systematically organized on Github, providing an extensive\nreference for those delving into NQG.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18267v1",
    "authors": ["Shasha Guo", "Lizi Liao", "Cuiping Li", "Tat-Seng Chua"]
  },
  {
    "id": "2402.18272",
    "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the\n  Key?",
    "abstract": "  Recent progress in LLMs discussion suggests that multi-agent discussion\nimproves the reasoning abilities of LLMs. In this work, we reevaluate this\nclaim through systematic experiments, where we propose a novel group discussion\nframework to enrich the set of discussion mechanisms. Interestingly, our\nresults show that a single-agent LLM with strong prompts can achieve almost the\nsame performance as the best existing discussion approach on a wide range of\nreasoning tasks and backbone LLMs. We observe that the multi-agent discussion\nperforms better than a single agent only when there is no demonstration in the\nprompt. Further study reveals the common interaction mechanisms of LLMs during\nthe discussion.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18272v1",
    "authors": ["Qineng Wang", "Zihao Wang", "Ying Su", "Hanghang Tong", "Yangqiu Song"]
  },
  {
    "id": "2402.18284",
    "title": "Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of\n  Pre-trained Language Models with Proximal Policy Optimization",
    "abstract": "  Wide usage of ChatGPT has highlighted the potential of reinforcement learning\nfrom human feedback. However, its training pipeline relies on manual ranking, a\nresource-intensive process. To reduce labor costs, we propose a self-supervised\ntext ranking approach for applying Proximal-Policy-Optimization to fine-tune\nlanguage models while eliminating the need for human annotators. Our method\nbegins with probabilistic sampling to encourage a language model to generate\ndiverse responses for each input. We then employ TextRank and ISODATA\nalgorithms to rank and cluster these responses based on their semantics.\nSubsequently, we construct a reward model to learn the rank and optimize our\ngenerative policy. Our experimental results, conducted using two language\nmodels on three tasks, demonstrate that the models trained by our method\nconsiderably outperform baselines regarding BLEU, GLEU, and METEOR scores.\nFurthermore, our manual evaluation shows that our ranking results exhibit a\nremarkably high consistency with that of humans. This research significantly\nreduces training costs of proximal policy-guided models and demonstrates the\npotential for self-correction of language models.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18284v2",
    "authors": ["Shuo Yang", "Gjergji Kasneci"]
  },
  {
    "id": "2402.18320",
    "title": "Location-guided Head Pose Estimation for Fisheye Image",
    "abstract": "  Camera with a fisheye or ultra-wide lens covers a wide field of view that\ncannot be modeled by the perspective projection. Serious fisheye\n\\textcolor{blue}{lens} distortion in the peripheral region of the image leads\nto degraded performance of the \\textcolor{blue}{existing} head pose estimation\nmodels trained on undistorted images. This paper presents a new approach for\nhead pose estimation that uses the knowledge of head location in the image to\nreduce the negative effect of fisheye distortion. We develop an end-to-end\nconvolutional neural network to estimate the head pose with the multi-task\nlearning of head pose and head location. Our proposed network estimates the\nhead pose directly from the fisheye image without the operation of\nrectification or calibration. We also created \\textcolor{blue}{a}\nfisheye-\\textcolor{blue}{distorted} version of the three popular head pose\nestimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments.\nExperiments results show that our network remarkably improves the accuracy of\nhead pose estimation compared with other state-of-the-art one-stage and\ntwo-stage methods.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18320v1",
    "authors": ["Bing Li", "Dong Zhang", "Cheng Huang", "Yun Xian", "Ming Li", "Dah-Jye Lee"]
  },
  {
    "id": "2402.18326",
    "title": "When Should Algorithms Resign?",
    "abstract": "  This paper discusses algorithmic resignation, a strategic approach for\nmanaging the use of AI systems within organizations. Algorithmic resignation\ninvolves the deliberate and informed disengagement from AI assistance in\ncertain scenarios, by embedding governance mechanisms directly into AI systems.\nOur proposal is not merely about disuse of AI but includes guiding when and how\nthese systems should be used or avoided. We discuss the multifaceted benefits\nof algorithmic resignation, spanning economic efficiency, reputational gains,\nand legal compliance. Further, we outline the operationalization of resignation\nthrough various methods such as positive and negative nudges, stakeholder\nincentive alignment, and careful consideration of the level of AI engagement.\nUsing techniques like barring access to AI outputs selectively or providing\nexplicit disclaimers on system performance, algorithmic resignation not only\nmitigates risks associated with AI but also leverages its benefits, ensuring\nthe responsible and effective use of AI systems.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18326v1",
    "authors": ["Umang Bhatt", "Holli Sargeant"]
  },
  {
    "id": "2402.18362",
    "title": "Objective and Interpretable Breast Cosmesis Evaluation with Attention\n  Guided Denoising Diffusion Anomaly Detection Model",
    "abstract": "  As advancements in the field of breast cancer treatment continue to progress,\nthe assessment of post-surgical cosmetic outcomes has gained increasing\nsignificance due to its substantial impact on patients' quality of life.\nHowever, evaluating breast cosmesis presents challenges due to the inherently\nsubjective nature of expert labeling. In this study, we present a novel\nautomated approach, Attention-Guided Denoising Diffusion Anomaly Detection\n(AG-DDAD), designed to assess breast cosmesis following surgery, addressing the\nlimitations of conventional supervised learning and existing anomaly detection\nmodels. Our approach leverages the attention mechanism of the distillation with\nno label (DINO) self-supervised Vision Transformer (ViT) in combination with a\ndiffusion model to achieve high-quality image reconstruction and precise\ntransformation of discriminative regions. By training the diffusion model on\nunlabeled data predominantly with normal cosmesis, we adopt an unsupervised\nanomaly detection perspective to automatically score the cosmesis. Real-world\ndata experiments demonstrate the effectiveness of our method, providing\nvisually appealing representations and quantifiable scores for cosmesis\nevaluation. Compared to commonly used rule-based programs, our fully automated\napproach eliminates the need for manual annotations and offers objective\nevaluation. Moreover, our anomaly detection model exhibits state-of-the-art\nperformance, surpassing existing models in accuracy. Going beyond the scope of\nbreast cosmesis, our research represents a significant advancement in\nunsupervised anomaly detection within the medical domain, thereby paving the\nway for future investigations.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18362v1",
    "authors": [
      "Sangjoon Park",
      "Yong Bae Kim",
      "Jee Suk Chang",
      "Seo Hee Choi",
      "Hyungjin Chung",
      "Ik Jae Lee",
      "Hwa Kyung Byun"
    ]
  },
  {
    "id": "2402.18376",
    "title": "Tokenization Is More Than Compression",
    "abstract": "  Tokenization is a foundational step in Natural Language Processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18376v1",
    "authors": [
      "Craig W. Schmidt",
      "Varshini Reddy",
      "Haoran Zhang",
      "Alec Alameddine",
      "Omri Uzan",
      "Yuval Pinter",
      "Chris Tanner"
    ]
  },
  {
    "id": "2402.18385",
    "title": "The First Place Solution of WSDM Cup 2024: Leveraging Large Language\n  Models for Conversational Multi-Doc QA",
    "abstract": "  Conversational multi-doc question answering aims to answer specific questions\nbased on the retrieved documents as well as the contextual conversations. In\nthis paper, we introduce our winning approach for the \"Conversational Multi-Doc\nQA\" challenge in WSDM Cup 2024, which exploits the superior natural language\nunderstanding and generation capability of Large Language Models (LLMs). We\nfirst adapt LLMs to the task, then devise a hybrid training strategy to make\nthe most of in-domain unlabeled data. Moreover, an advanced text embedding\nmodel is adopted to filter out potentially irrelevant documents and several\napproaches are designed and compared for the model ensemble. Equipped with all\nthese techniques, our solution finally ranked 1st place in WSDM Cup 2024,\nsurpassing its rivals to a large extent. The source codes have been released at\nhttps://github.com/zhangzhao219/WSDM-Cup-2024.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18385v1",
    "authors": ["Yiming Li", "Zhao Zhang"]
  },
  {
    "id": "2402.18426",
    "title": "A Relational Inductive Bias for Dimensional Abstraction in Neural\n  Networks",
    "abstract": "  The human cognitive system exhibits remarkable flexibility and generalization\ncapabilities, partly due to its ability to form low-dimensional, compositional\nrepresentations of the environment. In contrast, standard neural network\narchitectures often struggle with abstract reasoning tasks, overfitting, and\nrequiring extensive data for training. This paper investigates the impact of\nthe relational bottleneck -- a mechanism that focuses processing on relations\namong inputs -- on the learning of factorized representations conducive to\ncompositional coding and the attendant flexibility of processing. We\ndemonstrate that such a bottleneck not only improves generalization and\nlearning efficiency, but also aligns network performance with human-like\nbehavioral biases. Networks trained with the relational bottleneck developed\northogonal representations of feature dimensions latent in the dataset,\nreflecting the factorized structure thought to underlie human cognitive\nflexibility. Moreover, the relational network mimics human biases towards\nregularity without pre-specified symbolic primitives, suggesting that the\nbottleneck fosters the emergence of abstract representations that confer\nflexibility akin to symbols.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18426v1",
    "authors": ["Declan Campbell", "Jonathan D. Cohen"]
  },
  {
    "id": "2402.18439",
    "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for\n  Enhanced Reasoning and Communication",
    "abstract": "  Natural language (NL) has long been the predominant format for human\ncognition and communication, and by extension, has been similarly pivotal in\nthe development and application of Large Language Models (LLMs). Yet, besides\nNL, LLMs have seen various non-NL formats during pre-training, such as code and\nlogical expression. NL's status as the optimal format for LLMs, particularly in\nsingle-LLM reasoning and multi-agent communication, has not been thoroughly\nexamined. In this work, we challenge the default use of NL by exploring the\nutility of non-NL formats in these contexts. We show that allowing LLMs to\nautonomously select the most suitable format before reasoning or communicating\nleads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs,\nand up to a 72.7\\% reduction in token usage in multi-agent communication, all\nwhile maintaining communicative effectiveness. Our comprehensive analysis\nfurther reveals that LLMs can devise a format from limited task instructions\nand that the devised format is effectively transferable across different LLMs.\nIntriguingly, the structured communication format decided by LLMs exhibits\nnotable parallels with established agent communication languages, suggesting a\nnatural evolution towards efficient, structured communication in agent\ncommunication. Our code is released at\n\\url{https://github.com/thunlp/AutoForm}.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18439v1",
    "authors": [
      "Weize Chen",
      "Chenfei Yuan",
      "Jiarui Yuan",
      "Yusheng Su",
      "Chen Qian",
      "Cheng Yang",
      "Ruobing Xie",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  {
    "id": "2402.18443",
    "title": "LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs",
    "abstract": "  Building efficient neural network architectures can be a time-consuming task\nrequiring extensive expert knowledge. This task becomes particularly\nchallenging for edge devices because one has to consider parameters such as\npower consumption during inferencing, model size, inferencing speed, and CO2\nemissions. In this article, we introduce a novel framework designed to\nautomatically discover new neural network architectures based on user-defined\nparameters, an expert system, and an LLM trained on a large amount of\nopen-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be\nused by non-AI experts, does not require a predetermined neural architecture\nsearch space, and considers a large set of edge device-specific parameters. We\nimplement and validate this proposed neural architecture discovery framework\nusing CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo\nand Gemini as the LLM component. We observe that the proposed framework can\nrapidly (within hours) discover intricate neural network models that perform\nextremely well across a diverse set of application settings defined by the\nuser.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18443v1",
    "authors": ["Md Hafizur Rahman", "Prabuddha Chakraborty"]
  },
  {
    "id": "2402.18485",
    "title": "A Multimodal Foundation Agent for Financial Trading: Tool-Augmented,\n  Diversified, and Generalist",
    "abstract": "  Financial trading is a crucial component of the markets, informed by a\nmultimodal information landscape encompassing news, prices, and Kline charts,\nand encompasses diverse tasks such as quantitative trading and high-frequency\ntrading with various assets. While advanced AI techniques like deep learning\nand reinforcement learning are extensively utilized in finance, their\napplication in financial trading tasks often faces challenges due to inadequate\nhandling of multimodal data and limited generalizability across various tasks.\nTo address these challenges, we present FinAgent, a multimodal foundational\nagent with tool augmentation for financial trading. FinAgent's market\nintelligence module processes a diverse range of data-numerical, textual, and\nvisual-to accurately analyze the financial market. Its unique dual-level\nreflection module not only enables rapid adaptation to market dynamics but also\nincorporates a diversified memory retrieval system, enhancing the agent's\nability to learn from historical data and improve decision-making processes.\nThe agent's emphasis on reasoning for actions fosters trust in its financial\ndecisions. Moreover, FinAgent integrates established trading strategies and\nexpert insights, ensuring that its trading approaches are both data-driven and\nrooted in sound financial principles. With comprehensive experiments on 6\nfinancial datasets, including stocks and Crypto, FinAgent significantly\noutperforms 9 state-of-the-art baselines in terms of 6 financial metrics with\nover 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%\nrelative improvement) is achieved on one dataset. Notably, FinAgent is the\nfirst advanced multimodal foundation agent designed for financial trading\ntasks.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18485v2",
    "authors": [
      "Wentao Zhang",
      "Lingxuan Zhao",
      "Haochong Xia",
      "Shuo Sun",
      "Jiaze Sun",
      "Molei Qin",
      "Xinyi Li",
      "Yuqing Zhao",
      "Yilei Zhao",
      "Xinyu Cai",
      "Longtao Zheng",
      "Xinrun Wang",
      "Bo An"
    ]
  },
  {
    "id": "2402.18496",
    "title": "Language Models Represent Beliefs of Self and Others",
    "abstract": "  Understanding and attributing mental states, known as Theory of Mind (ToM),\nemerges as a fundamental capability for human social reasoning. While Large\nLanguage Models (LLMs) appear to possess certain ToM abilities, the mechanisms\nunderlying these capabilities remain elusive. In this study, we discover that\nit is possible to linearly decode the belief status from the perspectives of\nvarious agents through neural activations of language models, indicating the\nexistence of internal representations of self and others' beliefs. By\nmanipulating these representations, we observe dramatic changes in the models'\nToM performance, underscoring their pivotal role in the social reasoning\nprocess. Additionally, our findings extend to diverse social reasoning tasks\nthat involve different causal inference patterns, suggesting the potential\ngeneralizability of these representations.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18496v2",
    "authors": ["Wentao Zhu", "Zhining Zhang", "Yizhou Wang"]
  },
  {
    "id": "2402.18609",
    "title": "ICE-SEARCH: A Language Model-Driven Feature Selection Approach",
    "abstract": "  This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method,\nthe first work that melds language models (LMs) with evolutionary algorithms\nfor feature selection (FS) tasks and demonstrates its effectiveness in Medical\nPredictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and\nmutation capabilities inherent in LMs within an evolutionary framework,\nsignificantly improving FS through the model's comprehensive world knowledge\nand its adaptability to a variety of roles. Our evaluation of this methodology\nspans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes,\nwhere ICE-SEARCH outperforms traditional FS methods in pinpointing essential\nfeatures for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA)\nperformance in stroke prediction and diabetes prediction; the\nDecision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease\nprediction. Our results not only demonstrate the efficacy of ICE-SEARCH in\nmedical FS but also underscore the versatility, efficiency, and scalability of\nintegrating LMs in FS tasks. The study emphasizes the critical role of\nincorporating domain-specific insights, illustrating ICE-SEARCH's robustness,\ngeneralizability, and swift convergence. This opens avenues for further\nresearch into comprehensive and intricate FS landscapes, marking a significant\nstride in the application of artificial intelligence in medical predictive\nanalytics.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18609v2",
    "authors": ["Tianze Yang", "Tianyi Yang", "Shaoshan Liu", "Fuyuan Lvu", "Xue Liu"]
  },
  {
    "id": "2402.18616",
    "title": "JCLEC-MO: a Java suite for solving many-objective optimization\n  engineering problems",
    "abstract": "  Although metaheuristics have been widely recognized as efficient techniques\nto solve real-world optimization problems, implementing them from scratch\nremains difficult for domain-specific experts without programming skills. In\nthis scenario, metaheuristic optimization frameworks are a practical\nalternative as they provide a variety of algorithms composed of customized\nelements, as well as experimental support. Recently, many engineering problems\nrequire to optimize multiple or even many objectives, increasing the interest\nin appropriate metaheuristic algorithms and frameworks that might integrate new\nspecific requirements while maintaining the generality and reusability\nprinciples they were conceived for. Based on this idea, this paper introduces\nJCLEC-MO, a Java framework for both multi- and many-objective optimization that\nenables engineers to apply, or adapt, a great number of multi-objective\nalgorithms with little coding effort. A case study is developed and explained\nto show how JCLEC-MO can be used to address many-objective engineering\nproblems, often requiring the inclusion of domain-specific elements, and to\nanalyze experimental outcomes by means of conveniently connected R utilities.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18616v1",
    "authors": ["Aurora Ramrez", "Jos Ral Romero", "Carlos Garca-Martnez", "Sebastin Ventura"]
  },
  {
    "id": "2402.18649",
    "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World\n  LLM-based Systems",
    "abstract": "  Large Language Model (LLM) systems are inherently compositional, with\nindividual LLM serving as the core foundation with additional layers of objects\nsuch as plugins, sandbox, and so on. Along with the great potential, there are\nalso increasing concerns over the security of such probabilistic intelligent\nsystems. However, existing studies on LLM security often focus on individual\nLLM, but without examining the ecosystem through the lens of LLM systems with\nother objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we\nsystematically analyze the security of LLM systems, instead of focusing on the\nindividual LLMs. To do so, we build on top of the information flow and\nformulate the security of LLM systems as constraints on the alignment of the\ninformation flow within LLM and between LLM and other objects. Based on this\nconstruction and the unique probabilistic nature of LLM, the attack surface of\nthe LLM system can be decomposed into three key components: (1) multi-layer\nsecurity analysis, (2) analysis of the existence of constraints, and (3)\nanalysis of the robustness of these constraints. To ground this new attack\nsurface, we propose a multi-layer and multi-step approach and apply it to the\nstate-of-art LLM system, OpenAI GPT4. Our investigation exposes several\nsecurity issues, not just within the LLM model itself but also in its\nintegration with other components. We found that although the OpenAI GPT4 has\ndesigned numerous safety constraints to improve its safety features, these\nsafety constraints are still vulnerable to attackers. To further demonstrate\nthe real-world threats of our discovered vulnerabilities, we construct an\nend-to-end attack where an adversary can illicitly acquire the user's chat\nhistory, all without the need to manipulate the user's input or gain direct\naccess to OpenAI GPT4. Our demo is in the link:\nhttps://fzwark.github.io/LLM-System-Attack-Demo/\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18649v1",
    "authors": ["Fangzhou Wu", "Ning Zhang", "Somesh Jha", "Patrick McDaniel", "Chaowei Xiao"]
  },
  {
    "id": "2402.18673",
    "title": "Trends, Applications, and Challenges in Human Attention Modelling",
    "abstract": "  Human attention modelling has proven, in recent years, to be particularly\nuseful not only for understanding the cognitive processes underlying visual\nexploration, but also for providing support to artificial intelligence models\nthat aim to solve problems in various domains, including image and video\nprocessing, vision-and-language applications, and language modelling. This\nsurvey offers a reasoned overview of recent efforts to integrate human\nattention mechanisms into contemporary deep learning models and discusses\nfuture research directions and challenges. For a comprehensive overview on the\nongoing research refer to our dedicated repository available at\nhttps://github.com/aimagelab/awesome-human-visual-attention.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18673v1",
    "authors": [
      "Giuseppe Cartella",
      "Marcella Cornia",
      "Vittorio Cuculo",
      "Alessandro D'Amelio",
      "Dario Zanca",
      "Giuseppe Boccignone",
      "Rita Cucchiara"
    ]
  },
  {
    "id": "2402.18679",
    "title": "Data Interpreter: An LLM Agent For Data Science",
    "abstract": "  Large Language Model (LLM)-based agents have demonstrated remarkable\neffectiveness. However, their performance can be compromised in data science\nscenarios that require real-time data adjustment, expertise in optimization due\nto complex dependencies among various tasks, and the ability to identify\nlogical errors for precise reasoning. In this study, we introduce the Data\nInterpreter, a solution designed to solve with code that emphasizes three\npivotal techniques to augment problem-solving in data science: 1) dynamic\nplanning with hierarchical graph structures for real-time data adaptability;2)\ntool integration dynamically to enhance code proficiency during execution,\nenriching the requisite expertise;3) logical inconsistency identification in\nfeedback, and efficiency enhancement through experience recording. We evaluate\nthe Data Interpreter on various data science and real-world tasks. Compared to\nopen-source baselines, it demonstrated superior performance, exhibiting\nsignificant improvements in machine learning tasks, increasing from 0.86 to\n0.95. Additionally, it showed a 26% increase in the MATH dataset and a\nremarkable 112% improvement in open-ended tasks. The solution will be released\nat https://github.com/geekan/MetaGPT.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18679v2",
    "authors": [
      "Sirui Hong",
      "Yizhang Lin",
      "Bangbang Liu",
      "Binhao Wu",
      "Danyang Li",
      "Jiaqi Chen",
      "Jiayi Zhang",
      "Jinlin Wang",
      "Lingyao Zhang",
      "Mingchen Zhuge",
      "Taicheng Guo",
      "Tuo Zhou",
      "Wei Tao",
      "Wenyi Wang",
      "Xiangru Tang",
      "Xiangtao Lu",
      "Xinbing Liang",
      "Yaying Fei",
      "Yuheng Cheng",
      "Zongze Xu",
      "Chenglin Wu",
      "Li Zhang",
      "Min Yang",
      "Xiawu Zheng"
    ]
  },
  {
    "id": "2402.18715",
    "title": "Commonsense Ontology Micropatterns",
    "abstract": "  The previously introduced Modular Ontology Modeling methodology (MOMo)\nattempts to mimic the human analogical process by using modular patterns to\nassemble more complex concepts. To support this, MOMo organizes organizes\nontology design patterns into design libraries, which are programmatically\nqueryable, to support accelerated ontology development, for both human and\nautomated processes. However, a major bottleneck to large-scale deployment of\nMOMo is the (to-date) limited availability of ready-to-use ontology design\npatterns. At the same time, Large Language Models have quickly become a source\nof common knowledge and, in some cases, replacing search engines for questions.\nIn this paper, we thus present a collection of 104 ontology design patterns\nrepresenting often occurring nouns, curated from the common-sense knowledge\navailable in LLMs, organized into a fully-annotated modular ontology design\nlibrary ready for use with MOMo.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18715v1",
    "authors": ["Andrew Eells", "Brandon Dave", "Pascal Hitzler", "Cogan Shimizu"]
  },
  {
    "id": "2402.18732",
    "title": "GAIA: Categorical Foundations of Generative AI",
    "abstract": "  In this paper, we propose GAIA, a generative AI architecture based on\ncategory theory. GAIA is based on a hierarchical model where modules are\norganized as a simplicial complex. Each simplicial complex updates its internal\nparameters biased on information it receives from its superior simplices and in\nturn relays updates to its subordinate sub-simplices. Parameter updates are\nformulated in terms of lifting diagrams over simplicial sets, where inner and\nouter horn extensions correspond to different types of learning problems.\nBackpropagation is modeled as an endofunctor over the category of parameters,\nleading to a coalgebraic formulation of deep learning.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18732v1",
    "authors": ["Sridhar Mahadevan"]
  },
  {
    "id": "2402.18743",
    "title": "A revision on Multi-Criteria Decision Making methods for Multi-UAV\n  Mission Planning Support",
    "abstract": "  Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively\nused in many commercial applications due to their manageability and risk\navoidance. One of the main problems considered is the Mission Planning for\nmultiple UAVs, where a solution plan must be found satisfying the different\nconstraints of the problem. This problem has multiple variables that must be\noptimized simultaneously, such as the makespan, the cost of the mission or the\nrisk. Therefore, the problem has a lot of possible optimal solutions, and the\noperator must select the final solution to be executed among them. In order to\nreduce the workload of the operator in this decision process, a Decision\nSupport System (DSS) becomes necessary. In this work, a DSS consisting of\nranking and filtering systems, which order and reduce the optimal solutions,\nhas been designed. With regard to the ranking system, a wide range of\nMulti-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are\ncompared on a multi-UAV mission planning scenario, in order to study which\nmethod could fit better in a multi-UAV decision support system. Expert\noperators have evaluated the solutions returned, and the results show, on the\none hand, that fuzzy methods generally achieve better average scores, and on\nthe other, that all of the tested methods perform better when the preferences\nof the operators are biased towards a specific variable, and worse when their\npreferences are balanced. For the filtering system, a similarity function based\non the proximity of the solutions has been designed, and on top of that, a\nthreshold is tuned empirically to decide how to filter solutions without losing\nmuch of the hypervolume of the space of solutions.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18743v1",
    "authors": ["Cristian Ramirez-Atencia", "Victor Rodriguez-Fernandez", "David Camacho"]
  },
  {
    "id": "2402.18747",
    "title": "Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains",
    "abstract": "  We introduce a new, extensive multidimensional quality metrics (MQM)\nannotated dataset covering 11 language pairs in the biomedical domain. We use\nthis dataset to investigate whether machine translation (MT) metrics which are\nfine-tuned on human-generated MT quality judgements are robust to domain shifts\nbetween training and inference. We find that fine-tuned metrics exhibit a\nsubstantial performance drop in the unseen domain scenario relative to metrics\nthat rely on the surface form, as well as pre-trained metrics which are not\nfine-tuned on MT quality judgments.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18747v1",
    "authors": [
      "Vilm Zouhar",
      "Shuoyang Ding",
      "Anna Currey",
      "Tatyana Badeka",
      "Jenyuan Wang",
      "Brian Thompson"
    ]
  },
  {
    "id": "2403.00016",
    "title": "Deep Sensitivity Analysis for Objective-Oriented Combinatorial\n  Optimization",
    "abstract": "  Pathogen control is a critical aspect of modern poultry farming, providing\nimportant benefits for both public health and productivity. Effective poultry\nmanagement measures to reduce pathogen levels in poultry flocks promote food\nsafety by lowering risks of food-borne illnesses. They also support animal\nhealth and welfare by preventing infectious diseases that can rapidly spread\nand impact flock growth, egg production, and overall health. This study frames\nthe search for optimal management practices that minimize the presence of\nmultiple pathogens as a combinatorial optimization problem. Specifically, we\nmodel the various possible combinations of management settings as a solution\nspace that can be efficiently explored to identify configurations that\noptimally reduce pathogen levels. This design incorporates a neural network\nfeedback-based method that combines feature explanations with global\nsensitivity analysis to ensure combinatorial optimization in multiobjective\nsettings. Our preliminary experiments have promising results when applied to\ntwo real-world agricultural datasets. While further validation is still needed,\nthese early experimental findings demonstrate the potential of the model to\nderive targeted feature interactions that adaptively optimize pathogen control\nunder varying real-world constraints.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00016v1",
    "authors": [
      "Ganga Gireesan",
      "Nisha Pillai",
      "Michael J Rothrock",
      "Bindu Nanduri",
      "Zhiqian Chen",
      "Mahalingam Ramkumar"
    ]
  },
  {
    "id": "2403.00017",
    "title": "Towards Interpreting Multi-Objective Feature Associations",
    "abstract": "  Understanding how multiple features are associated and contribute to a\nspecific objective is as important as understanding how each feature\ncontributes to a particular outcome. Interpretability of a single feature in a\nprediction may be handled in multiple ways; however, in a multi-objective\nprediction, it is difficult to obtain interpretability of a combination of\nfeature values. To address this issue, we propose an objective specific feature\ninteraction design using multi-labels to find the optimal combination of\nfeatures in agricultural settings. One of the novel aspects of this design is\nthe identification of a method that integrates feature explanations with global\nsensitivity analysis in order to ensure combinatorial optimization in\nmulti-objective settings. We have demonstrated in our preliminary experiments\nthat an approximate combination of feature values can be found to achieve the\ndesired outcome using two agricultural datasets: one with pre-harvest poultry\nfarm practices for multi-drug resistance presence, and one with post-harvest\npoultry farm practices for food-borne pathogens. In our combinatorial\noptimization approach, all three pathogens are taken into consideration\nsimultaneously to account for the interaction between conditions that favor\ndifferent types of pathogen growth. These results indicate that\nexplanation-based approaches are capable of identifying combinations of\nfeatures that reduce pathogen presence in fewer iterations than a baseline.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00017v1",
    "authors": [
      "Nisha Pillai",
      "Ganga Gireesan",
      "Michael J. Rothrock Jr.",
      "Bindu Nanduri",
      "Zhiqian Chen",
      "Mahalingam Ramkumar"
    ]
  },
  {
    "id": "2403.00025",
    "title": "On the Challenges and Opportunities in Generative AI",
    "abstract": "  The field of deep generative modeling has grown rapidly and consistently over\nthe years. With the availability of massive amounts of training data coupled\nwith advances in scalable unsupervised learning paradigms, recent large-scale\ngenerative models show tremendous promise in synthesizing high-resolution\nimages and text, as well as structured data such as videos and molecules.\nHowever, we argue that current large-scale generative AI models do not\nsufficiently address several fundamental issues that hinder their widespread\nadoption across domains. In this work, we aim to identify key unresolved\nchallenges in modern generative AI paradigms that should be tackled to further\nenhance their capabilities, versatility, and reliability. By identifying these\nchallenges, we aim to provide researchers with valuable insights for exploring\nfruitful research directions, thereby fostering the development of more robust\nand accessible generative AI solutions.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00025v1",
    "authors": [
      "Laura Manduchi",
      "Kushagra Pandey",
      "Robert Bamler",
      "Ryan Cotterell",
      "Sina Dubener",
      "Sophie Fellenz",
      "Asja Fischer",
      "Thomas Grtner",
      "Matthias Kirchler",
      "Marius Kloft",
      "Yingzhen Li",
      "Christoph Lippert",
      "Gerard de Melo",
      "Eric Nalisnick",
      "Bjrn Ommer",
      "Rajesh Ranganath",
      "Maja Rudolph",
      "Karen Ullrich",
      "Guy Van den Broeck",
      "Julia E Vogt",
      "Yixin Wang",
      "Florian Wenzel",
      "Frank Wood",
      "Stephan Mandt",
      "Vincent Fortuin"
    ]
  },
  {
    "id": "2403.00829",
    "title": "TroubleLLM: Align to Red Team Expert",
    "abstract": "  Large Language Models (LLMs) become the start-of-the-art solutions for a\nvariety of natural language tasks and are integrated into real-world\napplications. However, LLMs can be potentially harmful in manifesting\nundesirable safety issues like social biases and toxic content. It is\nimperative to assess its safety issues before deployment. However, the quality\nand diversity of test prompts generated by existing methods are still far from\nsatisfactory. Not only are these methods labor-intensive and require large\nbudget costs, but the controllability of test prompt generation is lacking for\nthe specific testing domain of LLM applications. With the idea of LLM for LLM\ntesting, we propose the first LLM, called TroubleLLM, to generate controllable\ntest prompts on LLM safety issues. Extensive experiments and human evaluation\nillustrate the superiority of TroubleLLM on generation quality and generation\ncontrollability.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00829v1",
    "authors": ["Zhuoer Xu", "Jianping Zhang", "Shiwen Cui", "Changhua Meng", "Weiqiang Wang"]
  },
  {
    "id": "2403.00830",
    "title": "MedAide: Leveraging Large Language Models for On-Premise Medical\n  Assistance on Edge Devices",
    "abstract": "  Large language models (LLMs) are revolutionizing various domains with their\nremarkable natural language processing (NLP) abilities. However, deploying LLMs\nin resource-constrained edge computing and embedded systems presents\nsignificant challenges. Another challenge lies in delivering medical assistance\nin remote areas with limited healthcare facilities and infrastructure. To\naddress this, we introduce MedAide, an on-premise healthcare chatbot. It\nleverages tiny-LLMs integrated with LangChain, providing efficient edge-based\npreliminary medical diagnostics and support. MedAide employs model\noptimizations for minimal memory footprint and latency on embedded edge devices\nwithout server infrastructure. The training process is optimized using low-rank\nadaptation (LoRA). Additionally, the model is trained on diverse medical\ndatasets, employing reinforcement learning from human feedback (RLHF) to\nenhance its domain-specific capabilities. The system is implemented on various\nconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\%\naccuracy in medical consultations and scores 56 in USMLE benchmark, enabling an\nenergy-efficient healthcare assistance platform that alleviates privacy\nconcerns due to edge-based deployment, thereby empowering the community.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00830v1",
    "authors": ["Abdul Basit", "Khizar Hussain", "Muhammad Abdullah Hanif", "Muhammad Shafique"]
  },
  {
    "id": "2403.00832",
    "title": "Explainable Session-based Recommendation via Path Reasoning",
    "abstract": "  This paper explores providing explainability for session-based recommendation\n(SR) by path reasoning. Current SR models emphasize accuracy but lack\nexplainability, while traditional path reasoning prioritizes knowledge graph\nexploration, ignoring sequential patterns present in the session history.\nTherefore, we propose a generalized hierarchical reinforcement learning\nframework for SR, which improves the explainability of existing SR models via\nPath Reasoning, namely PR4SR. Considering the different importance of items to\nthe session, we design the session-level agent to select the items in the\nsession as the starting point for path reasoning and the path-level agent to\nperform path reasoning. In particular, we design a multi-target reward\nmechanism to adapt to the skip behaviors of sequential patterns in SR, and\nintroduce path midpoint reward to enhance the exploration efficiency in\nknowledge graphs. To improve the completeness of the knowledge graph and to\ndiversify the paths of explanation, we incorporate extracted feature\ninformation from images into the knowledge graph. We instantiate PR4SR in five\nstate-of-the-art SR models (i.e., GRU4REC, NARM, GCSAN, SR-GNN, SASRec) and\ncompare it with other explainable SR frameworks, to demonstrate the\neffectiveness of PR4SR for recommendation and explanation tasks through\nextensive experiments with these approaches on four datasets.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00832v1",
    "authors": ["Yang Cao", "Shuo Shang", "Jun Wang", "Wei Zhang"]
  },
  {
    "id": "2403.00835",
    "title": "CLLMs: Consistency Large Language Models",
    "abstract": "  Parallel decoding methods such as Jacobi decoding show promise for more\nefficient LLM inference as it breaks the sequential nature of the LLM decoding\nprocess and transforms it into parallelizable computation. However, in\npractice, it achieves little speedup compared to traditional autoregressive\n(AR) decoding, primarily because Jacobi decoding seldom accurately predicts\nmore than one token in a single fixed-point iteration step. To address this, we\ndevelop a new approach aimed at realizing fast convergence from any state to\nthe fixed point on a Jacobi trajectory. This is accomplished by refining the\ntarget LLM to consistently predict the fixed point given any state as input.\nExtensive experiments demonstrate the effectiveness of our method, showing\n2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving\ngeneration quality across both domain-specific and open-domain benchmarks.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00835v2",
    "authors": ["Siqi Kou", "Lanxiang Hu", "Zhezhi He", "Zhijie Deng", "Hao Zhang"]
  },
  {
    "id": "2402.17971",
    "title": "All in a Single Image: Large Multimodal Models are In-Image Learners",
    "abstract": "  This paper introduces a new in-context learning (ICL) mechanism called\nIn-Image Learning (I$^2$L) that combines demonstration examples, visual cues,\nand instructions into a single image to enhance the capabilities of GPT-4V.\nUnlike previous approaches that rely on converting images to text or\nincorporating visual input into language models, I$^2$L consolidates all\ninformation into one image and primarily leverages image processing,\nunderstanding, and reasoning abilities. This has several advantages: it avoids\ninaccurate textual descriptions of complex images, provides flexibility in\npositioning demonstration examples, reduces the input burden, and avoids\nexceeding input limits by eliminating the need for multiple images and lengthy\ntext. To further combine the strengths of different ICL methods, we introduce\nan automatic strategy to select the appropriate ICL method for a data example\nin a given task. We conducted experiments on MathVista and Hallusionbench to\ntest the effectiveness of I$^2$L in complex multimodal reasoning tasks and\nmitigating language hallucination and visual illusion. Additionally, we\nexplored the impact of image resolution, the number of demonstration examples,\nand their positions on the effectiveness of I$^2$L. Our code is publicly\navailable at https://github.com/AGI-Edgerunners/IIL.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.17971v1",
    "authors": [
      "Lei Wang",
      "Wanyu Xu",
      "Zhiqiang Hu",
      "Yihuai Lan",
      "Shan Dong",
      "Hao Wang",
      "Roy Ka-Wei Lee",
      "Ee-Peng Lim"
    ]
  },
  {
    "id": "2402.17978",
    "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in\n  Multi-Agent Reinforcement Learning",
    "abstract": "  Effective exploration is crucial to discovering optimal strategies for\nmulti-agent reinforcement learning (MARL) in complex coordination tasks.\nExisting methods mainly utilize intrinsic rewards to enable committed\nexploration or use role-based learning for decomposing joint action spaces\ninstead of directly conducting a collective search in the entire\naction-observation space. However, they often face challenges obtaining\nspecific joint action sequences to reach successful states in long-horizon\ntasks. To address this limitation, we propose Imagine, Initialize, and Explore\n(IIE), a novel method that offers a promising solution for efficient\nmulti-agent exploration in complex scenarios. IIE employs a transformer model\nto imagine how the agents reach a critical state that can influence each\nother's transition functions. Then, we initialize the environment at this state\nusing a simulator before the exploration phase. We formulate the imagination as\na sequence modeling problem, where the states, observations, prompts, actions,\nand rewards are predicted autoregressively. The prompt consists of\ntimestep-to-go, return-to-go, influence value, and one-shot demonstration,\nspecifying the desired state and trajectory as well as guiding the action\ngeneration. By initializing agents at the critical states, IIE significantly\nincreases the likelihood of discovering potentially important under-explored\nregions. Despite its simplicity, empirical results demonstrate that our method\noutperforms multi-agent exploration baselines on the StarCraft Multi-Agent\nChallenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved\nperformance in the sparse-reward SMAC tasks and produces more effective\ncurricula over the initialized states than other generative methods, such as\nCVAE-GAN and diffusion models.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.17978v2",
    "authors": [
      "Zeyang Liu",
      "Lipeng Wan",
      "Xinrui Yang",
      "Zhuoran Chen",
      "Xingyu Chen",
      "Xuguang Lan"
    ]
  },
  {
    "id": "2402.17979",
    "title": "Ensemble Methodology:Innovations in Credit Default Prediction Using\n  LightGBM, XGBoost, and LocalEnsemble",
    "abstract": "  In the realm of consumer lending, accurate credit default prediction stands\nas a critical element in risk mitigation and lending decision optimization.\nExtensive research has sought continuous improvement in existing models to\nenhance customer experiences and ensure the sound economic functioning of\nlending institutions. This study responds to the evolving landscape of credit\ndefault prediction, challenging conventional models and introducing innovative\napproaches. By building upon foundational research and recent innovations, our\nwork aims to redefine the standards of accuracy in credit default prediction,\nsetting a new benchmark for the industry. To overcome these challenges, we\npresent an Ensemble Methods framework comprising LightGBM, XGBoost, and\nLocalEnsemble modules, each making unique contributions to amplify diversity\nand improve generalization. By utilizing distinct feature sets, our methodology\ndirectly tackles limitations identified in previous studies, with the\noverarching goal of establishing a novel standard for credit default prediction\naccuracy. Our experimental findings validate the effectiveness of the ensemble\nmodel on the dataset, signifying substantial contributions to the field. This\ninnovative approach not only addresses existing obstacles but also sets a\nprecedent for advancing the accuracy and robustness of credit default\nprediction models.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.17979v1",
    "authors": ["Mengran Zhu", "Ye Zhang", "Yulu Gong", "Kaijuan Xing", "Xu Yan", "Jintong Song"]
  },
  {
    "id": "2402.17985",
    "title": "FlattenQuant: Breaking Through the Inference Compute-bound for Large\n  Language Models with Per-tensor Quantization",
    "abstract": "  Large language models (LLMs) have demonstrated state-of-the-art performance\nacross various tasks. However, the latency of inference and the large GPU\nmemory consumption of LLMs restrict their deployment performance. Recently,\nthere have been some efficient attempts to quantize LLMs, yet inference with\nlarge batch size or long sequence still has the issue of being compute-bound.\nFine-grained quantization methods have showcased their proficiency in achieving\nlow-bit quantization for LLMs, while requiring FP16 data type for linear layer\ncomputations, which is time-consuming when dealing with large batch size or\nlong sequence. In this paper, we introduce a method called FlattenQuant, which\nsignificantly reduces the maximum value of the tensor by flattening the large\nchannels in the tensor, to achieve low bit per-tensor quantization with minimal\naccuracy loss. Our experiments show that FlattenQuant can directly use 4 bits\nto achieve 48.29% of the linear layer calculation in LLMs, with the remaining\nlayers using 8 bits. The 4-bit matrix multiplication introduced in the\nFlattenQuant method can effectively address the compute-bound caused by large\nmatrix calculation. Our work achieves up to 2$\\times$ speedup and 2.3$\\times$\nmemory reduction for LLMs with negligible loss in accuracy.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.17985v1",
    "authors": ["Yi Zhang", "Fei Yang", "Shuang Peng", "Fangyu Wang", "Aimin Pan"]
  },
  {
    "id": "2402.18002",
    "title": "Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial\n  Observability with a Soft Wrist",
    "abstract": "  This study tackles the representative yet challenging contact-rich\npeg-in-hole task of robotic assembly, using a soft wrist that can operate more\nsafely and tolerate lower-frequency control signals than a rigid one. Previous\nstudies often use a fully observable formulation, requiring external setups or\nestimators for the peg-to-hole pose. In contrast, we use a partially observable\nformulation and deep reinforcement learning from demonstrations to learn a\nmemory-based agent that acts purely on haptic and proprioceptive signals.\nMoreover, previous works do not incorporate potential domain symmetry and thus\nmust search for solutions in a bigger space. Instead, we propose to leverage\nthe symmetry for sample efficiency by augmenting the training data and\nconstructing auxiliary losses to force the agent to adhere to the symmetry.\nResults in simulation with five different symmetric peg shapes show that our\nproposed agent can be comparable to or even outperform a state-based agent. In\nparticular, the sample efficiency also allows us to learn directly on the real\nrobot within 3 hours.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18002v1",
    "authors": ["Hai Nguyen", "Tadashi Kozuno", "Cristian C. Beltran-Hernandez", "Masashi Hamaya"]
  },
  {
    "id": "2402.18007",
    "title": "Mixer is more than just a model",
    "abstract": "  Recently, MLP structures have regained popularity, with MLP-Mixer standing\nout as a prominent example. In the field of computer vision, MLP-Mixer is noted\nfor its ability to extract data information from both channel and token\nperspectives, effectively acting as a fusion of channel and token information.\nIndeed, Mixer represents a paradigm for information extraction that amalgamates\nchannel and token information. The essence of Mixer lies in its ability to\nblend information from diverse perspectives, epitomizing the true concept of\n\"mixing\" in the realm of neural network architectures. Beyond channel and token\nconsiderations, it is possible to create more tailored mixers from various\nperspectives to better suit specific task requirements. This study focuses on\nthe domain of audio recognition, introducing a novel model named Audio\nSpectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates\ninsights from both time and frequency domains. Experimental results demonstrate\nthat ASM-RH is particularly well-suited for audio data and yields promising\noutcomes across multiple classification tasks. The models and optimal weights\nfiles will be published.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18007v2",
    "authors": ["Qingfeng Ji", "Yuxin Wang", "Letong Sun"]
  },
  {
    "id": "2402.18091",
    "title": "Polos: Multimodal Metric Learning from Human Feedback for Image\n  Captioning",
    "abstract": "  Establishing an automatic evaluation metric that closely aligns with human\njudgments is essential for effectively developing image captioning models.\nRecent data-driven metrics have demonstrated a stronger correlation with human\njudgments than classic metrics such as CIDEr; however they lack sufficient\ncapabilities to handle hallucinations and generalize across diverse images and\ntexts partially because they compute scalar similarities merely using\nembeddings learned from tasks unrelated to image captioning evaluation. In this\nstudy, we propose Polos, a supervised automatic evaluation metric for image\ncaptioning models. Polos computes scores from multimodal inputs, using a\nparallel feature extraction mechanism that leverages embeddings trained through\nlarge-scale contrastive learning. To train Polos, we introduce Multimodal\nMetric Learning from Human Feedback (M$^2$LHF), a framework for developing\nmetrics based on human feedback. We constructed the Polaris dataset, which\ncomprises 131K human judgments from 550 evaluators, which is approximately ten\ntimes larger than standard datasets. Our approach achieved state-of-the-art\nperformance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and\nthe Polaris dataset, thereby demonstrating its effectiveness and robustness.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18091v1",
    "authors": ["Yuiga Wada", "Kanta Kaneda", "Daichi Saito", "Komei Sugiura"]
  },
  {
    "id": "2402.18129",
    "title": "On the Inductive Biases of Demographic Parity-based Fair Learning\n  Algorithms",
    "abstract": "  Fair supervised learning algorithms assigning labels with little dependence\non a sensitive attribute have attracted great attention in the machine learning\ncommunity. While the demographic parity (DP) notion has been frequently used to\nmeasure a model's fairness in training fair classifiers, several studies in the\nliterature suggest potential impacts of enforcing DP in fair learning\nalgorithms. In this work, we analytically study the effect of standard DP-based\nregularization methods on the conditional distribution of the predicted label\ngiven the sensitive attribute. Our analysis shows that an imbalanced training\ndataset with a non-uniform distribution of the sensitive attribute could lead\nto a classification rule biased toward the sensitive attribute outcome holding\nthe majority of training data. To control such inductive biases in DP-based\nfair learning, we propose a sensitive attribute-based distributionally robust\noptimization (SA-DRO) method improving robustness against the marginal\ndistribution of the sensitive attribute. Finally, we present several numerical\nresults on the application of DP-based learning methods to standard centralized\nand distributed learning problems. The empirical findings support our\ntheoretical results on the inductive biases in DP-based fair learning\nalgorithms and the debiasing effects of the proposed SA-DRO method.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18129v1",
    "authors": ["Haoyu Lei", "Amin Gohari", "Farzan Farnia"]
  },
  {
    "id": "2402.18150",
    "title": "Unsupervised Information Refinement Training of Large Language Models\n  for Retrieval-Augmented Generation",
    "abstract": "  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating additional information from retrieval. However, studies have\nshown that LLMs still face challenges in effectively using the retrieved\ninformation, even ignoring it or being misled by it. The key reason is that the\ntraining of LLMs does not clearly make LLMs learn how to utilize input\nretrieved texts with varied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in RAG as ``Information Refiner'',\nwhich means that regardless of correctness, completeness, or usefulness of\nretrieved texts, LLMs can consistently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts that are more concise,\naccurate, and complete than the retrieved texts. To this end, we propose an\ninformation refinement training method named InFO-RAG that optimizes LLMs for\nRAG in an unsupervised manner. InFO-RAG is low-cost and general across various\ntasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse\ntasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,\nand Code Generation show that InFO-RAG improves the performance of LLaMA2 by an\naverage of 9.39\\% relative points. InFO-RAG also shows advantages in in-context\nlearning and robustness of RAG.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18150v1",
    "authors": [
      "Shicheng Xu",
      "Liang Pang",
      "Mo Yu",
      "Fandong Meng",
      "Huawei Shen",
      "Xueqi Cheng",
      "Jie Zhou"
    ]
  },
  {
    "id": "2402.18152",
    "title": "Boosting Neural Representations for Videos with a Conditional Decoder",
    "abstract": "  Implicit neural representations (INRs) have emerged as a promising approach\nfor video storage and processing, showing remarkable versatility across various\nvideo tasks. However, existing methods often fail to fully leverage their\nrepresentation capabilities, primarily due to inadequate alignment of\nintermediate features during target frame decoding. This paper introduces a\nuniversal boosting framework for current implicit video representation\napproaches. Specifically, we utilize a conditional decoder with a\ntemporal-aware affine transform module, which uses the frame index as a prior\ncondition to effectively align intermediate features with target frames.\nBesides, we introduce a sinusoidal NeRV-like block to generate diverse\nintermediate features and achieve a more balanced parameter distribution,\nthereby enhancing the model's capacity. With a high-frequency\ninformation-preserving reconstruction loss, our approach successfully boosts\nmultiple baseline INRs in the reconstruction quality and convergence speed for\nvideo regression, and exhibits superior inpainting and interpolation results.\nFurther, we integrate a consistent entropy minimization technique and develop\nvideo codecs based on these boosted INRs. Experiments on the UVG dataset\nconfirm that our enhanced codecs significantly outperform baseline INRs and\noffer competitive rate-distortion performance compared to traditional and\nlearning-based codecs.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18152v1",
    "authors": [
      "Xinjie Zhang",
      "Ren Yang",
      "Dailan He",
      "Xingtong Ge",
      "Tongda Xu",
      "Yan Wang",
      "Hongwei Qin",
      "Jun Zhang"
    ]
  },
  {
    "id": "2402.18154",
    "title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and\n  Mitigating Knowledge Conflicts in Language Models",
    "abstract": "  Recently, retrieval augmentation and tool augmentation have demonstrated a\nremarkable capability to expand the internal memory boundaries of language\nmodels (LMs) by providing external context. However, internal memory and\nexternal context inevitably clash, leading to knowledge conflicts within LMs.\nIn this paper, we aim to interpret the mechanism of knowledge conflicts through\nthe lens of information flow, and then mitigate conflicts by precise\ninterventions at the pivotal point. We find there are some attention heads with\nopposite effects in the later layers, where memory heads can recall knowledge\nfrom internal memory, and context heads can retrieve knowledge from external\ncontext. Moreover, we reveal that the pivotal point at which knowledge\nconflicts emerge in LMs is the integration of inconsistent information flows by\nmemory heads and context heads. Inspired by the insights, we propose a novel\nmethod called Pruning Head via PatH PatcHing (PH3), which can efficiently\nmitigate knowledge conflicts by pruning conflicting attention heads without\nupdating model parameters. PH3 can flexibly control eight LMs to use internal\nmemory ($\\uparrow$ 44.0%) or external context ($\\uparrow$ 38.5%). Moreover, PH3\ncan also improve the performance of LMs on open-domain QA tasks. We also\nconduct extensive experiments to demonstrate the cross-model, cross-relation,\nand cross-format generalization of our method.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18154v1",
    "authors": [
      "Zhuoran Jin",
      "Pengfei Cao",
      "Hongbang Yuan",
      "Yubo Chen",
      "Jiexin Xu",
      "Huaijun Li",
      "Xiaojian Jiang",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  {
    "id": "2402.18157",
    "title": "From Summary to Action: Enhancing Large Language Models for Complex\n  Tasks with Open World APIs",
    "abstract": "  The distinction between humans and animals lies in the unique ability of\nhumans to use and create tools. Tools empower humans to overcome physiological\nlimitations, fostering the creation of magnificent civilizations. Similarly,\nenabling foundational models like Large Language Models (LLMs) with the\ncapacity to learn external tool usage may serve as a pivotal step toward\nrealizing artificial general intelligence. Previous studies in this field have\npredominantly pursued two distinct approaches to augment the tool invocation\ncapabilities of LLMs. The first approach emphasizes the construction of\nrelevant datasets for model fine-tuning. The second approach, in contrast, aims\nto fully exploit the inherent reasoning abilities of LLMs through in-context\nlearning strategies. In this work, we introduce a novel tool invocation\npipeline designed to control massive real-world APIs. This pipeline mirrors the\nhuman task-solving process, addressing complicated real-life user queries. At\neach step, we guide LLMs to summarize the achieved results and determine the\nnext course of action. We term this pipeline `from Summary to action', Sum2Act\nfor short. Empirical evaluations of our Sum2Act pipeline on the ToolBench\nbenchmark show significant performance improvements, outperforming established\nmethods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in\nenhancing LLMs for complex real-world tasks.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18157v1",
    "authors": [
      "Yulong Liu",
      "Yunlong Yuan",
      "Chunwei Wang",
      "Jianhua Han",
      "Yongqiang Ma",
      "Li Zhang",
      "Nanning Zheng",
      "Hang Xu"
    ]
  },
  {
    "id": "2402.18225",
    "title": "CogBench: a large language model walks into a psychology lab",
    "abstract": "  Large language models (LLMs) have significantly advanced the field of\nartificial intelligence. Yet, evaluating them comprehensively remains\nchallenging. We argue that this is partly due to the predominant focus on\nperformance metrics in most benchmarks. This paper introduces CogBench, a\nbenchmark that includes ten behavioral metrics derived from seven cognitive\npsychology experiments. This novel approach offers a toolkit for phenotyping\nLLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse\ndataset. We analyze this data using statistical multilevel modeling techniques,\naccounting for the nested dependencies among fine-tuned versions of specific\nLLMs. Our study highlights the crucial role of model size and reinforcement\nlearning from human feedback (RLHF) in improving performance and aligning with\nhuman behavior. Interestingly, we find that open-source models are less\nrisk-prone than proprietary models and that fine-tuning on code does not\nnecessarily enhance LLMs' behavior. Finally, we explore the effects of\nprompt-engineering techniques. We discover that chain-of-thought prompting\nimproves probabilistic reasoning, while take-a-step-back prompting fosters\nmodel-based behaviors.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18225v1",
    "authors": ["Julian Coda-Forno", "Marcel Binz", "Jane X. Wang", "Eric Schulz"]
  },
  {
    "id": "2402.18285",
    "title": "PiShield: A NeSy Framework for Learning with Requirements",
    "abstract": "  Deep learning models have shown their strengths in various application\ndomains, however, they often struggle to meet safety requirements for their\noutputs. In this paper, we introduce PiShield, the first framework ever\nallowing for the integration of the requirements into the neural networks'\ntopology. PiShield guarantees compliance with these requirements, regardless of\ninput. Additionally, it allows for integrating requirements both at inference\nand/or training time, depending on the practitioners' needs. Given the\nwidespread application of deep learning, there is a growing need for frameworks\nallowing for the integration of the requirements across various domains. Here,\nwe explore three application scenarios: functional genomics, autonomous\ndriving, and tabular data generation.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18285v1",
    "authors": [
      "Mihaela Ctlina Stoian",
      "Alex Tatomir",
      "Thomas Lukasiewicz",
      "Eleonora Giunchiglia"
    ]
  },
  {
    "id": "2402.18286",
    "title": "Self-Supervised Learning in Electron Microscopy: Towards a Foundation\n  Model for Advanced Image Analysis",
    "abstract": "  In this work, we explore the potential of self-supervised learning from\nunlabeled electron microscopy datasets, taking a step toward building a\nfoundation model in this field. We show how self-supervised pretraining\nfacilitates efficient fine-tuning for a spectrum of downstream tasks, including\nsemantic segmentation, denoising, noise & background removal, and\nsuper-resolution. Experimentation with varying model complexities and receptive\nfield sizes reveals the remarkable phenomenon that fine-tuned models of lower\ncomplexity consistently outperform more complex models with random weight\ninitialization. We demonstrate the versatility of self-supervised pretraining\nacross various downstream tasks in the context of electron microscopy, allowing\nfaster convergence and better performance. We conclude that self-supervised\npretraining serves as a powerful catalyst, being especially advantageous when\nlimited annotated data are available and efficient scaling of computational\ncost are important.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18286v1",
    "authors": ["Bashir Kazimi", "Karina Ruzaeva", "Stefan Sandfeld"]
  },
  {
    "id": "2402.18292",
    "title": "FSL Model can Score Higher as It Is",
    "abstract": "  In daily life, we tend to present the front of our faces by staring squarely\nat a facial recognition machine, instead of facing it sideways, in order to\nincrease the chance of being correctly recognised. Few-shot-learning (FSL)\nclassification is challenging in itself because a model has to identify images\nthat belong to classes previously unseen during training. Therefore, a warped\nand non-typical query or support image during testing can make it even more\nchallenging for a model to predict correctly. In our work, to increase the\nchance of correct prediction during testing, we aim to rectify the test input\nof a trained FSL model by generating new samples of the tested classes through\nimage-to-image translation. An FSL model is usually trained on classes with\nsufficient samples, and then tested on classes with few-shot samples. Our\nproposed method first captures the style or shape of the test image, and then\nidentifies a suitable trained class sample. It then transfers the style or\nshape of the test image to the train-class images for generation of more\ntest-class samples, before performing classification based on a set of\ngenerated samples instead of just one sample. Our method has potential in\nempowering a trained FSL model to score higher during the testing phase without\nany extra training nor dataset. According to our experiments, by augmenting the\nsupport set with just 1 additional generated sample, we can achieve around 2%\nimprovement for trained FSL models on datasets consisting of either animal\nfaces or traffic signs. By augmenting both the support set and the queries, we\ncan achieve even more performance improvement. Our Github Repository is\npublicly available.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18292v1",
    "authors": ["Yunwei Bai", "Ying Kiat Tan", "Tsuhan Chen"]
  },
  {
    "id": "2402.18309",
    "title": "Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis",
    "abstract": "  In the efforts for safer roads, ensuring adequate vertical clearance above\nroadways is of great importance. Frequently, trees or other vegetation is\ngrowing above the roads, blocking the sight of traffic signs and lights and\nposing danger to traffic participants. Accurately estimating this space from\nsimple images proves challenging due to a lack of depth information. This is\nwhere LiDAR technology comes into play, a laser scanning sensor that reveals a\nthree-dimensional perspective. Thus far, LiDAR point clouds at the street level\nhave mainly been used for applications in the field of autonomous driving.\nThese scans, however, also open up possibilities in urban management. In this\npaper, we present a new point cloud algorithm that can automatically detect\nthose parts of the trees that grow over the street and need to be trimmed. Our\nsystem uses semantic segmentation to filter relevant points and downstream\nprocessing steps to create the required volume to be kept clear above the road.\nChallenges include obscured stretches of road, the noisy unstructured nature of\nLiDAR point clouds, and the assessment of the road shape. The identified points\nof non-compliant trees can be projected from the point cloud onto images,\nproviding municipalities with a visual aid for dealing with such occurrences.\nBy automating this process, municipalities can address potential road space\nconstraints, enhancing safety for all. They may also save valuable time by\ncarrying out the inspections more systematically. Our open-source code gives\ncommunities inspiration on how to automate the process themselves.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18309v1",
    "authors": ["Miriam Louise Carnot", "Eric Peukert", "Bogdan Franczyk"]
  },
  {
    "id": "2402.18360",
    "title": "Similarity-based analogical proportions",
    "abstract": "  The author has recently introduced abstract algebraic frameworks of\nanalogical proportions and similarity within the general setting of universal\nalgebra. The purpose of this paper is to build a bridge from similarity to\nanalogical proportions by formulating the latter in terms of the former. The\nbenefit of this similarity-based approach is that the connection between\nproportions and similarity is built into the framework and therefore evident\nwhich is appealing since proportions and similarity are both at the center of\nanalogy; moreover, future results on similarity can directly be applied to\nanalogical proportions.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18360v1",
    "authors": ["Christian Anti"]
  },
  {
    "id": "2402.18377",
    "title": "Out-of-Domain Generalization in Dynamical Systems Reconstruction",
    "abstract": "  In science we are interested in finding the governing equations, the\ndynamical rules, underlying empirical phenomena. While traditionally scientific\nmodels are derived through cycles of human insight and experimentation,\nrecently deep learning (DL) techniques have been advanced to reconstruct\ndynamical systems (DS) directly from time series data. State-of-the-art\ndynamical systems reconstruction (DSR) methods show promise in capturing\ninvariant and long-term properties of observed DS, but their ability to\ngeneralize to unobserved domains remains an open challenge. Yet, this is a\ncrucial property we would expect from any viable scientific theory. In this\nwork, we provide a formal framework that addresses generalization in DSR. We\nexplain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly\ndiffers from OODG considered elsewhere in machine learning. We introduce\nmathematical notions based on topological concepts and ergodic theory to\nformalize the idea of learnability of a DSR model. We formally prove that\nblack-box DL techniques, without adequate structural priors, generally will not\nbe able to learn a generalizing DSR model. We also show this empirically,\nconsidering major classes of DSR algorithms proposed so far, and illustrate\nwhere and why they fail to generalize across the whole phase space. Our study\nprovides the first comprehensive mathematical treatment of OODG in DSR, and\ngives a deeper conceptual understanding of where the fundamental problems in\nOODG lie and how they could possibly be addressed in practice.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18377v1",
    "authors": [
      "Niclas Gring",
      "Florian Hess",
      "Manuel Brenner",
      "Zahra Monfared",
      "Daniel Durstewitz"
    ]
  },
  {
    "id": "2402.18381",
    "title": "Large Language Models As Evolution Strategies",
    "abstract": "  Large Transformer models are capable of implementing a plethora of so-called\nin-context learning algorithms. These include gradient descent, classification,\nsequence completion, transformation, and improvement. In this work, we\ninvestigate whether large language models (LLMs), which never explicitly\nencountered the task of black-box optimization, are in principle capable of\nimplementing evolutionary optimization algorithms. While previous works have\nsolely focused on language-based task specification, we move forward and focus\non the zero-shot application of LLMs to black-box optimization. We introduce a\nnovel prompting strategy, consisting of least-to-most sorting of discretized\npopulation members and querying the LLM to propose an improvement to the mean\nstatistic, i.e. perform a type of black-box recombination operation.\nEmpirically, we find that our setup allows the user to obtain an LLM-based\nevolution strategy, which we call `EvoLLM', that robustly outperforms baseline\nalgorithms such as random search and Gaussian Hill Climbing on synthetic BBOB\nfunctions as well as small neuroevolution tasks. Hence, LLMs can act as\n`plug-in' in-context recombination operators. We provide several comparative\nstudies of the LLM's model size, prompt strategy, and context construction.\nFinally, we show that one can flexibly improve EvoLLM's performance by\nproviding teacher algorithm information via instruction fine-tuning on\npreviously collected teacher optimization trajectories.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18381v1",
    "authors": ["Robert Tjarko Lange", "Yingtao Tian", "Yujin Tang"]
  },
  {
    "id": "2402.18392",
    "title": "Unveiling the Potential of Robustness in Evaluating Causal Inference\n  Models",
    "abstract": "  The growing demand for personalized decision-making has led to a surge of\ninterest in estimating the Conditional Average Treatment Effect (CATE). The\nintersection of machine learning and causal inference has yielded various\neffective CATE estimators. However, deploying these estimators in practice is\noften hindered by the absence of counterfactual labels, making it challenging\nto select the desirable CATE estimator using conventional model selection\nprocedures like cross-validation. Existing approaches for CATE estimator\nselection, such as plug-in and pseudo-outcome metrics, face two inherent\nchallenges. Firstly, they are required to determine the metric form and the\nunderlying machine learning models for fitting nuisance parameters or plug-in\nlearners. Secondly, they lack a specific focus on selecting a robust estimator.\nTo address these challenges, this paper introduces a novel approach, the\nDistributionally Robust Metric (DRM), for CATE estimator selection. The\nproposed DRM not only eliminates the need to fit additional models but also\nexcels at selecting a robust CATE estimator. Experimental studies demonstrate\nthe efficacy of the DRM method, showcasing its consistent effectiveness in\nidentifying superior estimators while mitigating the risk of selecting inferior\nones.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18392v1",
    "authors": ["Yiyan Huang", "Cheuk Hang Leung", "Siyi Wang", "Yijun Li", "Qi Wu"]
  },
  {
    "id": "2402.18393",
    "title": "Evaluating Decision Optimality of Autonomous Driving via Metamorphic\n  Testing",
    "abstract": "  Autonomous Driving System (ADS) testing is crucial in ADS development, with\nthe current primary focus being on safety. However, the evaluation of\nnon-safety-critical performance, particularly the ADS's ability to make optimal\ndecisions and produce optimal paths for autonomous vehicles (AVs), is equally\nvital to ensure the intelligence and reduce risks of AVs. Currently, there is\nlittle work dedicated to assessing ADSs' optimal decision-making performance\ndue to the lack of corresponding oracles and the difficulty in generating\nscenarios with non-optimal decisions. In this paper, we focus on evaluating the\ndecision-making quality of an ADS and propose the first method for detecting\nnon-optimal decision scenarios (NoDSs), where the ADS does not compute optimal\npaths for AVs. Firstly, to deal with the oracle problem, we propose a novel\nmetamorphic relation (MR) aimed at exposing violations of optimal decisions.\nThe MR identifies the property that the ADS should retain optimal decisions\nwhen the optimal path remains unaffected by non-invasive changes. Subsequently,\nwe develop a new framework, Decictor, designed to generate NoDSs efficiently.\nDecictor comprises three main components: Non-invasive Mutation, MR Check, and\nFeedback. The Non-invasive Mutation ensures that the original optimal path in\nthe mutated scenarios is not affected, while the MR Check is responsible for\ndetermining whether non-optimal decisions are made. To enhance the\neffectiveness of identifying NoDSs, we design a feedback metric that combines\nboth spatial and temporal aspects of the AV's movement. We evaluate Decictor on\nBaidu Apollo, an open-source and production-grade ADS. The experimental results\nvalidate the effectiveness of Decictor in detecting non-optimal decisions of\nADSs. Our work provides valuable and original insights into evaluating the\nnon-safety-critical performance of ADSs.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18393v1",
    "authors": [
      "Mingfei Cheng",
      "Yuan Zhou",
      "Xiaofei Xie",
      "Junjie Wang",
      "Guozhu Meng",
      "Kairui Yang"
    ]
  },
  {
    "id": "2402.18409",
    "title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision Language Models",
    "abstract": "  Large Vision Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the \"Cookie Theft\" task in human cognition test, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive ability of LVLMs\nusing images with rich semantics. It defines eight reasoning capabilities and\nconsists of an image description task and a visual question answering task. Our\nevaluation on well-known LVLMs shows that there is still a large gap in\ncognitive ability between LVLMs and humans.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18409v2",
    "authors": ["Xiujie Song", "Mengyue Wu", "Kenny Q. Zhu", "Chunhao Zhang", "Yanyi Chen"]
  },
  {
    "id": "2402.18419",
    "title": "Can GPT Improve the State of Prior Authorization via Guideline Based\n  Automated Question Answering?",
    "abstract": "  Health insurance companies have a defined process called prior authorization\n(PA) which is a health plan cost-control process that requires doctors and\nother healthcare professionals to get clearance in advance from a health plan\nbefore performing a particular procedure on a patient in order to be eligible\nfor payment coverage. For health insurance companies, approving PA requests for\npatients in the medical domain is a time-consuming and challenging task. One of\nthose key challenges is validating if a request matches up to certain criteria\nsuch as age, gender, etc. In this work, we evaluate whether GPT can validate\nnumerous key factors, in turn helping health plans reach a decision drastically\nfaster. We frame it as a question answering task, prompting GPT to answer a\nquestion from patient electronic health record. We experiment with different\nconventional prompting techniques as well as introduce our own novel prompting\ntechnique. Moreover, we report qualitative assessment by humans on the natural\nlanguage generation outputs from our approach. Results show that our method\nachieves superior performance with the mean weighted F1 score of 0.61 as\ncompared to its standard counterparts.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18419v1",
    "authors": ["Shubham Vatsal", "Ayush Singh", "Shabnam Tafreshi"]
  },
  {
    "id": "2402.18424",
    "title": "Emotion Classification in Low and Moderate Resource Languages",
    "abstract": "  It is important to be able to analyze the emotional state of people around\nthe globe. There are 7100+ active languages spoken around the world and\nbuilding emotion classification for each language is labor intensive.\nParticularly for low-resource and endangered languages, building emotion\nclassification can be quite challenging. We present a cross-lingual emotion\nclassifier, where we train an emotion classifier with resource-rich languages\n(i.e. \\textit{English} in our work) and transfer the learning to low and\nmoderate resource languages. We compare and contrast two approaches of transfer\nlearning from a high-resource language to a low or moderate-resource language.\nOne approach projects the annotation from a high-resource language to low and\nmoderate-resource language in parallel corpora and the other one uses direct\ntransfer from high-resource language to the other languages. We show the\nefficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano,\nOdia, and Azerbaijani. Our results indicate that our approaches outperform\nrandom baselines and transfer emotions across languages successfully. For all\nlanguages, the direct cross-lingual transfer of emotion yields better results.\nWe also create annotated emotion-labeled resources for four languages: Farsi,\nAzerbaijani, Ilocano and Odia.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18424v1",
    "authors": ["Shabnam Tafreshi", "Shubham Vatsal", "Mona Diab"]
  },
  {
    "id": "2402.18449",
    "title": "HOP to the Next Tasks and Domains for Continual Learning in NLP",
    "abstract": "  Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and\ndomains) by transferring knowledge acquired on previous problems, whilst\navoiding forgetting of past ones. Different from previous approaches which\nfocused on CL for one NLP task or domain in a specific use-case, in this paper,\nwe address a more general CL setting to learn from a sequence of problems in a\nunique framework. Our method, HOP, permits to hop across tasks and domains by\naddressing the CL problem along three directions: (i) we employ a set of\nadapters to generalize a large pre-trained model to unseen problems, (ii) we\ncompute high-order moments over the distribution of embedded representations to\ndistinguish independent and correlated statistics across different tasks and\ndomains, (iii) we process this enriched information with auxiliary heads\nspecialized for each end problem. Extensive experimental campaign on 4 NLP\napplications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of\nour HOP.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18449v1",
    "authors": ["Umberto Michieli", "Mete Ozay"]
  },
  {
    "id": "2402.18477",
    "title": "Signature Kernel Conditional Independence Tests in Causal Discovery for\n  Stochastic Processes",
    "abstract": "  Inferring the causal structure underlying stochastic dynamical systems from\nobservational data holds great promise in domains ranging from science and\nhealth to finance. Such processes can often be accurately modeled via\nstochastic differential equations (SDEs), which naturally imply causal\nrelationships via \"which variables enter the differential of which other\nvariables\". In this paper, we develop a kernel-based test of conditional\nindependence (CI) on \"path-space\" -- solutions to SDEs -- by leveraging recent\nadvances in signature kernels. We demonstrate strictly superior performance of\nour proposed CI test compared to existing approaches on path-space. Then, we\ndevelop constraint-based causal discovery algorithms for acyclic stochastic\ndynamical systems (allowing for loops) that leverage temporal information to\nrecover the entire directed graph. Assuming faithfulness and a CI oracle, our\nalgorithm is sound and complete. We empirically verify that our developed CI\ntest in conjunction with the causal discovery algorithm reliably outperforms\nbaselines across a range of settings.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18477v1",
    "authors": [
      "Georg Manten",
      "Cecilia Casolo",
      "Emilio Ferrucci",
      "Sren Wengel Mogensen",
      "Cristopher Salvi",
      "Niki Kilbertus"
    ]
  },
  {
    "id": "2402.18487",
    "title": "Human-Centric Aware UAV Trajectory Planning in Search and Rescue\n  Missions Employing Multi-Objective Reinforcement Learning with AHP and\n  Similarity-Based Experience Replay",
    "abstract": "  The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue\n(SAR) missions presents a promising avenue for enhancing operational efficiency\nand effectiveness. However, the success of these missions is not solely\ndependent on the technical capabilities of the drones but also on their\nacceptance and interaction with humans on the ground. This paper explores the\neffect of human-centric factor in UAV trajectory planning for SAR missions. We\nintroduce a novel approach based on the reinforcement learning augmented with\nAnalytic Hierarchy Process and novel similarity-based experience replay to\noptimize UAV trajectories, balancing operational objectives with human comfort\nand safety considerations. Additionally, through a comprehensive survey, we\ninvestigate the impact of gender cues and anthropomorphism in UAV design on\npublic acceptance and trust, revealing significant implications for drone\ninteraction strategies in SAR. Our contributions include (1) a reinforcement\nlearning framework for UAV trajectory planning that dynamically integrates\nmulti-objective considerations, (2) an analysis of human perceptions towards\ngendered and anthropomorphized drones in SAR contexts, and (3) the application\nof similarity-based experience replay for enhanced learning efficiency in\ncomplex SAR scenarios. The findings offer valuable insights into designing UAV\nsystems that are not only technically proficient but also aligned with\nhuman-centric values.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18487v1",
    "authors": ["Mahya Ramezani", "Jose Luis Sanchez-Lopez"]
  },
  {
    "id": "2402.18540",
    "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt\n  Templates",
    "abstract": "  Public LLMs such as the Llama 2-Chat have driven huge activity in LLM\nresearch. These models underwent alignment training and were considered safe.\nRecently Qi et al. (2023) reported that even benign fine-tuning (e.g., on\nseemingly safe datasets) can give rise to unsafe behaviors in the models. The\ncurrent paper is about methods and best practices to mitigate such loss of\nalignment. Through extensive experiments on several chat models (Meta's Llama\n2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo),\nthis paper uncovers that the prompt templates used during fine-tuning and\ninference play a crucial role in preserving safety alignment, and proposes the\n\"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a\nsafety prompt, but include it at test time. Fine-tuning experiments on GSM8K,\nChatDoctor, and OpenOrca show that PTST significantly reduces the rise of\nunsafe behaviors, and even almost eliminates them in some cases.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18540v1",
    "authors": [
      "Kaifeng Lyu",
      "Haoyu Zhao",
      "Xinran Gu",
      "Dingli Yu",
      "Anirudh Goyal",
      "Sanjeev Arora"
    ]
  },
  {
    "id": "2402.18563",
    "title": "Approaching Human-Level Forecasting with Language Models",
    "abstract": "  Forecasting future events is important for policy and decision making. In\nthis work, we study whether language models (LMs) can forecast at the level of\ncompetitive human forecasters. Towards this goal, we develop a\nretrieval-augmented LM system designed to automatically search for relevant\ninformation, generate forecasts, and aggregate predictions. To facilitate our\nstudy, we collect a large dataset of questions from competitive forecasting\nplatforms. Under a test set published after the knowledge cut-offs of our LMs,\nwe evaluate the end-to-end performance of our system against the aggregates of\nhuman forecasts. On average, the system nears the crowd aggregate of\ncompetitive forecasters, and in some settings surpasses it. Our work suggests\nthat using LMs to forecast the future could provide accurate predictions at\nscale and help to inform institutional decision making.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18563v1",
    "authors": ["Danny Halawi", "Fred Zhang", "Chen Yueh-Han", "Jacob Steinhardt"]
  },
  {
    "id": "2402.18571",
    "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional\n  Preference Alignment with Multi-Objective Rewards",
    "abstract": "  Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18571v2",
    "authors": [
      "Haoxiang Wang",
      "Yong Lin",
      "Wei Xiong",
      "Rui Yang",
      "Shizhe Diao",
      "Shuang Qiu",
      "Han Zhao",
      "Tong Zhang"
    ]
  },
  {
    "id": "2402.18600",
    "title": "Artificial Intelligence and Diabetes Mellitus: An Inside Look Through\n  the Retina",
    "abstract": "  Diabetes mellitus (DM) predisposes patients to vascular complications.\nRetinal images and vasculature reflect the body's micro- and macrovascular\nhealth. They can be used to diagnose DM complications, including diabetic\nretinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular\ndisease, as well as forecast the risk of cardiovascular events. Artificial\nintelligence (AI)-enabled systems developed for high-throughput detection of DR\nusing digitized retinal images have become clinically adopted. Beyond DR\nscreening, AI integration also holds immense potential to address challenges\nassociated with the holistic care of the patient with DM. In this work, we aim\nto comprehensively review the literature for studies on AI applications based\non retinal images related to DM diagnosis, prognostication, and management. We\nwill describe the findings of holistic AI-assisted diabetes care, including but\nnot limited to DR screening, and discuss barriers to implementing such systems,\nincluding issues concerning ethics, data privacy, equitable access, and\nexplainability. With the ability to evaluate the patient's health status vis a\nvis DM complication as well as risk prognostication of future cardiovascular\ncomplications, AI-assisted retinal image analysis has the potential to become a\ncentral tool for modern personalized medicine in patients with DM.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18600v1",
    "authors": [
      "Yasin Sadeghi Bazargani",
      "Majid Mirzaei",
      "Navid Sobhi",
      "Mirsaeed Abdollahi",
      "Ali Jafarizadeh",
      "Siamak Pedrammehr",
      "Roohallah Alizadehsani",
      "Ru San Tan",
      "Sheikh Mohammed Shariful Islam",
      "U. Rajendra Acharya"
    ]
  },
  {
    "id": "2402.18603",
    "title": "MMSR: Symbolic Regression is a Multimodal Task",
    "abstract": "  Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression. Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and GP and reinforcement learning\nalgorithms were used to solve it. However, GP is sensitive to hyperparameters,\nand these two types of algorithms are inefficient. To solve this problem,\nresearchers treat the mapping from data to expressions as a translation\nproblem. And the corresponding large-scale pre-trained model is introduced.\nHowever, the data and expression skeletons do not have very clear word\ncorrespondences as the two languages do. Instead, they are more like two\nmodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.\nThe SR problem is solved as a pure multimodal problem, and contrastive learning\nis also introduced in the training process for modal alignment to facilitate\nlater modal feature fusion. It is worth noting that in order to better promote\nthe modal feature fusion, we adopt the strategy of training contrastive\nlearning loss and other losses at the same time, which only needs one-step\ntraining, instead of training contrastive learning loss first and then training\nother losses. Because our experiments prove training together can make the\nfeature extraction module and feature fusion module running-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18603v1",
    "authors": [
      "Yanjie Li",
      "Jingyi Liu",
      "Weijun Li",
      "Lina Yu",
      "Min Wu",
      "Wenqiang Li",
      "Meilan Hao",
      "Su Wei",
      "Yusong Deng"
    ]
  },
  {
    "id": "2402.18606",
    "title": "Impact of network topology on the performance of Decentralized Federated\n  Learning",
    "abstract": "  Fully decentralized learning is gaining momentum for training AI models at\nthe Internet's edge, addressing infrastructure challenges and privacy concerns.\nIn a decentralized machine learning system, data is distributed across multiple\nnodes, with each node training a local model based on its respective dataset.\nThe local models are then shared and combined to form a global model capable of\nmaking accurate predictions on new data. Our exploration focuses on how\ndifferent types of network structures influence the spreading of knowledge -\nthe process by which nodes incorporate insights gained from learning patterns\nin data available on other nodes across the network. Specifically, this study\ninvestigates the intricate interplay between network structure and learning\nperformance using three network topologies and six data distribution methods.\nThese methods consider different vertex properties, including degree\ncentrality, betweenness centrality, and clustering coefficient, along with\nwhether nodes exhibit high or low values of these metrics. Our findings\nunderscore the significance of global centrality metrics (degree, betweenness)\nin correlating with learning performance, while local clustering proves less\npredictive. We highlight the challenges in transferring knowledge from\nperipheral to central nodes, attributed to a dilution effect during model\naggregation. Additionally, we observe that central nodes exert a pull effect,\nfacilitating the spread of knowledge. In examining degree distribution, hubs in\nBarabasi-Albert networks positively impact learning for central nodes but\nexacerbate dilution when knowledge originates from peripheral nodes. Finally,\nwe demonstrate the formidable challenge of knowledge circulation outside of\nsegregated communities.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18606v1",
    "authors": [
      "Luigi Palmieri",
      "Chiara Boldrini",
      "Lorenzo Valerio",
      "Andrea Passarella",
      "Marco Conti"
    ]
  },
  {
    "id": "2402.18607",
    "title": "Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An\n  Adversarial Perspective",
    "abstract": "  Diffusion models have recently gained significant attention in both academia\nand industry due to their impressive generative performance in terms of both\nsampling quality and distribution coverage. Accordingly, proposals are made for\nsharing pre-trained diffusion models across different organizations, as a way\nof improving data utilization while enhancing privacy protection by avoiding\nsharing private data directly. However, the potential risks associated with\nsuch an approach have not been comprehensively examined.\n  In this paper, we take an adversarial perspective to investigate the\npotential privacy and fairness risks associated with the sharing of diffusion\nmodels. Specifically, we investigate the circumstances in which one party (the\nsharer) trains a diffusion model using private data and provides another party\n(the receiver) black-box access to the pre-trained model for downstream tasks.\nWe demonstrate that the sharer can execute fairness poisoning attacks to\nundermine the receiver's downstream models by manipulating the training data\ndistribution of the diffusion model. Meanwhile, the receiver can perform\nproperty inference attacks to reveal the distribution of sensitive features in\nthe sharer's dataset. Our experiments conducted on real-world datasets\ndemonstrate remarkable attack performance on different types of diffusion\nmodels, which highlights the critical importance of robust data auditing and\nprivacy protection protocols in pertinent applications.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18607v2",
    "authors": [
      "Xinjian Luo",
      "Yangfan Jiang",
      "Fei Wei",
      "Yuncheng Wu",
      "Xiaokui Xiao",
      "Beng Chin Ooi"
    ]
  },
  {
    "id": "2402.18617",
    "title": "ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games",
    "abstract": "  Offline learning has become widely used due to its ability to derive\neffective policies from offline datasets gathered by expert demonstrators\nwithout interacting with the environment directly. Recent research has explored\nvarious ways to enhance offline learning efficiency by considering the\ncharacteristics (e.g., expertise level or multiple demonstrators) of the\ndataset. However, a different approach is necessary in the context of zero-sum\ngames, where outcomes vary significantly based on the strategy of the opponent.\nIn this study, we introduce a novel approach that uses unsupervised learning\ntechniques to estimate the exploited level of each trajectory from the offline\ndataset of zero-sum games made by diverse demonstrators. Subsequently, we\nincorporate the estimated exploited level into the offline learning to maximize\nthe influence of the dominant strategy. Our method enables interpretable\nexploited level estimation in multiple zero-sum games and effectively\nidentifies dominant strategy data. Also, our exploited level augmented offline\nlearning significantly enhances the original offline learning algorithms\nincluding imitation learning and offline reinforcement learning for zero-sum\ngames.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18617v1",
    "authors": ["Shiqi Lei", "Kanghoon Lee", "Linjing Li", "Jinkyoo Park", "Jiachen Li"]
  },
  {
    "id": "2402.18659",
    "title": "Large Language Models and Games: A Survey and Roadmap",
    "abstract": "  Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18659v1",
    "authors": [
      "Roberto Gallotta",
      "Graham Todd",
      "Marvin Zammit",
      "Sam Earle",
      "Antonios Liapis",
      "Julian Togelius",
      "Georgios N. Yannakakis"
    ]
  },
  {
    "id": "2402.18677",
    "title": "Fault Tolerant Neural Control Barrier Functions for Robotic Systems\n  under Sensor Faults and Attacks",
    "abstract": "  Safety is a fundamental requirement of many robotic systems. Control barrier\nfunction (CBF)-based approaches have been proposed to guarantee the safety of\nrobotic systems. However, the effectiveness of these approaches highly relies\non the choice of CBFs. Inspired by the universal approximation power of neural\nnetworks, there is a growing trend toward representing CBFs using neural\nnetworks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however,\nare trained and deployed in benign environments, making them ineffective for\nscenarios where robotic systems experience sensor faults and attacks. In this\npaper, we study safety-critical control synthesis for robotic systems under\nsensor faults and attacks. Our main contribution is the development and\nsynthesis of a new class of CBFs that we term fault tolerant neural control\nbarrier function (FT-NCBF). We derive the necessary and sufficient conditions\nfor FT-NCBFs to guarantee safety, and develop a data-driven method to learn\nFT-NCBFs by minimizing a loss function constructed using the derived\nconditions. Using the learned FT-NCBF, we synthesize a control input and\nformally prove the safety guarantee provided by our approach. We demonstrate\nour proposed approach using two case studies: obstacle avoidance problem for an\nautonomous mobile robot and spacecraft rendezvous problem, with code available\nvia https://github.com/HongchaoZhang-HZ/FTNCBF.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18677v1",
    "authors": ["Hongchao Zhang", "Luyao Niu", "Andrew Clark", "Radha Poovendran"]
  },
  {
    "id": "2402.18700",
    "title": "Learning to Compress Prompt in Natural Language Formats",
    "abstract": "  Large language models (LLMs) are great at processing multiple natural\nlanguage processing tasks, but their abilities are constrained by inferior\nperformance with long context, slow inference speed, and the high cost of\ncomputing the results. Deploying LLMs with precise and informative context\nhelps users process large-scale datasets more effectively and cost-efficiently.\nExisting works rely on compressing long prompt contexts into soft prompts.\nHowever, soft prompt compression encounters limitations in transferability\nacross different LLMs, especially API-based LLMs. To this end, this work aims\nto compress lengthy prompts in the form of natural language with LLM\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\nimposing length constraints. In this work, we propose a Natural Language Prompt\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\nformatted Capsule Prompt while maintaining the prompt utility and\ntransferability. Specifically, to tackle the first challenge, the\nNano-Capsulator is optimized by a reward function that interacts with the\nproposed semantics preserving loss. To address the second question, the\nNano-Capsulator is optimized by a reward function featuring length constraints.\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\nbudget overheads while providing transferability across diverse LLMs and\ndifferent datasets.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18700v1",
    "authors": [
      "Yu-Neng Chuang",
      "Tianwei Xing",
      "Chia-Yuan Chang",
      "Zirui Liu",
      "Xun Chen",
      "Xia Hu"
    ]
  },
  {
    "id": "2402.18724",
    "title": "Learning Associative Memories with Gradient Descent",
    "abstract": "  This work focuses on the training dynamics of one associative memory module\nstoring outer products of token embeddings. We reduce this problem to the study\nof a system of particles, which interact according to properties of the data\ndistribution and correlations between embeddings. Through theory and\nexperiments, we provide several insights. In overparameterized regimes, we\nobtain logarithmic growth of the ``classification margins.'' Yet, we show that\nimbalance in token frequencies and memory interferences due to correlated\nembeddings lead to oscillatory transitory regimes. The oscillations are more\npronounced with large step sizes, which can create benign loss spikes, although\nthese learning rates speed up the dynamics and accelerate the asymptotic\nconvergence. In underparameterized regimes, we illustrate how the cross-entropy\nloss can lead to suboptimal memorization schemes. Finally, we assess the\nvalidity of our findings on small Transformer models.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18724v1",
    "authors": ["Vivien Cabannes", "Berfin Simsek", "Alberto Bietti"]
  },
  {
    "id": "2402.18726",
    "title": "Unveiling Privacy, Memorization, and Input Curvature Links",
    "abstract": "  Deep Neural Nets (DNNs) have become a pervasive tool for solving many\nemerging problems. However, they tend to overfit to and memorize the training\nset. Memorization is of keen interest since it is closely related to several\nconcepts such as generalization, noisy learning, and privacy. To study\nmemorization, Feldman (2019) proposed a formal score, however its computational\nrequirements limit its practical use. Recent research has shown empirical\nevidence linking input loss curvature (measured by the trace of the loss\nHessian w.r.t inputs) and memorization. It was shown to be ~3 orders of\nmagnitude more efficient than calculating the memorization score. However,\nthere is a lack of theoretical understanding linking memorization with input\nloss curvature. In this paper, we not only investigate this connection but also\nextend our analysis to establish theoretical links between differential\nprivacy, memorization, and input loss curvature. First, we derive an upper\nbound on memorization characterized by both differential privacy and input loss\ncurvature. Second, we present a novel insight showing that input loss curvature\nis upper-bounded by the differential privacy parameter. Our theoretical\nfindings are further empirically validated using deep models on CIFAR and\nImageNet datasets, showing a strong correlation between our theoretical\npredictions and results observed in practice.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18726v1",
    "authors": ["Deepak Ravikumar", "Efstathia Soufleri", "Abolfazl Hashemi", "Kaushik Roy"]
  },
  {
    "id": "2402.18759",
    "title": "Learning with Language-Guided State Abstractions",
    "abstract": "  We describe a framework for using natural language to design state\nabstractions for imitation learning. Generalizable policy learning in\nhigh-dimensional observation spaces is facilitated by well-designed state\nrepresentations, which can surface important features of an environment and\nhide irrelevant ones. These state representations are typically manually\nspecified, or derived from other labor-intensive labeling procedures. Our\nmethod, LGA (language-guided abstraction), uses a combination of natural\nlanguage supervision and background knowledge from language models (LMs) to\nautomatically build state representations tailored to unseen tasks. In LGA, a\nuser first provides a (possibly incomplete) description of a target task in\nnatural language; next, a pre-trained LM translates this task description into\na state abstraction function that masks out irrelevant features; finally, an\nimitation policy is trained using a small number of demonstrations and\nLGA-generated abstract states. Experiments on simulated robotic tasks show that\nLGA yields state abstractions similar to those designed by humans, but in a\nfraction of the time, and that these abstractions improve generalization and\nrobustness in the presence of spurious correlations and ambiguous\nspecifications. We illustrate the utility of the learned abstractions on mobile\nmanipulation tasks with a Spot robot.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18759v1",
    "authors": [
      "Andi Peng",
      "Ilia Sucholutsky",
      "Belinda Z. Li",
      "Theodore R. Sumers",
      "Thomas L. Griffiths",
      "Jacob Andreas",
      "Julie A. Shah"
    ]
  },
  {
    "id": "2403.00023",
    "title": "Auditable Homomorphic-based Decentralized Collaborative AI with\n  Attribute-based Differential Privacy",
    "abstract": "  In recent years, the notion of federated learning (FL) has led to the new\nparadigm of distributed artificial intelligence (AI) with privacy preservation.\nHowever, most current FL systems suffer from data privacy issues due to the\nrequirement of a trusted third party. Although some previous works introduce\ndifferential privacy to protect the data, however, it may also significantly\ndeteriorate the model performance. To address these issues, we propose a novel\ndecentralized collaborative AI framework, named Auditable Homomorphic-based\nDecentralised Collaborative AI (AerisAI), to improve security with homomorphic\nencryption and fine-grained differential privacy. Our proposed AerisAI directly\naggregates the encrypted parameters with a blockchain-based smart contract to\nget rid of the need of a trusted third party. We also propose a brand-new\nconcept for eliminating the negative impacts of differential privacy for model\nperformance. Moreover, the proposed AerisAI also provides the broadcast-aware\ngroup key management based on ciphertext-policy attribute-based encryption\n(CPABE) to achieve fine-grained access control based on different service-level\nagreements. We provide a formal theoretical analysis of the proposed AerisAI as\nwell as the functionality comparison with the other baselines. We also conduct\nextensive experiments on real datasets to evaluate the proposed approach. The\nexperimental results indicate that our proposed AerisAI significantly\noutperforms the other state-of-the-art baselines.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00023v1",
    "authors": ["Lo-Yao Yeh", "Sheng-Po Tseng", "Chia-Hsun Lu", "Chih-Ya Shen"]
  },
  {
    "id": "2403.00026",
    "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated\n  Vehicle Routing Problem",
    "abstract": "  In this paper, we present the Foundation Model for the Montreal Capacitated\nVehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that\napproximates high-quality solutions to a variant of the Capacitated Vehicle\nRouting Problem (CVRP) that characterizes many real-world applications. The\nso-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally\ndescribed by Bengio et al. (2021), is defined on a fixed and finite graph,\nwhich is analogous to a city. Each MCVRP instance is essentially the sub-graph\nconnecting a randomly sampled subset of the nodes in the fixed graph, which\nrepresent a set of potential addresses in a real-world delivery problem on a\ngiven day. Our work exploits this problem structure to frame the MCVRP as an\nanalogous Natural Language Processing (NLP) task. Specifically, we leverage a\nTransformer architecture embedded in a Large Language Model (LLM) framework to\ntrain our model in a supervised manner on computationally inexpensive,\nsub-optimal MCVRP solutions obtained algorithmically. Through comprehensive\ncomputational experiments, we show that FM-MCVRP produces better MCVRP\nsolutions than the training data and generalizes to larger sized problem\ninstances not seen during training. Even when compared to near-optimal\nsolutions from state-of-the-art heuristics, FM-MCVRP yields competitive results\ndespite being trained on inferior data. For instance, for 400-customer\nproblems, FM-MCVRP solutions on average fall within 2% of the benchmark. Our\nresults further demonstrate that unlike prior works in the literature, FM-MCVRP\nis a unified model, which performs consistently and reliably on a range of\nproblem instance sizes and parameter values such as the vehicle capacity.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00026v1",
    "authors": ["Samuel J. K. Chin", "Matthias Winkenbach", "Akash Srivastava"]
  },
  {
    "id": "2403.00030",
    "title": "GraphPub: Generation of Differential Privacy Graph with High\n  Availability",
    "abstract": "  In recent years, with the rapid development of graph neural networks (GNN),\nmore and more graph datasets have been published for GNN tasks. However, when\nan upstream data owner publishes graph data, there are often many privacy\nconcerns, because many real-world graph data contain sensitive information like\nperson's friend list. Differential privacy (DP) is a common method to protect\nprivacy, but due to the complex topological structure of graph data, applying\nDP on graphs often affects the message passing and aggregation of GNN models,\nleading to a decrease in model accuracy. In this paper, we propose a novel\ngraph edge protection framework, graph publisher (GraphPub), which can protect\ngraph topology while ensuring that the availability of data is basically\nunchanged. Through reverse learning and the encoder-decoder mechanism, we\nsearch for some false edges that do not have a large negative impact on the\naggregation of node features, and use them to replace some real edges. The\nmodified graph will be published, which is difficult to distinguish between\nreal and false data. Sufficient experiments prove that our framework achieves\nmodel accuracy close to the original graph with an extremely low privacy\nbudget.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00030v2",
    "authors": ["Wanghan Xu", "Bin Shi", "Ao Liu", "Jiqiang Zhang", "Bo Dong"]
  },
  {
    "id": "2403.00032",
    "title": "Time to Cite: Modeling Citation Networks using the Dynamic Impact\n  Single-Event Embedding Model",
    "abstract": "  Understanding the structure and dynamics of scientific research, i.e., the\nscience of science (SciSci), has become an important area of research in order\nto address imminent questions including how scholars interact to advance\nscience, how disciplines are related and evolve, and how research impact can be\nquantified and predicted. Central to the study of SciSci has been the analysis\nof citation networks. Here, two prominent modeling methodologies have been\nemployed: one is to assess the citation impact dynamics of papers using\nparametric distributions, and the other is to embed the citation networks in a\nlatent space optimal for characterizing the static relations between papers in\nterms of their citations. Interestingly, citation networks are a prominent\nexample of single-event dynamic networks, i.e., networks for which each dyad\nonly has a single event (i.e., the point in time of citation). We presently\npropose a novel likelihood function for the characterization of such\nsingle-event networks. Using this likelihood, we propose the Dynamic Impact\nSingle-Event Embedding model (DISEE). The \\textsc{\\modelabbrev} model\ncharacterizes the scientific interactions in terms of a latent distance model\nin which random effects account for citation heterogeneity while the\ntime-varying impact is characterized using existing parametric representations\nfor assessment of dynamic impact. We highlight the proposed approach on several\nreal citation networks finding that the DISEE well reconciles static latent\ndistance network embedding approaches with classical dynamic impact\nassessments.\n",
    "pdfLink": "http://arxiv.org/pdf/2403.00032v1",
    "authors": [
      "Nikolaos Nakis",
      "Abdulkadir Celikkanat",
      "Louis Boucherie",
      "Sune Lehmann",
      "Morten Mrup"
    ]
  },
  {
    "id": "2402.18137",
    "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference\n  Learning",
    "abstract": "  Multimodal pretraining has emerged as an effective strategy for the trinity\nof goals of representation learning in autonomous robots: 1) extracting both\nlocal and global task progression information; 2) enforcing temporal\nconsistency of visual representation; 3) capturing trajectory-level language\ngrounding. Most existing methods approach these via separate objectives, which\noften reach sub-optimal solutions. In this paper, we propose a universal\nunified objective that can simultaneously extract meaningful task progression\ninformation from image sequences and seamlessly align them with language\ninstructions. We discover that via implicit preferences, where a visual\ntrajectory inherently aligns better with its corresponding language instruction\nthan mismatched pairs, the popular Bradley-Terry model can transform into\nrepresentation learning through proper reward reparameterizations. The resulted\nframework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively\ntailored for decision-making tasks, providing an embodied representation\nlearning framework that elegantly extracts both local and global task\nprogression features, with temporal consistency enforced through implicit time\ncontrastive learning, while ensuring trajectory-level instruction grounding via\nmultimodal joint encoding. Evaluation on both simulated and real robots\ndemonstrates that DecisionNCE effectively facilitates diverse downstream policy\nlearning tasks, offering a versatile solution for unified representation and\nreward learning. Project Page: https://2toinf.github.io/DecisionNCE/\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18137v1",
    "authors": [
      "Jianxiong Li",
      "Jinliang Zheng",
      "Yinan Zheng",
      "Liyuan Mao",
      "Xiao Hu",
      "Sijie Cheng",
      "Haoyi Niu",
      "Jihao Liu",
      "Yu Liu",
      "Jingjing Liu",
      "Ya-Qin Zhang",
      "Xianyuan Zhan"
    ]
  },
  {
    "id": "2402.18390",
    "title": "Neuromorphic Event-Driven Semantic Communication in Microgrids",
    "abstract": "  Synergies between advanced communications, computing and artificial\nintelligence are unraveling new directions of coordinated operation and\nresiliency in microgrids. On one hand, coordination among sources is\nfacilitated by distributed, privacy-minded processing at multiple locations,\nwhereas on the other hand, it also creates exogenous data arrival paths for\nadversaries that can lead to cyber-physical attacks amongst other reliability\nissues in the communication layer. This long-standing problem necessitates new\nintrinsic ways of exchanging information between converters through power lines\nto optimize the system's control performance. Going beyond the existing power\nand data co-transfer technologies that are limited by efficiency and\nscalability concerns, this paper proposes neuromorphic learning to implant\ncommunicative features using spiking neural networks (SNNs) at each node, which\nis trained collaboratively in an online manner simply using the power exchanges\nbetween the nodes. As opposed to the conventional neuromorphic sensors that\noperate with spiking signals, we employ an event-driven selective process to\ncollect sparse data for training of SNNs. Finally, its multi-fold effectiveness\nand reliable performance is validated under simulation conditions with\ndifferent microgrid topologies and components to establish a new direction in\nthe sense-actuate-compute cycle for power electronic dominated grids and\nmicrogrids.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18390v1",
    "authors": ["Xiaoguang Diao", "Yubo Song", "Subham Sahoo", "Yuan Li"]
  },
  {
    "id": "2402.18651",
    "title": "Quantifying Human Priors over Social and Navigation Networks",
    "abstract": "  Human knowledge is largely implicit and relational -- do we have a friend in\ncommon? can I walk from here to there? In this work, we leverage the\ncombinatorial structure of graphs to quantify human priors over such relational\ndata. Our experiments focus on two domains that have been continuously relevant\nover evolutionary timescales: social interaction and spatial navigation. We\nfind that some features of the inferred priors are remarkably consistent, such\nas the tendency for sparsity as a function of graph size. Other features are\ndomain-specific, such as the propensity for triadic closure in social\ninteractions. More broadly, our work demonstrates how nonclassical statistical\nanalysis of indirect behavioral experiments can be used to efficiently model\nlatent biases in the data.\n",
    "pdfLink": "http://arxiv.org/pdf/2402.18651v1",
    "authors": ["Gecia Bravo-Hermsdorff"]
  }
]
